[{"body":"Overview Building and using a TensorRT engine for inference involves two distinct phases: build time (optimization) and runtime (execution).\nBuild time (optimization and compilation) Load model typically from format like ONNX. Parse model, understand network structure and weights. Optimize model, including layer, tensor fusion, precision calibration, kernel auto-tunning. Serialize model into a single, executable engine file. Runtime (Execution) Create Runtime instance. Deserialize engine into an ICudaEngine object Create context IExecutionContext from the engine, it manages a single inference session. Prepare buffer for input and output. Execute inference. Retrieve results. Free resources Runtime Deployment Steps 1. Create a Runtime Object Create an instance of the TensorRT runtime. This object is the entry point for all inference operations and is responsible for managing the execution of engines. It's a key part of the TensorRT API and is a lightweight object designed to be quickly instantiated.\n1nvinfer1::IRuntime* runtime = nvinfer1::createInferRuntime(gLogger); Or\n1runtime = trt.Runtime(trt.Logger()) 2. Deserialize the Engine The engine file (.engine or .plan) is a serialized binary file. The runtime's job is to read the file and deserialize it back into an ICudaEngine object. This process loads the pre-optimized network graph and weights into the GPU's memory.\n1std::ifstream file(\u0026#34;my_model.engine\u0026#34;, std::ios::binary); 2file.seekg(0, file.end); 3size_t size = file.tellg(); 4file.seekg(0, file.beg); 5char* engine_data = new char[size]; 6file.read(engine_data, size); 7nvinfer1::ICudaEngine* engine = runtime-\u0026gt;deserializeCudaEngine(engine_data, size); 1with open(\u0026#34;my_model.engine\u0026#34;, \u0026#34;rb\u0026#34;) as f: 2 engine = runtime.deserialize_cuda_engine(f.read()) 3. Create an Execution Context The engine holds the model's core structure and weights, but to actually perform an inference run, need an execution context. This context is a lightweight object that holds the state for a single inference, including temporary GPU memory for intermediate tensors (activations). An engine can create multiple contexts, which is useful for running parallel inferences on the same model\n1nvinfer1::IExecutionContext* context = engine-\u0026gt;createExecutionContext(); 1context = engine.create_execution_context() 4. Prepare Input and Output Buffers Before running the model, must allocate memory on the GPU for the input and output. Copy input data from CPU to GPU's memory.\n1void* buffers[2]; // Pointers for input and output 2cudaMalloc(\u0026amp;buffers[0], input_size); 3cudaMalloc(\u0026amp;buffers[1], output_size); 4cudaMemcpy(buffers[0], host_input_data, input_size, cudaMemcpyHostToDevice); 1d_input = cuda.mem_alloc(input_data.nbytes) 2d_output = cuda.mem_alloc(output_data.nbytes) 3cuda.memcpy_htod(d_input, input_data) 5. Run the Inference With the context and buffers ready, can execute the model. The enqueueV2 or enqueueV3 method of the execution context is used to launch the optimized kernels on the GPU. The is an asynchronous operation, so it will return immediately without waiting for the computation to finish.\n1context-\u0026gt;executeV2(buffers); 1context.execute_v2(bindings=[int(d_input), int(d_output)]) 6. Retrieve the Results After the inference is complete, copy the results from GPU memory back to the CPU memory for post-processing and use. Use CUDA synchronization like cudaStreamSynchronize to ensure the GPU computation has finished before reading the data.\n1cudaMemcpy(host_output_data, buffers[1], output_size, cudaMemcpyDeviceToHost); 1cuda.memcpy_dtoh(output_data, d_output) 7. Free Allocated Memory, Destroy Objects, Contexts Free the allocated memory for TensorRT engine, and destroy context and objects. This ensures GPU memory and resources are properly released, preventing memory leaks.\n1cudaFree(buffers[0]); 2cudaFree(buffers[1]); 3 4context-\u0026gt;destroy(); 5engine-\u0026gt;destroy(); 6runtime-\u0026gt;destroy(); 1d_input.free() 2d_output.free() 3 4context.destroy() 5engine.destroy() 6runtime.destroy() Example Code Below is an example loading a pre-built TensorRT engine and running inference.\n1#include \u0026lt;iostream\u0026gt; 2#include \u0026lt;fstream\u0026gt; 3#include \u0026lt;vector\u0026gt; 4#include \u0026lt;string\u0026gt; 5#include \u0026lt;numeric\u0026gt; 6#include \u0026lt;cuda_runtime.h\u0026gt; 7 8#include \u0026#34;NvInfer.h\u0026#34; 9#include \u0026#34;NvOnnxParser.h\u0026#34; 10 11// Custom TensorRT logger to catch warnings and errors 12class Logger : public nvinfer1::ILogger { 13public: 14 void log(Severity severity, const char* msg) noexcept override { 15 if (severity \u0026lt;= Severity::kINFO) { // Adjust severity level as needed 16 std::cout \u0026lt;\u0026lt; msg \u0026lt;\u0026lt; std::endl; 17 } 18 } 19}; 20 21// --- Helper Functions for CUDA Memory Management --- 22void* safeCudaMalloc(size_t memSize) { 23 void* deviceMem; 24 cudaMalloc(\u0026amp;deviceMem, memSize); 25 if (deviceMem == nullptr) { 26 std::cerr \u0026lt;\u0026lt; \u0026#34;CUDA memory allocation failed.\u0026#34; \u0026lt;\u0026lt; std::endl; 27 exit(EXIT_FAILURE); 28 } 29 return deviceMem; 30} 31 32void checkCudaErrors(cudaError_t err) { 33 if (err != cudaSuccess) { 34 std::cerr \u0026lt;\u0026lt; \u0026#34;CUDA error: \u0026#34; \u0026lt;\u0026lt; cudaGetErrorString(err) \u0026lt;\u0026lt; std::endl; 35 exit(EXIT_FAILURE); 36 } 37} 38 39// --- Main Function for Inference --- 40int main() { 41 Logger logger; 42 43 // 1. Create a TensorRT Runtime object 44 nvinfer1::IRuntime* runtime = nvinfer1::createInferRuntime(logger); 45 if (!runtime) { 46 std::cerr \u0026lt;\u0026lt; \u0026#34;Failed to create IRuntime.\u0026#34; \u0026lt;\u0026lt; std::endl; 47 return -1; 48 } 49 50 // 2. Load and deserialize the engine file 51 std::string engine_path = \u0026#34;model.engine\u0026#34;; 52 std::ifstream file(engine_path, std::ios::binary); 53 if (!file.good()) { 54 std::cerr \u0026lt;\u0026lt; \u0026#34;Error opening engine file: \u0026#34; \u0026lt;\u0026lt; engine_path \u0026lt;\u0026lt; std::endl; 55 return -1; 56 } 57 58 file.seekg(0, std::ios::end); 59 size_t size = file.tellg(); 60 file.seekg(0, std::ios::beg); 61 std::vector\u0026lt;char\u0026gt; engine_data(size); 62 file.read(engine_data.data(), size); 63 64 nvinfer1::ICudaEngine* engine = runtime-\u0026gt;deserializeCudaEngine(engine_data.data(), size); 65 if (!engine) { 66 std::cerr \u0026lt;\u0026lt; \u0026#34;Failed to deserialize engine.\u0026#34; \u0026lt;\u0026lt; std::endl; 67 return -1; 68 } 69 70 // 3. Create an Execution Context 71 nvinfer1::IExecutionContext* context = engine-\u0026gt;createExecutionContext(); 72 if (!context) { 73 std::cerr \u0026lt;\u0026lt; \u0026#34;Failed to create IExecutionContext.\u0026#34; \u0026lt;\u0026lt; std::endl; 74 return -1; 75 } 76 77 // 4. Prepare host and device memory buffers 78 // Assume a single-input, single-output network with known dimensions 79 const int input_idx = engine-\u0026gt;getBindingIndex(\u0026#34;input_tensor\u0026#34;); 80 const int output_idx = engine-\u0026gt;getBindingIndex(\u0026#34;output_tensor\u0026#34;); 81 82 nvinfer1::Dims input_dims = engine-\u0026gt;getBindingDimensions(input_idx); 83 size_t input_size = std::accumulate(input_dims.d + 1, input_dims.d + input_dims.nbDims, engine-\u0026gt;getBindingDataType(input_idx) == nvinfer1::DataType::kFLOAT ? sizeof(float) : 1, std::multiplies\u0026lt;size_t\u0026gt;()) * input_dims.d[0]; 84 85 nvinfer1::Dims output_dims = engine-\u0026gt;getBindingDimensions(output_idx); 86 size_t output_size = std::accumulate(output_dims.d + 1, output_dims.d + output_dims.nbDims, engine-\u0026gt;getBindingDataType(output_idx) == nvinfer1::DataType::kFLOAT ? sizeof(float) : 1, std::multiplies\u0026lt;size_t\u0026gt;()) * output_dims.d[0]; 87 88 std::vector\u0026lt;float\u0026gt; host_input_data(input_size / sizeof(float)); 89 std::vector\u0026lt;float\u0026gt; host_output_data(output_size / sizeof(float)); 90 91 // Fill host_input_data with your actual input (e.g., pre-processed image) 92 // For demonstration, we\u0026#39;ll use a dummy input 93 std::cout \u0026lt;\u0026lt; \u0026#34;Preparing dummy input...\u0026#34; \u0026lt;\u0026lt; std::endl; 94 for (size_t i = 0; i \u0026lt; host_input_data.size(); ++i) { 95 host_input_data[i] = static_cast\u0026lt;float\u0026gt;(rand()) / static_cast\u0026lt;float\u0026gt;(RAND_MAX); 96 } 97 98 // Allocate GPU memory 99 void* device_buffers[2]; 100 device_buffers[input_idx] = safeCudaMalloc(input_size); 101 device_buffers[output_idx] = safeCudaMalloc(output_size); 102 103 // Copy input data from host to device 104 checkCudaErrors(cudaMemcpy(device_buffers[input_idx], host_input_data.data(), input_size, cudaMemcpyHostToDevice)); 105 106 // 5. Run the inference 107 std::cout \u0026lt;\u0026lt; \u0026#34;Running inference...\u0026#34; \u0026lt;\u0026lt; std::endl; 108 bool status = context-\u0026gt;executeV2(device_buffers); 109 if (!status) { 110 std::cerr \u0026lt;\u0026lt; \u0026#34;Inference execution failed.\u0026#34; \u0026lt;\u0026lt; std::endl; 111 return -1; 112 } 113 114 // 6. Retrieve the results 115 checkCudaErrors(cudaMemcpy(host_output_data.data(), device_buffers[output_idx], output_size, cudaMemcpyDeviceToHost)); 116 117 // Print a few output values for verification 118 std::cout \u0026lt;\u0026lt; \u0026#34;Inference successful. First 5 output values:\u0026#34; \u0026lt;\u0026lt; std::endl; 119 for (int i = 0; i \u0026lt; 5; ++i) { 120 std::cout \u0026lt;\u0026lt; host_output_data[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; 121 } 122 std::cout \u0026lt;\u0026lt; std::endl; 123 124 // 7. Clean up and free resources 125 checkCudaErrors(cudaFree(device_buffers[input_idx])); 126 checkCudaErrors(cudaFree(device_buffers[output_idx])); 127 128 context-\u0026gt;destroy(); 129 engine-\u0026gt;destroy(); 130 runtime-\u0026gt;destroy(); 131 132 std::cout \u0026lt;\u0026lt; \u0026#34;Cleaned up resources successfully.\u0026#34; \u0026lt;\u0026lt; std::endl; 133 134 return 0; 135} ","link":"http://localhost:1313/post/dl/014-tensorrt-usage/","section":"post","tags":["AI"],"title":"TensorRT Engine Deployment Guide"},{"body":" CPU and GPU CPU is designed to excel at executing a sequence of operations, called a thread. It can execute a few tens of threads in parallel. General purpose, good for serial processing, great for task parallelism, large area dedicated cache and control. GPU is designed to excel at executing many thousands of them in parallel. Highly specialized for parallelism, good for parallel processing, great for data parallelism, high throughput, hundreds of floating point execution units. GPU Programming Model Key Concepts Host and Device: Host is CPU and device is GPU, CPU runs the main program and is responsible for offloading parallel tasks to the GPU. GPU performs the computations. Kernel: Function that runs on the GPU. It is code will be executed by many GPU threads in parallel. The host program launches a kernel on the device. Data Transfer: Data must be explicitly transferred between the host's main memory and the GPU's memory. The host allocates memory on the device, copies data from the host to the device, launches the kernel, and then copies the results back from the device to the host. Execution Model: The accelerator model uses a data-parallel execution model. A single kernel is launched to process a large dataset, with each thread handling a small piece of the data. For example in a vector addition, each thread would be responsible for adding a single pair of elements. Advantages and Disadvantages Advantages:\nIt simplifies GPU programming by abstracting away the hardware complexities. Code is more portable across different GPU architectures. Allows developers to focus on the parallel algorithms rather than low-level implementation details. Disadvantages:\nData transfer between host and device can introduce significant overhead, which can be a bottleneck. High-level abstraction may not be suitable for application that require fine-grained control over hardware resources for optimal performance. Debugging can be more challenging due to the asynchronous nature of host-device execution. Accelerator Models CUDA: NVIDIA's proprietary programming model for its GPUs. OpenCL: An open standard for parallel programming of heterogeneous systems, including GPUs, CPUs, and other processors. It provides a vendor-neutral approach. HIP (Heterogeneous-Compute Interface for Portability): A C++ runtime API and kernel language designed to port CUDA applications to AMD GPUs. It acts as a layer that translates CUDA code to run on AMD hardware. HIP HIP is designed to be a bridge between CUDA and AMD's GPU platform ROCm. It includes a tool called hipify which can automatically convert most CUDA code into HIP code.\nHIP API code is compiled using compiler driver hipcc, it uses the underlying ROCm software stack to compile and run the code on the AMD GPU; It translates the HIP API calls into their corresponding CUDA API calls and then compiles the code with NVCC to run on the NVIDIA GPU.\nHIP is a layer on top of CUDA and ROCm. It provides the portability the CUDA lacks, it can run on both AMD and NVIDIA GPUs.\nVolta GPU Microarchitecture Volta is a microarchitecture developed by NVIDIA in 2017, it was a significant leap forward in GPU technology, especially for AI and high-performance computing (HPC) workloads. The most prominent product featuring the Volta architecture was the NVIDIA Tesla V100, a high-end accelerator card for data centers.\nKey features:\nTensor Cores: are specialized hardware units that are highly efficient at performing mixed-precision matrix operations. Tensor cores enabled a massive acceleration of AI training and inference. High-Bandwidth Memory (HBM2): This type of memory is stacked in 3D, providing much higher memory bandwidth compared to traditional GDDR memory. This is crucial for handling the large datasets used in AI and HPC. NVLink 2.0: New version of high-speed interconnect technology, NVLink. Faster communication link between the GPU and CPU, as well as between multiple GPUs in a multi-GPU system. This was vital for scaling AI and HPC applications across multiple accelerators, preventing data transfer from becoming a bottleneck. Volta was a foundational architecture for NVIDIA, newer generations build upon the previous one, adding new features and improving performance:\nTuring 2018: Introduced RT Cores for real-time ray tracing. Ampere 2020: Introduced 3rd generation Tensor Cores for enhanced AI performanc. 2nd generation RT Cores, and multi-instance GPU, allows a single GPU to be partitioned into multiple instances to serve different users or applications. Hopper 2022: Most recent architecture. 4th generation Tensor Cores, 2nd generation MIG GPU Architecture Scheme Architecture Scheme:\nGPC (Graphics processing cluster) Highest-level cluster on the gpu. It contains all the essential resources for a workload. A GPU chip is made up of multiple GPCs. Each GPC typically has its own set of texture processing clusters (TPCs) and a raster engine. TPC (Texture processing cluster) Sub-unit of GPC, it groups together multiple streaming multiprocessors (SMs) and other dedicated hardware. SM (Streaming Multiprocessor) Fundamental building block of a GPU. This is where the actual parallel computation takes place. An SM contains a collections of processing cores (CUDA cores, in Volta, Tensor Cores), a large register file, a cache hierarchy (L1 cache and shared memory), and other specialized units. It's the core engine that executes a single thread block of CUDA or OpenCL program. L2 Cache (Level 2 Cache) Large, high-speed memory shared by all the SMs on the GPU. It sits between the individual SM's L1 cache and the slower global memory. Its purpose is to reduce latency by storing frequently accessed data, preventing the need to fetch it from global memory, which is much slower. All data access to the global memory must pass through the L2 cache. Memory Controller Dedicated circuit that manages the flow of data between the GPU's processing cores and the on-board memory (Like HBM2 on Volta) Controls memory timing, address mapping, and data transfer rates, ensuring high bandwidth and low latency. Streaming Multiprocessors NVIDIA Volta SM contains:\nProcessing Cores: The heart of SM, including CUDA Cores for general purpose parallel computations. In mordern architectures, Tensor cores for AI tasks, and RT cores for ray tracking. 64 single precision cores 32 double precision cores 64 integer cores 8 Tensor cores Memory: Each SM has its own on-chip memory hierarchy, including a large register file for per thread data, a fast L1 cache, and shared memory for inter-thread communication within a thread block. 128 KB memory block for L1 and shared memory 0-96 KB can be set to user managed shared memory The rest is L1 L0 cache is extremely small and fast cache that sits between the register file and the instruction fetch unit. It acts as a buffer to ensure the warp scheduler always has instructions ready to dispatch, which helps maintain a steady flow of work and prevent stalls. Register file is a very large, high=speed memory that stores the private variables for each thread running on the SM. Warp scheduler is the brain of the SM, managing and scheduling the execution of warps, which are groups of 32 threads. Dispatch Unit receives instructions from the warp scheduler and issues them to the execution units. 65536 registers - enables the GPU to run a very large number of threads. Special Function Units (SFUs): Theese are dedicated units that handle complex mathematical operations like square roots and sine functions, which would be slower to compute on standard cores. Load/Store Units: These manage all data movement between the SM's memory and the GPU's global memory. Thread Thread Hierarchy All loops in which the individual iterations are independent of each other can be parallelized. When a kernel is called tens of thousands of threads are created. Single Instruction Multiple Data parallel programming model. Threads are grouped in blocks which are assigned to the SMs.\nHierarchical programming model:\nThread is the smallest unit execution. Single lightweight process that runs a portion of the main parallel function (kernel). Each thread executes the same kernel code but on different piece of data. Block is a group of threads. A key abstraction because all threads within a block can communicate and cooperate with each other. Threads in a block share a fast on-chip memory called shared memory, and can be synchronized using barriers. It allows them to work together on a common task like transposing a matrix. Grid is a collection of thread blocks. It represents the entire workload of the kernel. It is highest level of this hierarchy. Grid is launched from CPU, and its blocks are distributed among the available SMs on the GPU. The blocs in a grid are designed to be independent and have no direct communication with each other. This allows the GPU to schedule them flexibly and in any order. Thread Scheduling, SIMT Warp (CUDA) or Wave (HIP) is a group of GPU threads which are grouped physically. Warp contains 32 threads, wave contains 64 threads. All threads in a warp can only execute the same instruction (SIMT).\nPhysical and Logical Scheme In the picture, there are 16 SMs, each SM contains 32 Cores.\nSPs or cores are main processing units. Cores are like individual students, one student being one core.\nSM is a collections/grouping of Cores, like a class. It is higher level unit that manages tasks across the cores.\nThread is a unit of work, each thread represents an individual task.\nBlock refers to a collection of threads. There is a maximum limit on the number of threads in a block, typically 1024 threads.\nGrid refers to a collection or set of blocks.\nThreads and blocks are fundamental units of parallel execution.\nSM is class room, core is student, thread is task, block is collection of tasks. One core can handle more than 1 thread. To distribute tasks / blocks among the SMs, we need an intermediary called WARP to handle the distribution.\nWarp refers to a group of threads that are executed together in parallel. Typically a warp contains 32 threads. Follows SIMD fashion. All 32 threads execute the same instruction but operate on different data. Warp represents the class monitors, they go and fetch the blocks to bring back to their group for processing; after bringing the blocks to their group, the warps distribute the blocks among the individual students for processing.\nMemory Architecture Memory architecture is critical for writing efficient program.\nFrom fastest to slowest, the memory hierarchy is:\nRegisters: The fast memory, in each SM. Each thread has its own private set of registers, to store local variables and are managed by compiler Shared memory, small, fast, programmer-managed memory on each SM. Threads within a single thread block can use it to cooperate and share data. L1 cache: small cache located on each SM, shared by all threads within the SM. It is a hardware-managed cache for frequently accessed data. L2 cache: large, unified cache shared by all SMs on the GPU. Global memory: Main GPU memory, located on GPU board. Largest and slowest memory, accessible to all threads on the GPU. Global memory: The largest and slowest memory, accessible by all threads and CPU. Main storage for data transferred to the GPU. Register: The fast memory, private to each thread. Variables declared in a kernel are stored here. Local memory: Private memory space for each thread that spills to global memory. Used for thread local data that doesn't fit in registers. shared memory: fast, on-chip memory shared by threads within a single block. constant memory: read only, cached memory space optimized for broadcasting data to all threads. texture memory: read only, cached memory optimized for 2D spatial locality. When training a model:\nGlobal memory store entire or mini batch dataset, models weights and bias, and intermediate values like activations and gradients. Shared memory, used for cooperative tasks within a block, such as parallel reduction to sum up gradients or to load a tile of a matrix for multiplication. Registers: weights and biases, single instance of the input data. are read from global memory and stored in registers for the fastest possible computation. constant memory: hyperparameters. texture memory: accelerate data loading from memory. ","link":"http://localhost:1313/post/tech/011-structure-of-gpu/","section":"post","tags":["gpu"],"title":"GPU Architecture Overview"},{"body":"KITTI Detailed Introduction\nWhen it comes to benchmarking algorithms for autonomous driving, computer vision, and 3D perception, few datasets are as influential as the KITTI dataset. Released by the Karlsruhe Institute of Technology and the Toyota Technological Institute at Chicago, KITTI has become a cornerstone for tasks such as stereo vision, optical flow, visual odometry, 3D object detection, and more.\nIn this blog, I’ll dive deep into the KITTI dataset, with a special focus on point cloud data, data formats, calibration files, annotations, and the 3D bounding box specifications. Whether you’re just getting started or want a technical refresher, this guide will help you understand the essentials.\nWhat is the KITTI Dataset? KITTI is a real-world dataset collected using a setup mounted on a car driving around Karlsruhe, Germany. It includes:\nStereo camera images (color and grayscale) 3D laser scans (Velodyne LiDAR point clouds) GPS/IMU data (for localization) Calibration files (aligning sensors) Ground truth labels (for tasks like 3D object detection) The KITTI dataset is divided into several benchmarks, each aimed at a specific task:\nStereo (depth estimation) Optical Flow Visual Odometry / SLAM 3D Object Detection Road/Lane Detection Tracking For this blog, I'll concentrate especially on the 3D Object Detection benchmark, which heavily utilizes the Velodyne LiDAR point cloud data.\nPoint Cloud Data in KITTI The point cloud data comes from a Velodyne HDL-64E rotating 3D laser scanner. It provides a sparse 3D representation of the environment around the car.\nFile format: .bin files Location: data_object_velodyne/velodyne/ Per file: One scan per file (i.e., per frame) Here is an example code to read and visualize point cloud data in Open3D.\nFormat of the Point Cloud Files Each .bin file contains raw, uncompressed point cloud data. Every point is stored as a float32 (4 bytes each) and is represented as:\n1[x, y, z, reflectance] where:\nx, y, z: 3D Cartesian coordinates (in meters) reflectance: intensity value measured by the LiDAR sensor Each point therefore uses 4 x 4 = 16 bytes.\nYou can read a point cloud using Python like this:\n1import numpy as np 2 3# Read a .bin file 4point_cloud = np.fromfile(\u0026#39;path_to_file.bin\u0026#39;, dtype=np.float32).reshape(-1, 4) Calibration Files The KITTI dataset includes calibration files that provide the necessary transformations between different sensors (e.g., LiDAR, cameras).\nLocation: data_object_calib/calib/ File extension: .txt Each calibration file includes:\nP0, P1, P2, P3: Projection matrices for each camera R0_rect: Rectification matrix (for transforming from camera coordinates to rectified camera coordinates) Tr_velo_to_cam: Transformation from Velodyne LiDAR to camera coordinates Tr_imu_to_velo: Transformation from IMU to Velodyne coordinates Example from a calibration file:\n1P0: 7.215377e+02 0.000000e+00 6.095593e+02 ... 2R0_rect: 9.999239e-01 9.837760e-03 -7.445048e-03 ... 3Tr_velo_to_cam: 7.533745e-03 -9.999714e-01 -6.166020e-04 ... 4Tr_imu_to_velo: 0.007027 -0.999963 -0.000000 ... Understanding these matrices is crucial for projecting 3D LiDAR points onto 2D images or transforming between coordinate frames.\nAnnotation and Label Files KITTI provides detailed object annotations for labeled frames. These are essential for training and evaluating 3D object detection models.\nLocation: data_object_label_2/label_2/ File format: .txt Each label file corresponds to one frame and contains multiple lines, each line describing one object.\nAnnotation Format Each line in the label file has the following fields:\n1Type Truncated Occluded Alpha BBox_xmin BBox_ymin BBox_xmax BBox_ymax 2Dimensions_h Dimensions_w Dimensions_l Location_x Location_y Location_z Rotation_y Breaking it down:\nType: Object class (Car, Van, Truck, Pedestrian, Cyclist, etc.) Truncated: Float (0-1), fraction of object that is outside image boundaries Occluded: 0 = fully visible, 1 = partly occluded, 2 = largely occluded, 3 = unknown Alpha: Observation angle of object (relative to camera center) 2D Bounding Box: BBox_xmin, BBox_ymin: Top-left corner BBox_xmax, BBox_ymax: Bottom-right corner 3D Object Dimensions (in meters, height, width, length): h, w, l 3D Object Location (camera coordinates): x, y, z (center of 3D box, bottom of the object) Rotation_y: Rotation around Y-axis in camera coordinates (yaw) Example line:\n1Car 0.00 0 -1.82 599.41 156.40 629.75 189.25 1.56 1.63 3.69 1.84 1.47 8.41 -1.56 3D Bounding Boxes The 3D bounding boxes provided in the annotations are crucial for evaluating 3D object detection.\nCentered at the bottom center of the object Oriented around the Y-axis (camera coordinate system) Size specified via (height, width, length) Rotated based on the rotation_y angle You can use these parameters to reconstruct the 3D bounding box in the camera or LiDAR coordinate system.\nTo visualize a bounding box:\nStart from the object center Extend along the object dimensions Rotate based on rotation_y Transform into the desired coordinate frame if necessary (e.g., from camera to LiDAR frame) Quick Summary Aspect Description Point Cloud .bin files, (x, y, z, reflectance) per point Calibration .txt files, matrices for aligning sensors Annotations .txt files, 2D bbox + 3D bbox + object class 3D Box Location Bottom center of object Rotation Around Y-axis in camera coordinates Conclusion The KITTI dataset offers one of the richest sets of real-world sensor data for autonomous driving research. Understanding the structure of its point cloud files, calibration data, annotation formats, and bounding box definitions is essential for working with modern perception models.\nBy mastering the data layout, you unlock the ability to build robust models for 3D object detection, sensor fusion, and more. Whether you are developing cutting-edge algorithms or simply learning about LiDAR-based perception, KITTI remains a crucial resource in the computer vision and robotics communities.\n","link":"http://localhost:1313/post/tech/007-kitti-introduction/","section":"post","tags":["lidar"],"title":"KITTI Dataset Introduction"},{"body":"Fundamentals of Reinforcement Learning\nBackground Reinforcement Learning (RL) is a powerful and rapidly advancing branch of machine learning, inspired by behavioral psychology. It focuses on how agents should take actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the learning agent is given input-output pairs, reinforcement learning emphasizes learning from interaction.\nReinforcement Learning (RL) is pivotal across diverse applications. In robotics, it powers autonomous navigation and manipulation tasks, while in gaming, it excels in strategic decision-making as seen with AlphaGo. RL optimizes trading in finance, adapts healthcare treatments, and enhances capabilities in natural language processing and computer vision. Its expanding reach includes smart grids, recommendation systems, and virtual assistants, highlighting RL's transformative impact across various domains.\nMathematical Foundations The RL problem is meant to be a straightforward framing of the problem of learning from interaction with environments $\\mathcal{E}$ over several discrete time steps to achieve a goal. At each time step $t$, the agent receives a state ${s}{t}$ in the environment's state space $\\mathcal{S}$ and selects an action, $a_t \\in \\mathcal {A}(s_t)$ according to a policy $\\pi({a}{t}|{s}{t})$, where $\\mathcal{A}(s_t)$ is the set of actions available in state $s_t$. The policy amounts to a conditional probability $\\pi(a|s)$ of the agent taking action if the current state is $s$. It is a mapping from state and action to the probability of taking an action. After that, the agent will receive a scalar reward ${r}{t}$ and store the transition in the agent's memory as experiences. The process continues until the agent reaches a terminal state. The agent seeks to learn a policy ${ \\pi }^{ \\ast }$ that maximizes the expected discounted return ${ R }_{ t }=\\sum { k=0 }^{ \\infty }{ { \\gamma }^{ k }{ r }{ t+k } }$, accumulated reward with the discount factor $\\gamma \\in (0,1]$ trades-off the importance of immediate and future rewards.\nRL tasks that satisfy the Markov property can be described as Markov decision processes (MDPs), which are defined by a tuple $(\\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R},\\gamma)$, where $\\mathcal{R}$ is a reward function $\\mathcal{R}(s,a)$ and $\\mathcal{P}$ is a state transition probability $\\mathcal{P}({s}{t+1}|{s}{t},{a}{t})$. The Markov property indicates the future states are conditionally independent of the past given the present. So, in an RL task, the decisions and values are assumed to be a function only of the current state. Markov property can be defined as $p({ s }{ t+1 }|{ s }{ 1 },{ a }{ 1 },...,{ s }{ t },{ a }{ t }) = p({ s }{ t+1 }|{ s }{ t },{ a }{ t })$ , which means the future states are conditionally independent of the past given the present. RL task which satisfies Markov property can be described as MDPs, defined by the 5-tuple $(\\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R},\\gamma)$, where $\\mathcal{R}$ is reward function $\\mathcal{R}(s,a)$ and $\\mathcal{P}$ is state transition probability $\\mathcal{P}({s}{t+1}|{s}{t},{a}{t})$. In an episodic task, the state will reset after each episode of length, and the sequence of states, actions, and rewards in an episode constitute a trajectory or rollout of the policy.\nValue Function Value functions are a core component of RL systems, which constructs a function approximator that estimates the long-term reward from any state. It estimates how good (expected return) it is for an agent to be in a given state (or given action in a given state). By this way, the function approximator exploits the structure in the state space to efficiently learn the value of observed states and generalize to the value of similar, unseen states. A typical form of value function can be defined as:\n$${ V }^{ \\pi }(s)=\\mathbb{ E }[R|s,\\pi ]= \\mathbb{E}[\\sum { k=0 }^{ \\infty }{ { \\gamma }^{ k }{ r }{ t+k } }|s,\\pi] $$\nNormally we refer to ${ V }^{ \\pi }(s)$ as the state-value function, which measures the expected discounted return when starting in a state $s$ and following a policy $\\pi$. When actions follow by the optimal policy ${\\pi}^{\\ast}$, the state-value function can be optimal:\n$${ V }^{ \\ast }(s)=\\max _{ \\pi }{ { V }^{ \\pi }(s) } \\quad \\forall s\\in \\mathcal{ S }$$\nIn addition to measuring the value of states, there is also an indicator for measuring the quality of actions' selection, which is denoted as state-action-value or quality function ${Q}^{\\pi}(s,a)$. It defines the value of choosing an action $a$ from a given state $s$ and thereafter following a policy $\\pi$.\n$${ Q }^{ \\pi }(s,a)=\\mathbb{ E }[R|s,a,\\pi ]= \\mathbb{E}[\\sum { k=0 }^{ \\infty }{ { \\gamma }^{ k }{ r }{ t+k } }|s,a,\\pi] $$\nState-action-value is similar to the state value $V^{\\pi}$ except the initial action $a$ is provided, and the policy $\\pi$ is only followed from the succeeding state onwards. The optimal state-action-value function is denoted as:\n$${ Q }^{ \\ast }(s,a)=\\max _{ \\pi }{ { Q }^{ \\pi }(s,a) } \\quad \\forall s\\in \\mathcal{ S } , \\forall a\\in \\mathcal{ A } $$\n${ Q }^{ \\ast }(s,a)$ gives the maximum state-action value for state $s$ and action $a$ achievable by any policy. This action value function satisfies a recursive property, which is a fundamental property of value functions in the RL setting, and it expresses a relationship between the value of a state and its successor states:\n$${Q}^{\\pi}(s,a)=\\mathbb{E}{{s}^{\\prime}}[r+\\gamma\\mathbb{E}{{a}^{\\prime}\\sim{\\pi}({s}^{\\prime})}[{Q}^{\\ast}({s}^{\\prime},{a}^{\\prime})]|s,a,\\pi] $$\nUnlike producing absolute state-action values as with $Q^{\\pi}$, an advantage function represents relative state-action values, which measures whether or not the action is better or worse than the policy's default behavior. Often, it is easier to learn that action yields higher reward than another, than it is to learn the actual return from taking one particular action. Advantage function expresses a relative advantage of actions through this simple relationship:\n$${ A }^{ \\pi }(s,a)={ Q }^{ \\pi }(s,a)-{ V }^{ \\pi }(s) $$\nMany successful value-based RL algorithms rely on the idea of advantage updates.\nKey Reinforcement Learning Algorithms Deep Q-Network Deep reinforcement learning (DRL) applies deep neural nets for representing the value functions within reinforcement learning methods. DRL algorithms have attained superhuman performance in several challenging task domains to attribute to the powerful function approximation and representation learning properties of the DL. The DQN algorithm achieves human-level performance on Atari series games from pixels input. It parameterizes the quality function $Q$ with a neural network $Q(s,a;\\theta)$ that approximates the $Q$ values. Two main techniques of the DQN algorithm can learn value functions in a stable and robust way are using the target network and experience replay. At each iteration, the network's parameters are updated by minimizing the following loss function:\n$${L}{i}({\\theta}{i})=\\mathbb{E}{s,a,r,{s}^{\\prime}}[({y}{i}^{DQN}-Q(s,a;{\\theta}_{i}))^{2}]$$\nwith\n$${y}_{i}^{DQN}=r+\\gamma \\underset {{a}^{\\prime}}{max}Q({s}^{\\prime},{a}^{\\prime};{\\theta}^{-}) $$\nin which ${\\theta}^{-}$ is the parameter for the target network. The first stabilizing method is fixing the target network's parameters rather than calculating the TD error based on its own rapidly fluctuating estimates of the $Q$-values. The second one, experience replay, uses a buffer for storing a certain size of transitions $({s}{t},{a}{t},{s}{t+1},{r}{t+1},)$ makes it possible for training off-policy and enhancing the efficiency of sampling data.\nThere is a series of improvements in the value-based RL setting after the DQN algorithm ignited this field. To reduce the overestimated $Q$-values in DQN, van Hasselt et al. proposed the double DQN algorithm. Wang et al. presented a dueling Q-network architecture to estimate state-value function $V(s)$ and associated advantage function $A(s,a)$ respectively. Tamar et al. proposed a value iteration network that can effectively learn to plan, and it leads to better generalization in many RL tasks. Schaul et al. developed the PER approach built on top of double DQN, it makes the experience replay process more efficient and effective than all transitions are replayed uniformly.\nDueling Network Architecture Unlike the standard single sequence $Q$-networks design, the dueling network structure consists of two sequences (streams) of networks (A-network and V-network) which separately learn action-advantage function and state-value function. This construction decouples the value and advantage functions and combines the two streams to produce the estimate of the state-action value function with a special aggregating module. The two streams share a common feature extraction layer (or lower layers). The deep $Q$-network focuses on estimating every state-action pairs' value. However, the idea of dueling network is to estimate action-independent state function and action-dependent advantage function separately, because in RL environments, not all states are related to a specific action, there are many states independent of action, and under these states the agent does not need to change actions to adapt to the new states. Therefore, it is meaningless and inefficient to estimate such state-action pairs' value. Dueling network firstly presented by Wang et al. and through this change, the training efficiency has been greatly improved than the single-stream $Q$ networks. The dueling network results in a new state of the art for tasks in the discrete action space according to Wang's work. Shortly, the $Q$-values generated by dueling network are more advantageous to the performance improvement than deep $Q$-network in an RL task.\nPolicy Gradient The methods mentioned above indirectly learn the policy $\\pi(s)$ based on the estimate of the value functions. These value-based approaches are effective in handling problem in a discrete actions field. However, when dealing with a problem with a continuous action space such as physical control tasks, the value-based approaches cannot be straightforwardly applied, and it is difficult to ensure the results' convergence since it relies on each actions' $Q$ value. An obvious approach to implement value-based algorithms such as DQN to continuous domains is to discretize the action space to several fixed actions. However, it has many drawbacks and limitations such as throwing information (maybe essential) about the structure of the action domain.\nThere is no such worry in policy-based approaches since the policy network output agent's actions without the estimation of the action-value function. They directly parameterize the control policy $\\pi(a|s;\\theta)$ and update the parameters $\\theta$ to optimize the cumulative reward, therefore, policy-based methods are more applicable to continuous control problem such as tasks of robotic controls than the value-based methods.\nPolicy gradient (PG) is an appealing policy-based algorithm which optimizes the parametric policy ${\\pi}{\\theta}(a|s)=\\mathbb{P}[a|s;\\theta]$ following the gradient ${\\nabla}{\\theta}J({\\pi}{\\theta})$ of its expectation of cumulative reward with respect to the policy parameters. Policy-gradient methods are effective in high-dimensional or continuous action spaces, and can learn stochastic policies. In an RL task, the agent's goal is to find parameter $\\theta$ maximizes the objective function $J(\\pi)$. A typical performance objective to be considered is the average reward function: $J(\\pi)=\\mathbb{E}[R|{\\pi}{\\theta}]$. The policy-gradient theorem provides the gradient of $J$ with respect to the parameters $\\theta$ of policy $\\pi$:\n$${\\nabla}{\\theta}J({\\pi}{\\theta})=\\int {\\mathcal{S}}^{ }{{\\rho}^{\\pi} }\\int{\\mathcal{A}}^{ }{{\\nabla}{\\theta}}{\\pi}{\\theta}(a|s){Q}^{ \\pi}(s,a)dads$$\n$$\\quad\\quad\\quad\\quad=\\mathbb{E}{s\\sim{\\rho}^{\\pi},a\\sim {\\pi}^{\\theta}}[{\\nabla}{\\theta} log{\\pi}^{\\theta}(a|s){Q}^{\\pi}(s,a)] $$\nWhere the ${\\rho}^{\\pi}(s)$ is the state distribution. The unknown part, ${Q}^{\\pi}(s,a)$ is normally estimated by using the actual returns ${ R }_{ t }=\\sum { k=0 }^{ \\infty }{ { \\gamma }^{ k }{ r }{ t+k } }$ as an approximation for each ${Q}^{\\pi}(s_t,a_t)$. Based on this theorem, Silver et al. proposed a deterministic policy-gradient (DPG) algorithm for estimating gradient and it is more efficient than the usual stochastic policy-gradient method. O'Donoghue et al. referred to a new technique by combining PGQL and discussed the practical implementation of this technique in RL setting.\nActor-Critic Algorithm Regular policy-gradient methods often exhibit slow convergence due to the large variances of the gradient estimates. The actor-critic methods attempt to reduce the variance by adopting a critic network to estimate the value of the current policy, which is then used to update the actor's policy parameters in a direction of performance improvement.\nThe action-selection policy is known as the actor ${\\pi}{\\theta}:\\mathcal{S}\\rightarrow \\mathcal{A}$, which make decisions without the need for optimization procedures on a value function, mapping representation of the states to action-selection probabilities . The~value function is known as the critic ${Q}{\\phi}^{\\pi}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$, which estimates the expected return to reduce variance and accelerate learning, mapping states to expected cumulative future reward.\nThe actor and critic are two separated networks share a common observation. At each step, the action selected by actor network is also an input factor to the critic network. In the process of policy improvement, the critic network estimates the state-action value of the current policy by DQN, then actor network updates its policy in a direction improves the $Q$-value. Compared with the previous pure policy-gradient methods, which do not have a value function, using a critic network to evaluate the current policy is more conducive to convergence and stability. The better the state-action value evaluation is, the lower the learning performance's variance is. It is important and helpful to have a better policy evaluation in the critic network.\nPolicy-gradient-based actor-critic algorithms are useful in many real-life applications because they can search for optimal policies using low-variance gradient estimates. Lillicrap et al. presented the DDPG algorithm, which combines the actor-critic approach with insights from DQN, to solve simulated physics tasks and it has been widely used in many robotic control tasks. It uses two neural networks; the actor network learns a deterministic policy and the critic network approximates the Q-function of the current policy.\nSummary Reinforcement Learning (RL) represents a powerful paradigm in machine learning, drawing inspiration from behavioral psychology to enable agents to make decisions that maximize cumulative rewards in complex environments. Formulated as Markov Decision Processes (MDPs), RL tasks involve states, actions, rewards, and transition probabilities. Algorithms such as Deep Q-Networks (DQN) leverage deep neural networks to approximate Q-values efficiently, facilitating decision-making in discrete action spaces.\nValue-based RL methods, exemplified by DQN, estimate state-action values to optimize policies. Actor-critic approaches improve upon traditional policy-gradient methods by incorporating a critic network to estimate value functions, thereby reducing variance and enhancing learning stability. These advancements extend to continuous action spaces with algorithms like Deep Deterministic Policy Gradient (DDPG), which combines deterministic policies and Q-function approximations.\nPolicy-gradient methods directly optimize policies based on gradient estimates of expected rewards, proving effective in scenarios with continuous action spaces. The dueling network architecture further refines training efficiency by separating state-value and advantage functions, emphasizing action-dependent advantages.\nOverall, RL continues to evolve through innovations in value estimation, policy optimization, and application across diverse domains such as robotics and game playing. Advances in neural network architectures and learning algorithms continue to drive progress in RL research and application. Recent trends focus on adapting RL to continuous action spaces, integrating with other domains like NLP and computer vision, and improving sample efficiency and training stability. Future directions include enhancing RL's applicability to real-world challenges through interdisciplinary collaborations and addressing ethical considerations in deployment.\n","link":"http://localhost:1313/post/dl/008-drl-summary/","section":"post","tags":null,"title":"An Introduction to the Fundamentals of Reinforcement Learning"},{"body":"ROS2\nROS2 (Robot Operating System 2) is the modern evolution of the original ROS project, designed for real-time, multi-robot systems with a focus on performance, flexibility, and production-readiness. If you're working with robotics, autonomous vehicles, drones, or intelligent devices, learning ROS2 is an essential skill.\nIn this blog, I'll introduce ROS2 and walk through some of the most common commands you'll use in daily development — covering workspace management, nodes, topics, rosbag recording, and RViz visualization.\nWhat is ROS2? ROS2 is a flexible framework for writing robot software. It provides tools, libraries, and conventions to simplify the task of creating complex and robust robot behavior across a wide variety of robotic platforms.\nCompared to ROS1, ROS2 improves in areas like:\nMulti-platform support (Linux, Windows, macOS) Real-time capability Better security Middleware flexibility (DDS-based) Production-readiness for industrial use Key Differences Between ROS1 and ROS2 While ROS1 laid the foundation for open-source robotics development, ROS2 was built to address its limitations and meet modern robotics needs. Here are the main differences:\nFeature ROS1 ROS2 Middleware Custom TCP-based (ROS Master, topics, services) Based on DDS (Data Distribution Service), providing decentralized, flexible communication Real-Time Support Very limited Designed with real-time systems in mind (e.g., better scheduling, memory control) Multi-Robot Support Difficult, requires complex networking setups Built-in multi-robot and multi-domain support Cross-Platform Primarily Linux Linux, Windows, macOS officially supported Security No official security mechanisms DDS provides built-in security features like encryption, authentication Build System catkin_make, catkin_tools Colcon build system, more modular and powerful Launch System XML-based launch files (.launch) Python-based (.py) launch files (more flexible, programmable) Lifecycle Management Manual node management Managed nodes with defined lifecycles for better control (e.g., configurable startup/shutdown states) Middleware Flexibility Fixed to custom communication layer Pluggable middlewares (e.g., different DDS vendors can be used) In short, ROS2 is not just an \u0026quot;update\u0026quot; of ROS1 — it’s a fundamental re-architecture to make ROS ready for the future of robotics.\nEssential ROS2 Commands 1. Workspace Setup A workspace is where you organize your ROS2 packages. You’ll build and source your workspace to compile and run your robot applications.\n1# Source ROS2 environment 2source /opt/ros/\u0026lt;ros2-distro\u0026gt;/setup.bash 3 4# Create a workspace 5mkdir -p ~/ros2_ws/src 6cd ~/ros2_ws 7 8# Build the workspace 9colcon build 10 11# Source the workspace (after build) 12source install/setup.bash 13 14# Launch a launch file 15ros2 launch \u0026lt;package_name\u0026gt; \u0026lt;launch_file_name\u0026gt; Notes:\nReplace \u0026lt;ros2-distro\u0026gt; with your installed version, e.g., humble, iron, foxy. Always source your workspace after building it so that ROS2 can find your new packages. 2. Working with Nodes and Topics In ROS2, nodes are basic processes that perform computation. Topics are used for sending and receiving messages between nodes.\nNode Commands 1# List all running nodes 2ros2 node list 3 4# Get detailed info about a specific node 5ros2 node info /\u0026lt;node_name\u0026gt; Topic Commands 1# List all available topics 2ros2 topic list 3 4# Get information about a topic (like message type) 5ros2 topic info /\u0026lt;topic_name\u0026gt; 6 7# Print (echo) live messages from a topic 8ros2 topic echo /\u0026lt;topic_name\u0026gt; Service Commands ROS2 services are request-response communication types:\n1# Call a service 2ros2 service call /\u0026lt;service_name\u0026gt; \u0026lt;service_type\u0026gt; \u0026#34;\u0026lt;request_data\u0026gt;\u0026#34; Example:\n1ros2 service call /reset_robot std_srvs/srv/Empty \u0026#34;{}\u0026#34; 3. Recording and Playing Data with Rosbag Rosbag allows you to record and replay ROS2 messages. It's invaluable for testing and debugging without needing live hardware.\n1# Record all topics 2ros2 bag record -a 3 4# Play back a recorded bag file 5ros2 bag play \u0026lt;bag_file_name\u0026gt;.db3 Tip: .db3 is the default format (SQLite3 database) for ROS2 bag files.\n4. Visualizing Data with RViz2 RViz2 is a powerful visualization tool for ROS2, helping you view sensor data, robot models, and more in a graphical environment.\nInstall RViz2 1# Install rviz2 package 2sudo apt-get install ros-\u0026lt;ros2-distro\u0026gt;-rviz2 Example:\n1sudo apt-get install ros-humble-rviz2 Launch RViz2 1rviz2 You can load robot models, display sensor topics like point clouds, lasers, maps, and more in real-time.\n","link":"http://localhost:1313/post/tech/005-ros2/","section":"post","tags":["ROS2"],"title":"Introduction to ROS2, Key Features and Common Commands"},{"body":"Based on NG's Machine Learning course\nC1-Supervised Machine Learning - Regression and Classification P25 Feature scaling Features may have different units and magnitudes, algorithms perform better when features are on a comparable scale.\nPrevent domination by large-scale features, algorithms like gradient descent, KNN, and clustering can be biased toward features with larger numerical values. Faster convergence in optimization. Common methods of feature scaling:\nMin-max normalization, keep data in a range, it is sensitive to outliers. Standardization or Z-score normalization. Centers data at mean 0, standard deviation to 1. Works better when data has outliers. Robust scaling (using median and IQR), good for datasets with many outliers. Mean normalization: $x_i:=\\dfrac{x_i - \\mu_i}{max-min}$\nZ-score Normalization $x_i:=\\dfrac{x_i - \\mu_i}{\\sigma_i}$\nAll features will have a mean of 0 and a standard deviation of 1. $\\sigma$ is the standard deviation$\nThe scaled features get very accurate results much, much faster.\nP29 Feature Engineering, P30 Multiple linear regression Feature Engineering: Using intuition to design new features, by transforming or combining original features. So the machine learning model can understand the data better and perform more accurately.\nIt turns raw sensory signals into meaningful information that makes patterns easier for a model to learn.\nquadratic function: 二次方程, cubic function: 三次方程\nWhen doing feature engineering, the feature scaling becomes increasingly important.\nGradient descent is picking the 'correct' features for us by emphasizing its associated parameter.\nP31 Motivation Binary classification, negative/positive class\nP32 Logistic regression Logistic regression is a supervised learning algorithm used for binary classification. Instead of predicting a continuous value like in linear regression, it predicts the probability that a sample belongs to a class.\nLogistic regression assumes that input features have a linear relationship with some underlying score. But probabilities must be between 0 and 1, so we pass that score through a sigmoid function. tumor: 肿瘤 malignant 恶性的\nLogistic function is sigmoid function\n$g(z)=\\dfrac{1}{1+e^{(-z)}}$\nThis maps any real number $z$ into the range (0, 1).\nLogistic regression model:\n$f_{w,b}{x}=g(w\\cdot x+b)=\\dfrac{1}{1+e^{-(w\\cdot x + b)}}$\n$f_{w,b}(x)=P(y=1|x;w,b)$ , That means: Probability that y is 1, given input x, parameters w,b\nP33 Decision Boundary A decision boundary is a line, plane, or hypersurface that separates different classes predicted by a classifier.\nDecision boundary: $z=w\\cdot x + b=0$\nNon-linear decision boundaries,\nP34 Cost Function for Logistic Regression Loss function: $L(f_{w,b}(x),y)$\nIf y = 1 $= -\\log(f_{w,b}(x))$\nIf y = 0 $= -\\log(1-f_{w,b}(x))$\nP35 Simplified Loss function Loss function measures the error for a single training example:\n$L(f_{w,b}(x,y))=-y\\log(f_{w,b}(x))-(1-y)\\log(1-f_{w,b}(x))$\nCost function measures the average error across the entire training set:\n$J(w,b)=\\dfrac{1}{m}\\sum[L(f_{w,b}(x),y)]=-\\dfrac{1}{m}\\sum[y\\log(f_{w,b}(x))-(1-y)\\log(1-f_{w,b}(x))]$\nWhy choose this as the cost function, it derives from statistics using maximum likelihood estimation, which is an idea from statistics on how to efficiently find parameters from different models.\nP37 Overfitting Overfitting happens when a model learns the training data too well, including its noise, outliers, and random fluctuations, instead of just the underlying pattern.\nUnderfit, high bias; Overfit, high variance\nCauses of overfitting:\nToo complex model Too few training samples Noisy data Too many training epochs without regularization P38 Addressing Overfitting More training data, collect more data. REgularization (L1, L2 penalties) Dropout in neural networks Early stopping, stop training before overfitting. Cross validation to check generalization Simplify the model, reduce depth, features, parameters. Feature selection, drop redundant or irrelevant features, use domain knowledge or use statistical methods. All features + insufficient data = Overfitting, this is the curse of dimensionality. P39 Regularization Regularization discourages a model from becoming too complex.\nA model with very large weights $w$ tends to fit the training data too perfectly, including noise. Regularization adds a penalty to the cost function, so the model prefers simpler weights (smaller values). $J(w,b)=\\dfrac{1}{2m}\\sum(f(x)-y)^2 + \\dfrac{\\lambda}{2m}\\sum w^2, \\lambda \u0026gt; 0$\nL2 regularization (Ridge):\nAdds the sum of squared weights to the cost function. Shrinks weights smoothly, keeps all features but reduces influence. Decision boundary becomes smoother. $J(w,b)=\\dfrac{1}{2m}\\sum(f(x)-y)^2 + \\dfrac{\\lambda}{m}\\sum |w|, \\lambda \u0026gt; 0$\nL1 regularization (Lasso)\nAdds the sum of absolute weights to the cost function. Some weights shrink to zero, automatic feature selection. Creates sparse models. P40 Regularization for linear regression In linear regression, if we have too many features or correlated features, weights can be come very large. Apply regularization prevents overfitting:\nL2: small weights smooth solution. L1: sparse weights, feature selection. Elastic Net: mix of both C2-Advanced Learning Algorithms P45 Requirement prediction\nInference (prediction)\nactivation; activation values;\n$a = f(x) = \\dfrac{1}{1+e^{wx+b}}$\n$a^{[1]}$ denotes the output (activation value) of layer 1. input layer is layer 0.\nP48\n$a_j^{[l]}=g(w_j^{[l]}\\cdot a^{[l-1]}+b_j^[l])$\ng is the activation function\nP51\nMatrix 2x3 in Numpy x = np.array([[1,2,3], [4,5,6]]) Matrix is 2D array.\nVector is 1D array. x = np.array([200, 17]), just a list.\nP53 Forward prop in a singile layer\nP61\nBinary classification problem, The logistic loss function, also known as binary cross entropy.\nCompute derivatives for gradient descent using back propagation.\nP62 Activation function\nSigmoid\nCommen used one is ReLU (rectified linear unit) $g(z)=\\max (0,z)$.\nP63\nClassification prolem, sigmoid function is nature choice for output layer.\nRegression: Linear activation function for output layer\nOr ReLU if for non-negative output values.\nFor hidden layer activation, ReLU is more common used. Sigmoid is rarely used today. ReLU is fast to compute, ReLU goes to flat only to the left, Sigmoid has two flat zone, it makes gradient descent fast.\nP66 Multiclass, Softmax\nSoftmax is used for multiclass classification problem\nSoftmax regression, N possible outputs\n$z_j = w_j \\cdot x + b_j, j=1,...,N$\nparameters w and b\nactivation function: $a_j=\\dfrac{e^{z_j}}{\\sum_{k=1}^{N}e^{z_k}}=\\mathbf{P}(y=j|x)$\n$a_1 + ... + a_N=1$\nIf use softmax for binary classification, it is same as sigmoid activation.\nP67\nFor digit recognition prolem, use softmax as output layer, and use 10 neutron to form the output layer.\nLoss function name in TF is SparseCategoricalCrossentropy\nP68\nTF, from_logits=True will make the round off more accurate when with softmax.\nmodel.compile(loss=SparseCategoricalCrossEntropy(from_logits=True))\nP69 Multi labels classification\nCan use sigmoid activations function for the output layer.\nP79 Advance Optimization\nAdam (Adaptive moment estimation) algorithm, automatically adjusts the learning rate alpha. It not just one alpha, but each neutro has one learning rate.\nIf w or b keeps moving in same direction, increase alpha If keeps oscillating, reduce alpha.\nIn code, model compile select optimizer use Adam. It uses one default init learning rate.\nIt is better than gradient descent algorithm.\nP71\nDense layer.\nConvolutional layer, Each neuron only looks at part of the previous layer's inputs. looking a window.\nFaster computation Need less training data, less prone to overfitting. LSTM, Transformer, attention models.\nP75\nDiagnostic: A test that you run to gain insight into what is/isn't working with a learning algorithm, to gain guidance into improving its performance.\nP77 model selcection\nTraining set, cross validation data, test set.\nEvaluate a model using cross validation data during training, and use testing set to estimate generalization error.\nP78 Bias/Variance\nhigh bias - underfit J_train is high; J_cv is high\nhigh variance (overfit) J_train is low; J_cv is high\nP80 Establishing a base line\nhigh bias model，more training dataset helps less.\nhigh variance model, more training dataset helps.\nP82\nGet more training examples - fixes high variance Try smaller sets of features - fixes high variance Reduce flexibility of the model Try getting additional features - fixes high bias Try adding polynomial features - fixes high bias Try decreasing lambda - high bias Try increasing lambda - high variance P83\nSimple model -\u0026gt; High bias Complex model -\u0026gt; High variance\nTradeoff between high bias and high variance.\nLarge neural networks are low bias machines. It just fits very compicated functions very well, so when training neural network, we offten faceing problems other than high bias problem if neural network is large enough.\nDoes it do well on training set? if not, use bigger network\nDoes it do well on the cross validation set? If not, use more training data.\nA large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately.\nP84\nInterative loop of ML development\nChoose architecture (model, data, etc.), Train model, Diagnostics (bias, variance, error analysis), choose architecture and so on.\nText classfication. Features: list the top 10000 words to compute input variables.\nLogistic model, or neural network model.\nP85 Error analysis\nGroup the misclassifies in corss validation set based on common traits/features.\nThese groups are not mutually exclusive.\nIf the dataset is large, can get samples randomly from misclassifies to analysis.\nHow to try to reduce your spam classifier's error?\nCollect more data Develop sophisticated features based on email routing, from email header Define sophisticated features from email body, some words are treated as the same world. Design algorithms to detect misspellings. P89\nData augmentation: Modifying an existing traning example to create a new training example.\nExamples:\nImage recognition: Distortion a image, mirro, rotate, enlarge, shrink, change contrast.\nSpeech recognition example. Different of noisy background, bad cellphone connection.\nUsually does not help to add purely random/meaningless noise to your data. It should be representation of the type of noise/distortions in the test set.\nData synthesis\nPhoto OCR example, generate sysnthesis data to train.\nConventional model-centric approach: AI = Code + Data; Works more on Code (algorithm/model)\nData-centric approach: AI = Code + Data; Focus on Data engineering.\nP90 Transfer learning\nEliminate the last layer of the origin model, and switch it to the layer needed.\nUse the parameter from the first layers from the origin model.\nOption 1: only train output layers parameters.\nOption 2: Train all parameters. The first layers are initialized with the origin model.\nTwo steps: Supervised pretraining, Fine tuning.\nConvolution NN: firs layer detects edges, then corners, then curvers/basic shapes.\nSummary: 1. Download NN parameters pretrained. 2. Further train (fine tuen) the network on your own data.\nP88 ML Project Development process\nDefine project: Scope the project.\nCollect data, Define and collect data.\nTrain model: Training, error analysis, iterative improvement.\nDeploy in production: Deploy, monitor and maintain system.\nImploement ML model to a inference server, Mobile app make API call to server, server inferences to mobile app.\nSoftware engineering may be needed for:\nEnsure reliable and efficient predictions Scaling Logging System monitoring Model updates MLOps: ML operations: Practice how to systematically build and deploy, maintain ML model.\nP89 Ethics, Pairness, bias, and other ethics.\nP90 Skewed datasets\nCannot know the best model based on its accuracy rate, because the dataset might be skewed.\nUse confusion matrix is a better matrix. Precision/recall\nPrecision: TP/#predicted positive = TP/(TP + FP)\nRecall: TP/#actual postive = TP / (TP + FN )\nP91 Trading off precision and recall\nRasing the logstic regression threshold will lead to higher precision, lower recall.\nLower the logistic regression will result in lower precision, higher recall.\nF1 score, F1 score = 1/[ 1/2 (1/P + 1/R) ] = 2 PR / (P+R); Harmonic mean of P and R.\nP91 Decision Trees\nCat classification example\nInput are categorical values (discrete)\nNode, Topmost node is root node, Decision nodes in the middle of tree, Bottom node is leaf nodes for prediction.\nP92\nDecision 1: how to choose what feature to split on at each node?\nMaximize purity (or minimize impurity). (Use the most important feature)\nDecision 2: When do you stop splitting?\nWhen a node is 100% one class When splitting a node will result in the tree exceeding a maximum depth. When improvements in purity score are below a threshold. When number of examples in a node is below a threshold. P93 Measuring purity\nEntropy as a measure of impurity\nEntropy is from 1 to 0, the lower the entropy, the higher the purity.\n$p_0 = 1 - p_1$\n$H(p_1)=-p_1\\log_2(p_1)-p_0\\log_2(p_0)$\nNote: $0\\log(0) = 0$\nThe peak of the function is 1 by making the log base is 2.\nP94: Decision Tree learning, choose a split: information gain\nChoose the feature with the lowest average weighted entropy.\nInformation gain.\nP96: Putting it together\nStart with all examples at the root node Calculate information gain for all possible features, and pick the one with the highest information gain Split dataset according to selected feature, and create left and right branches of the tree Keep repeating splitting process until stopping criteria is met When a node is 100% one class When splitting a node will result in the tree exceeding a maximum depth Information gain from additional splits is less than threshold When number of examples in a node is below a threshold. P97: Using one-hot encoding of categorical features\nIf a categorical feature can take on k values, create k binary features.\nP98: Continuous features\nP99: Regression tree\nChoose the feature that split the data with lower variance.\nCalculate the weighted variance of a feature to split on.\nUse variance reduction as a measurement. Use the feature with largest variance reduction to split data.\nP100: Tree ensembles\nTrees are highly sensitive to small changes of the data. Using multiple trees and vote for the final result makes prediction more robust.\nP101: Sampling with replacement\nTo construct a new traning set with a little bit similar but also pretty different from origin training set.\nP102: Random forest algorithm\nOne powerful decision tree (Trees bag) algorithm.\nGiven training set of size $m$\nFor $b=1$ to $B$:\nUse sampling with replacement to create a new training set of size $m$; Train a decision tree on the new dataset.\nRandomizing the feature choice: At each node, when choosing a feature to use to split, if $n$ features are available, pick a random subset of $k\u0026lt;n$ features and allow the algorithm to only choose from that subset of features. (e.g. $k=sqrt(n)$)\nP103: XGBoost decision tree\nensamble /ˌɑːnˈsɑːm.bəl/\nIt runs quickly, open source implementations are easily used.\nGiven training set of size $m$\nFor $b=1$ to $B$:\nUse sampling with replacement to create a new training set of size $m$ But instead of picking from all examples with equal (1/m) probability, make it more likely to pick examples that the previously trained trees misclassify.\nTrain a decision tree on the new dataset.\nXGBoost (extreme gradient boosting)\nOpen source implementation of boosted trees Fast efficient implementation Good choice of default splitting criteria and criteria for when to stop splitting Built in regularization to prevent overfitting Highly competitive algorithm for machine learning competitions e.g. Kaggle Sample for classfication:\n1from xgboost import XBGClassifier 2 3model = XGBClassifier() 4 5model.fit(x_train, y_train) 6y_pred = model.predict(x_test) Regression:\n1model = XGBegressor() 2model.fit(x_train, y_train) 3y_pred = model.predict(x_test) P104 Decision Trees vs Neural Networks\nDecision Trees and Tree ensembles\nWork well on tabular (structured) data Not recommended for unstructured data, images, audio, text Fast Small decision trees may be human interpretable Neural network\nWorks well on all types of data, including tabular and unstructured data. May be slower than a decision tree Works with transfer learning When building a system of multiple models working together, it might easier to string together multiple neural networks. C3-Unsupervised Learning P107 Clustering\nApplications: Grouping similar news, Market segmentation, DNA analysis, Astronomical data analysis.\nP108 K-means clustering\nCluster centroid.\nRepeat until converged:\nStep 1: Assign each point to its closest centroid.\nStep 2: Recompute the centroids.\nP109 K-means algorithms\nRandomly initialize $K$ cluster centroids $\\mu_1, \\mu_2, ...,\\mu_k$.\nRepeat{\n// Assign points to cluster centroids\nfor $i=1$ to $m$ training examples\n$c^{(i)}:=$index (from 1 to K) of cluster centroid closest to $x^{(i)}$\n// Move cluster centroids\nfor $k=1$ to K,\n$\\mu_k:=$ average of points assigned to cluster $k$\n}\nP110 Clustering, Optimazation objective\n$c^{(i)}=$ index of cluster to which example $x^{(i)} is currently assigned$\n$\\mu_k=$ cluster centroid $k$\n$\\mu_c(i)=$ cluster centroid of cluster to which example $x^{(i)}$ has been assigned\nCost function (Distortion function)\n$$J(c, \\mu) = \\frac{1}{m}\\sum_{i=1:m}||x-\\mu_{c}||$$\nP111 Initilizing K-means\nRandomly initialization\nChoose $K\u0026lt;m$, clusters number is smaller than traing exampings.\nRandomly pick K training examples, set centroids equal to these K examples.\nTo avoid local optima, can run K-means multiple times, and pick the clustering result that gave the lowest cost.\nP112 Choossing the number of clusters.\nElbow method: Try K clusters and plot the cost function value, pick the point looks like elbow position.\nMore common method: Evaluate K-means based on a metric for how well it performs for that later purpose.\nP113 Anomaly detection\nDensity estimation\nExample:\nFraud detection, Model features of user;s activities from data. Identify unusual users by checking which have less probability.\nP114 Gaussian Distribution\n$x$ is a distributed Gaussian with mean $\\mu$, $\\sigma^2$ variance. $\\sigma$ is standard deviation.\n$p(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}$\n$\\mu=\\frac{1}{m}\\sum x_i$\n$\\sigma^2=\\frac{1}{m}\\sum(x_i-\\mu)^2$\nP115 Algorithm\nTraining set: ${x^{(1)},x^{(2)},...,x^{(m)}}$ Each example $x_i$ has $n$ features.\n$p(x)=\\Pi_{j=1}^{n} p(x_j;\\mu_j,\\sigma_j^2)$\nChoose $n$ features $x_i$ that you think might be indicative of anomalous examples.\nFit parameters $\\mu_1,...,\\mu_n,\\sigma_1^2,...\\sigma_n^2$\n$\\mu_j=\\frac{1}{m}\\sum x_j^{(i)}$\n$\\sigma_j^2=\\frac{1}{m}(x_j^{(i)}-\\mu_j)^2$\nGiven a new example $x$, compute\n$p(x)=\\Pi_{j=1}^np(x_j;\\mu_j,\\sigma_j^2)$\nAnomaly if $p(x)\u0026lt;\\epsilon$\nP116 Developing and evaluatiing an anomly detection system\nHave some labled data.\nTraining set is unlabled dataset, assume those are normal (or anomalous).\nCross validation set.\nTest set.\nCross validation and test sets include a few anomalous examples.\nAircraft engines monitoring example\n10000 good engines, 20 flawed engines\nTraining set: 6000 good engines.\nCV: 2000 good engines, 10 anomalous\nTest: 2000 good engines, 10 anomalous\nTrain the algorithm using training set, verify the anomly detection performance on CV. Tuning $\\epsilon$ on CV. Test result on Test set.\nAlternative:\nTraining set: 6000 good engines; CV: 4000 good engines, 20 anomalous; No test set.\nAnomaly Detection VS. Supervised Learning\nAnomaly detection\nVery small number of positive examples. (0-20 is common), Large number of negative examples\nMany different types of anomalies. hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous examples we've seen so far.\nFraud detection\nManufacturing - finding new previously unseen defects.\nMonitoring machines in a data center.\nSupervised learning\nLarge number of positive and negative examples.\nEnough positive examples for algorithm to get a sense of what positive examples are like, future positve example likely to be similar to ones in training set.\nEmail spam classification.\nManufacturing - Finding known, previously seen defects.\nWeather prediction.\nDeseases classification.\nP118 Choosing what features to use\nNon-gaussian features\nTransform feature to more like Gaussian distribution.\nLog, Square, Sqrt, etc.\nError analysis for anomaly detection\nChoose features that might take on unusually large or small values in the event of an anomaly.\nP120 Recommended System\n","link":"http://localhost:1313/post/dl/005-ng-ml/","section":"post","tags":null,"title":"Note of NG's Machine Learning Course"},{"body":"Probability Knowledge Based on Deep Learning (2017, MIT) book.\n1 Overview Probability theory is a mathematical framework for representing uncertain statements. We use probability theory in two major ways in AI domain. First, the laws of probability tell us how AI systems should reason, so we design algorithms to compute or approximate various expressions derived using probability theory. Second, we can use probability and statistics to theoretically analyze the behavior of proposed AI systems.\n2 Knowledge 2.1 Discrete Variables and Probability Mass Function (PMF) A probability distribution over discrete variables can be described using a probability mass function (PMF). Descrete variable $x$ follows distribution $P(x)$: $\\mathrm{x}\\sim P(x)$.\nJoint probability distribution is a probability distribution over many variables: $P(\\mathrm{x}=x, \\mathrm{y}=y)$, or $P(x,y)$.\nProperties of PMF:\nThe domain of $P$ must be the set of all possible states of $\\mathrm{x}$. $\\forall x\\in \\mathrm{x}, 0\\leq P(x) \\leq 1$. $\\sum_{x\\in \\mathrm{x}}P(x)=1$. Uniform distribution: $P(\\mathrm{x}=x_i)=\\dfrac{1}{K}$.\n2.2 Continuous Variables and Probability Density Functions (PDF) Probability density function (PDF) is used for describing the probability distributions of continuous random variables. The function $p$ of a PDF must satisfy the following properties:\nThe domain of $p$ is the set of all possible states of $\\mathrm{x}$. $\\forall x\\in \\mathrm{x}, p(x)\\geq0$. Note do not require $p(x)\\leq 1$. $\\int p(x)dx=1$. PDF is not the probability, and PDF is not the same thing as PMF, PDF can be greater than 1. Discrete and continuous random variables are not defined the same way. For the continuous random variable, the necessary condition is $\\int p(x)dx=1$. A PDF does not give the probability of a specific state directly, instead the probability of landing inside an infinitesimal region with volume $\\delta x$ is given by $p(x)\\delta x$. The probability that $x$ lies in the interval $[a,b]$ is given by $\\int_{[a,b]}p(x)dx$.\nUniform distribution $u(x;a,b)=\\dfrac{1}{b-a}$, $a$ and $b$ are the endpoints of the interval. The $;$ notation means $parameterized by$. $x$ is the argument of the function, $a$ and $b$ are parameters. $x\\sim U(a,b)$ denotes that $x$ follows the uniform distrubtion.\n2.3 Marginal Probability The probability distribution over a subset of variables is known as the marginal probability distrubtion. E.g., discrete random variables $\\mathrm{x}$ and $\\mathrm{y}$, $P(\\mathrm{x},\\mathrm{y})$ is known, the $P(\\mathrm{x})$ can be computed with the sum rule: $\\forall x\\in \\mathrm{x}, P(\\mathrm{x}=x)=\\sum_{y}P(\\mathrm{x}=x, \\mathrm{y}=y)$. For continuous variables, need to use integration instead of summation: $p(x)=\\int p(x,y)dy$.\n2.4 Conditional Probability Calculate the probability of some event, given that some other event has happend. This is the conditional probability. $P(\\mathrm{y}=y|\\mathrm{x}=x)$, $\\mathrm{x}=x$ is the condition. It can be computed with the formula $P(\\mathrm{y}=y|\\mathrm{x}=x)=\\dfrac{P(\\mathrm{y}=y,\\mathrm{x}=x)}{P(\\mathrm{x}=x)}$.\nThe conditional probability is only defined when $P(\\mathrm{x}=x)\u0026gt;0$. We cannot compute the conditional probability conditioned on an event that never happens.\n2.5 Chain Rule of Conditional Probability Any joint probability distribution over many random variables may be decomposed into conditional distributions over only one variable, this is known as the chain rule or product rule. $P(\\mathrm{x}^{(1)},\\ldots,\\mathrm{x}^{(n)})=P(\\mathrm{x}^{(1)})\\Pi_{i=2}^nP(\\mathrm{x}^{(i)}|\\mathrm{x}^{(1)},\\ldots,\\mathrm{x}^{(i-1)})$.\nSome examples:\n$P(a,b,c)=P(a|b,c)P(b,c)$;\n$P(b,c)=P(b|c)P(c)$;\n$P(a,b,c)=P(a|b,c)P(b|c)P(c)$.\n2.6 Independence and Conditional Independence $x$ and $y$ are independent ($x\\perp y$) if: $\\forall x\\in \\mathrm{x}, y \\in \\mathrm{y}, p(\\mathrm{x}=x, \\mathrm{y}=y)=p(\\mathrm{x}=x)p(\\mathrm{y}=y)$.\nGiven a random variable $z$, $x$ and $y$ are conditionally independent ($x\\perp y|z$) if:\n$\\forall x\\in \\mathrm{x}, y\\in \\mathrm{y}, z\\in \\mathrm{z}, p(\\mathrm{x}=x, \\mathrm{y}=y, \\mathrm{z}=z)=p(\\mathrm{x}=x|\\mathrm{z}=z)p(\\mathrm{y}=y|\\mathrm{z}=z)$\n2.7 Expectation, Variance and Covariance Expectation\nFor discreate variables: $\\mathbb{E}{\\mathrm{x}\\sim P}[f(x)]=\\sum{x}P(x)f(x)$.\nFor continous variables: $\\mathbb{E}_{\\mathrm{x}\\sim P}[f(x)]=\\int{P(x)f(x)}dx$.\nExpectations are linear: $\\mathbb{E}{\\mathrm{x}}[\\alpha f(x)+\\beta g(x)]=\\alpha \\mathbb{E}{\\mathrm{x}}[f(x)] + \\beta \\mathbb{E}_{\\mathrm{x}}[g(x)]$\nVariance\n$Var(f(x))=\\mathbb{E}[(f(x)-\\mathbb{E}[f(x)])^2]$\nWhen the variance is low, the values of $f(x)$ cluster near their expected value. The square root of the variance is known as the standard deviation.\nCovariance\nCovariance gives sense of how much two values are linearly related to each other, as well as the scale of these variables: $Cov(f(x),g(y))=\\mathbb{E}[(f(x)-\\mathbb{E}[f(x)])(g(y)-\\mathbb{E}[g(y)])]$\nHigh absolute values of the covariance mean the values change very much and are both far from their respective means at the same time. Positive sign means both variables tend to take on relatively high values simultaneously. Negative sign means one variable is getting high value and the other is getting low value at the same time, and vice versa.\nCovariance and dependence are related:\nIndependent variables have zero covariance. Non-zero covariance's variables are dependent. Independece is a stronger requirement than zero covariance. It is possible for two variable to be dependent but have zero covariance. The covariance matrix of a random vector $x\\in \\mathbb{R}^n$ is an $n\\times n$ matrix: $Cov(\\mathbf{x})_{i,j}=Cov(\\mathbf{x}_i,\\mathbf{x}_j)$ The diagnal elements of the covariance give the variance: $Cov(\\mathbf{x}_i,\\mathbf{x}_i)=Var(\\mathbf{x}_i)$.\n2.8 Common Probability Distributions Several useful probabiliy distributions in machine learning.\nBernoulli Distribution\nDistribution over a single binary random variable. Properties:\n$P(\\mathbf{x}=1)=\\phi$, $p(\\mathbf{x}=0)=1-\\phi$ $P(\\mathbf{x}=x)=\\phi^x(1-\\phi)^{1-x}$ $\\mathbb{E}_{\\mathbf{x}}[\\mathbf{x}]=\\phi$ $Var_\\mathbf{x}(\\mathbf{x})=\\phi(1-\\phi)$ Multinoulli Distribution\nOr categorical distribution, is a distribution over a signle discrete variable with $k$ different states.\nGaussian Distribution\nOr Normal distribution: $\\mathcal{N}(x;\\mu,\\sigma^2)=\\sqrt{\\dfrac{1}{2\\pi \\sigma^2}}\\exp(-\\dfrac{1}{2\\sigma^2(x-\\mu)^2})$\n$\\mu$ gives the coordinate of the central peak, this is also the mean of the distribution: $\\mathbb{E}[\\mathbf{x}]=\\mu$ Standard deviation of the distribution: $\\sigma$ Variance: $\\sigma^2$ Exponential and Laplace Distributions\nExponential distribution: $p(x;\\lambda)=\\lambda 1_{x\\geq 0} \\exp(-\\lambda x)$\nTo all negative values of $x$, probability is zero.\nLaplace distribution: $Laplace(x;\\mu,\\gamma)=\\dfrac{1}{2\\gamma}\\exp(-\\dfrac{|x-\\mu|}{\\gamma})$\nDirac Distribution and Empirical Distribution\nDirac distribution: $p(x)=\\delta (x-\\mu)$\nEmpirical distribution: $\\hat{p}(x)=\\dfrac{1}{m}\\sum_{i=1}^m\\delta(x-x^{(i)})$\n2.9 Useful Properties of Common Functions Logistic sigmoid\n$\\sigma(x)=\\dfrac{1}{1+\\exp(-x)}$\nIt is commonly used to produce the $\\phi$ parameter of a Bermoulli distribution. The sigmoid function saturates when its argument is very positive or negative, meaning the function becoms very flat and insensitive to small changes in its input.\nSoftplus\n$\\zeta(x)=log(1+\\exp(x))$\nThe function can be useful for producing the $\\beta$ or $\\sigma$ parameter of a normal\nImportant properties\n$\\sigma(x)=\\dfrac{\\exp(x)}{\\exp(x)+1}$ $\\dfrac{d}{dx}\\sigma(x)=\\sigma(x)(1-\\sigma(x))$ $1-\\sigma(x)=\\sigma(-x)$ $\\log\\sigma(x) = -\\zeta(-x)$ $\\dfrac{d}{dx}\\zeta(x)=\\sigma (x)$ $\\forall x\\in (0,1), \\sigma^{-1}(x)=\\log(\\dfrac{x}{1-x})$ $\\forall x \u0026gt; 0, \\zeta^{-1}(x)=\\log (\\exp(x)-1)$ $\\zeta(x)=\\int_{-\\infin}^{x}\\sigma(y)dy$ $\\zeta (x) - \\zeta(-x) = x$ 2.10 Bayes' Rule $P(x|y)=\\dfrac{P(x)P(y|x)}{P(y)}$\nCalculate $P(x|y)$ via $P(y|x)$, note $P(y)=\\sum_xP(y|x)P(x)$. Bayes's rule is a way to figure out how likely something is to happen when have some old information.\n3 Application Questions Reference\nQ1: There is a fair coin (one side heads, one side tails) and an unfair coin (both sides tails). You pick one at random, flip it 5 times, and observe that it comes up as tails all five times. What is the chance that you are flipping the unfair coin?\\\nDefine $U$ is the case flipping the unfair coin; $F$ denotes flipping a fair coin. $5T$ denotes the event where we flip 5 heads in a row.\nWe know $P(U) = P(F) = 0.5$, need to solve the $P(U|5T)$. $$P(U|5T) = \\dfrac{P(5T|U)P(U)}{P(5T)}$$ $$=\\dfrac{10.5}{P(5T|U)P(U)+P(5T|F)P(F)}$$ $$=\\dfrac{0.5}{10.5+0.5^5*0.5}\\approx0.97$$ Therefore the probability picked the unfair coin is about 97%.\nQ2: You and your friend are playing a game. The two of you will continue to toss a coin until the sequence HH or TH shows up. If HH shows up first, you win. If TH shows up first, your friend wins. What are the probabilities of each winer?\nP(HH occurrs before TH) = P(HH in first two flips) = 1/4\nP(TH occurrs before HH) = P(first is T) + P(first two is HT) = 1/2 + 1/4 = 3/4\nQ3: 1/1000 people have a particular disease, and there is a test that is 98% correct if you have the disease. If you don’t have the disease, there is a 1% error rate. If someone tests positive, what are the odds they have the disease?\nP(D) = 1/1000 is the probability of having the disease\nP(H) = 1 - P(D) = 999/1000 is the probability of health\nP(P|D) = 98% is the probability of testing positive if having the disease\nP(P|H) = 1% is the probability of testing positive if do not have the disease\nNeed to solve P(D|P)\n$$P(D|P)=\\dfrac{P(P|D)P(D)}{P(P)}$$ $$= \\dfrac{98/100*1/1000}{P(P|D)P(D) + P(P|H)P(H)}$$ $$= \\dfrac{0.098%}{98%*1/1000 + 1% * 999/1000}$$ $$\\approx 8.94%$$\nSo, the probability that someone has the disease given that they tested positive is approximately 0.0894 or 8.94%.\n","link":"http://localhost:1313/post/dl/001-probability/","section":"post","tags":null,"title":"Probability Knowledge for Deep Learning"},{"body":"Application example\nPCA via Scikit-learn Use scikit-learn lib to perform PCA process to digit data. To compare the origin and reconstruced data difference, have a overall understanding.\nInput Data Import libs, and visualize the source input data.\n1# Import needed libs 2import numpy as np 3import matplotlib.pyplot as plt 4from sklearn.datasets import load_digits 5from sklearn.decomposition import PCA 6from sklearn.metrics import mean_squared_error 1# Load the digits dataset 2digits = load_digits() 3X = digits.data 4y = digits.target 5 6# Original data size 7original_size = X.nbytes / (1024 * 1024) # in megabytes 8print(\u0026#34;original data size is: %.2f MB\u0026#34; % original_size) original data size is: 0.88 MB 1# Plot the first 10 samples as images 2fig, axes = plt.subplots(1, 10, figsize=(12, 4)) 3for i in range(10): 4 axes[i].imshow(X[i].reshape(8, 8), cmap=\u0026#39;gray\u0026#39;) 5 axes[i].set_title(f\u0026#34;Label: {y[i]}\u0026#34;) 6 axes[i].axis(\u0026#39;off\u0026#39;) 7plt.tight_layout() 8plt.show() Reconstruction Result Define a series of function to perform reconstruction and reconstruction metric.\n1# Function to calculate reconstruction error 2def reconstruction_error(original, reconstructed): 3 return mean_squared_error(original, reconstructed) 4 5# Function to perform PCA and reconstruct data with n_components 6def perform_pca(n_components): 7 pca = PCA(n_components=n_components) 8 X_pca = pca.fit_transform(X) 9 X_reconstructed = pca.inverse_transform(X_pca) 10 return X_reconstructed, pca 1# Function to perform PCA, and visualize result. Input is the number of principle components 2def analyze_pca(n_components): 3 X_reconstructed, pca = perform_pca(n_components) 4 reconstruction_error_val = reconstruction_error(X, X_reconstructed) 5 print(f\u0026#34;Number of Components: {n_components}, Reconstruction Error: {reconstruction_error_val}\u0026#34;) 6 7 # Size of compressed file 8 compressed_size = (pca.components_.nbytes + pca.mean_.nbytes + X_reconstructed.nbytes) / (1024 * 1024) # in megabytes 9 print(f\u0026#34;Size of Compressed File: {compressed_size} MB\u0026#34;) 10 11 # Difference in size 12 size_difference = original_size - compressed_size 13 print(f\u0026#34;Difference in Size: {size_difference} MB\u0026#34;) 14 15 # Plot original and reconstructed images for each digit 16 fig, axes = plt.subplots(2, 10, figsize=(10, 2)) 17 for digit in range(10): 18 digit_indices = np.where(y == digit)[0] # Indices of samples with the current digit 19 original_matrix = X[digit_indices[0]].reshape(8, 8) # Take the first sample for each digit 20 reconstructed_matrix = np.round(X_reconstructed[digit_indices[0]].reshape(8, 8), 1) # Round to one decimal place 21 axes[0, digit].imshow(original_matrix, cmap=\u0026#39;gray\u0026#39;) 22 axes[0, digit].axis(\u0026#39;off\u0026#39;) 23 axes[1, digit].imshow(reconstructed_matrix, cmap=\u0026#39;gray\u0026#39;) 24 axes[1, digit].axis(\u0026#39;off\u0026#39;) 25 26 plt.suptitle(f\u0026#39;Reconstruction with {n_components} Components\u0026#39;) 27 plt.show() 28 29 # Print the first data\u0026#39;s matrix 30 print(\u0026#34;Original Matrix of the First Data:\u0026#34;) 31 print(original_matrix) 32 33 # Print the reconstruction matrix 34 print(\u0026#34;\\nReconstruction Matrix of the First Data:\u0026#34;) 35 print(reconstructed_matrix) Analyze the result when we use one principle component\n1analyze_pca(1) Number of Components: 1, Reconstruction Error: 15.977678462238496 Size of Compressed File: 0.87841796875 MB Difference in Size: -0.0009765625 MB Original Matrix of the First Data: [[ 0. 0. 11. 12. 0. 0. 0. 0.] [ 0. 2. 16. 16. 16. 13. 0. 0.] [ 0. 3. 16. 12. 10. 14. 0. 0.] [ 0. 1. 16. 1. 12. 15. 0. 0.] [ 0. 0. 13. 16. 9. 15. 2. 0.] [ 0. 0. 0. 3. 0. 9. 11. 0.] [ 0. 0. 0. 0. 9. 15. 4. 0.] [ 0. 0. 9. 12. 13. 3. 0. 0.]] Reconstruction Matrix of the First Data: [[-0. 0.4 6.4 12.6 12. 6.3 1.4 0.1] [ 0. 2.6 11.7 11.2 10.5 9.4 1.9 0.1] [ 0. 3. 9.4 5.8 8. 8.7 1.6 0. ] [ 0. 2.1 7.7 9. 11.1 7.8 2. 0. ] [ 0. 1.5 5.6 8.2 9.8 8.5 2.8 0. ] [ 0. 1. 5.2 5.9 6.5 8.2 3.7 0. ] [ 0. 0.8 7.8 9. 8.8 9.5 4.1 0.2] [ 0. 0.4 6.8 12.9 11.9 7.3 2.3 0.4]] Analyze when we choose 5 principle components.\n1analyze_pca(5) Number of Components: 5, Reconstruction Error: 8.542447616249266 Size of Compressed File: 0.88037109375 MB Difference in Size: -0.0029296875 MB Original Matrix of the First Data: [[ 0. 0. 11. 12. 0. 0. 0. 0.] [ 0. 2. 16. 16. 16. 13. 0. 0.] [ 0. 3. 16. 12. 10. 14. 0. 0.] [ 0. 1. 16. 1. 12. 15. 0. 0.] [ 0. 0. 13. 16. 9. 15. 2. 0.] [ 0. 0. 0. 3. 0. 9. 11. 0.] [ 0. 0. 0. 0. 9. 15. 4. 0.] [ 0. 0. 9. 12. 13. 3. 0. 0.]] Reconstruction Matrix of the First Data: [[ 0. 0.2 5.2 11.1 12.1 7. 1.6 0.1] [ 0. 2.1 11.2 10.7 9.7 9.6 2.3 0.2] [ 0. 3.1 11.2 6.2 6. 9.2 2.5 0.1] [ 0. 3.1 10.3 9. 9.6 9.6 2.9 0. ] [ 0. 2.2 6. 5.3 8. 11.6 3.9 0. ] [ 0. 1.2 4.2 1.9 4.9 11.7 5.1 0. ] [ 0. 0.6 6.7 6.2 8.8 12.1 4.4 0.2] [ 0. 0.2 5.4 12.1 13.4 8.2 1.8 0.3]] The more the principle components used, the better the reconstruction result. Next we will mannualy compute the PCA matrix.\nManual PCA Maunal step-by-step way to perform the PCA analysis.\nInput Data Show the input data.\n1# Then use step-by-step wat to calculate the PCA steps; 2# Take the first data point for analysis 3first_data = X[0] 4print(\u0026#34;Raw input data: \\n\u0026#34;, X[0]) 5# Reshape the data point into a 2D array (image) 6input_matrix = first_data.reshape(8, 8) 7 8print(\u0026#34;Input matrix: \u0026#34;) 9for row in input_matrix: 10 print(\u0026#34; \u0026#34;.join(f\u0026#34;{val:4.0f}\u0026#34; for val in row)) 11 12# Print the original matrix (image) 13plt.imshow(input_matrix, cmap=\u0026#39;gray\u0026#39;) 14plt.title(\u0026#34;Input matrix (Image)\u0026#34;) 15plt.axis(\u0026#39;off\u0026#39;) 16plt.show() Raw input data: [ 0. 0. 5. 13. 9. 1. 0. 0. 0. 0. 13. 15. 10. 15. 5. 0. 0. 3. 15. 2. 0. 11. 8. 0. 0. 4. 12. 0. 0. 8. 8. 0. 0. 5. 8. 0. 0. 9. 8. 0. 0. 4. 11. 0. 1. 12. 7. 0. 0. 2. 14. 5. 10. 12. 0. 0. 0. 0. 6. 13. 10. 0. 0. 0.] Raw data shape: (64,) Input matrix: 0 0 5 13 9 1 0 0 0 0 13 15 10 15 5 0 0 3 15 2 0 11 8 0 0 4 12 0 0 8 8 0 0 5 8 0 0 9 8 0 0 4 11 0 1 12 7 0 0 2 14 5 10 12 0 0 0 0 6 13 10 0 0 0 Centering the Data This mean calculation helps us understand the average value of each feature, which is essential for centering the data and calculating the covariance matrix in subsequent steps. centering the data is a crucial preprocessing step in PCA that enhances the interpretability of the results, removes bias, and ensures numerical stability in computations.\n1# Step 1: Calculate the mean of each feature (column) 2mean_vec = np.mean(input_matrix, axis=0) 3print(mean_vec) [ 0. 2.25 10.5 6. 5. 8.5 4.5 0. ] 1# Step 2: Subtract the mean from each feature 2centered_matrix = input_matrix - mean_vec 3print(centered_matrix) [[ 0. -2.25 -5.5 7. 4. -7.5 -4.5 0. ] [ 0. -2.25 2.5 9. 5. 6.5 0.5 0. ] [ 0. 0.75 4.5 -4. -5. 2.5 3.5 0. ] [ 0. 1.75 1.5 -6. -5. -0.5 3.5 0. ] [ 0. 2.75 -2.5 -6. -5. 0.5 3.5 0. ] [ 0. 1.75 0.5 -6. -4. 3.5 2.5 0. ] [ 0. -0.25 3.5 -1. 5. 3.5 -4.5 0. ] [ 0. -2.25 -4.5 7. 5. -8.5 -4.5 0. ]] Covariance Calculation Calculate the Covariance matrix of centered data.\n1# Calculate covariance using np.dot. Bessel\u0026#39;s correction to minus 1 at the end. https://www.uio.no/studier/emner/matnat/math/MAT4010/data/forelesningsnotater/bessel-s-correction---wikipedia.pdf 2cov_matrix = np.dot(centered_matrix.T, centered_matrix) / (centered_matrix.shape[0] - 1) 3 4# Or calculate covariance using np.cov 5# cov_matrix = np.cov(centered_matrix, rowvar=False) 6 7print(cov_matrix) [[ 0. 0. 0. 0. 0. 0. 0. 0. ] [ 0. 4.21428571 2.28571429 -13.14285714 -9.42857143 4.14285714 6.14285714 0. ] [ 0. 2.28571429 14. -9.42857143 -4.85714286 17. 6.28571429 0. ] [ 0. -13.14285714 -9.42857143 43.42857143 29.57142857 -12.57142857 -17.85714286 0. ] [ 0. -9.42857143 -4.85714286 29.57142857 26. -7. -17.57142857 0. ] [ 0. 4.14285714 17. -12.57142857 -7. 28.85714286 11. 0. ] [ 0. 6.14285714 6.28571429 -17.85714286 -17.57142857 11. 14.85714286 0. ] [ 0. 0. 0. 0. 0. 0. 0. 0. ]] Eigen decomposition 1# Step 4: Compute the eigenvalues and eigenvectors of the covariance matrix 2eigenvalues, eigenvectors = np.linalg.eig(cov_matrix) 3 4print(eigenvalues) 5print(eigenvectors) [8.92158455e+01 3.14545089e+01 7.61850164e+00 2.85144338e+00 2.01453633e-01 1.53898738e-02 0.00000000e+00 0.00000000e+00] [[ 0. 0. 0. 0. 0. 0. 1. 0. ] [-0.20365153 0.09344175 0.07506402 -0.23052329 -0.41043409 -0.85003703 0. 0. ] [-0.22550077 -0.48188982 0.20855091 0.79993174 -0.1168451 -0.14104805 0. 0. ] [ 0.65318552 -0.28875672 -0.59464342 0.12374602 0.11324705 -0.32898247 0. 0. ] [ 0.48997693 -0.31860576 0.39448425 -0.20610464 -0.63307453 0.24399318 0. 0. ] [-0.33563583 -0.75773097 -0.0607778 -0.49775699 0.24837474 0.00681139 0. 0. ] [-0.35818338 -0.00212894 -0.66178497 0.03760326 -0.58531429 0.29955628 0. 0. ] [ 0. 0. 0. 0. 0. 0. 0. 1. ]] Select the principal component corresponding to the maximum eigenvalue\n1# Step 5: Choose the principal component corresponding to the maximum eigenvalue 2max_eigenvalue_index = np.argmax(eigenvalues) 3principal_component = eigenvectors[:, max_eigenvalue_index] 4print(principal_component) [ 0. -0.20365153 -0.22550077 0.65318552 0.48997693 -0.33563583 -0.35818338 0. ] 1# Step 6: Project the data onto the principal component 2reduced_data = np.dot(centered_matrix, principal_component) 3 4# Print the reduced data 5print(\u0026#34;Reduced data (1 principal component):\\n\u0026#34;, reduced_data) Reduced data (1 principal component): [12.35977044 5.86229378 -8.32285024 -8.14946302 -7.7867473 -8.41834525 1.49545914 12.95988243] So far, the data is compressed from 8x8 matrix to 8x1 vector.\nReconstruction Now reconstruc the data based on the reduced data.\n1# Step 7: Reconstruct the data 2reconstructed_data = np.dot(reduced_data.reshape(-1, 1), principal_component.reshape(1, -1)) 3 4# Step 8: Add back the mean to the reconstructed data 5reconstructed_data += mean_vec 6 7# Step 9: Visualize the original and reconstructed data 8fig, axes = plt.subplots(1, 2, figsize=(12, 6)) 9 10# Original data 11axes[0].imshow(input_matrix, cmap=\u0026#39;gray\u0026#39;) 12axes[0].set_title(\u0026#39;Original Data\u0026#39;) 13axes[0].axis(\u0026#39;off\u0026#39;) 14 15# Reconstructed data 16axes[1].imshow(reconstructed_data.real, cmap=\u0026#39;gray\u0026#39;) 17axes[1].set_title(\u0026#39;Reconstructed Data\u0026#39;) 18axes[1].axis(\u0026#39;off\u0026#39;) 19 20plt.show() ","link":"http://localhost:1313/post/math/003-pca-application/","section":"post","tags":null,"title":"Principle Components Analysis (PCA) application"},{"body":"Based on Deep Learning (2017, MIT) book.\n1 Overview Mordern dataset such as web indexing, high resolution image, meteorology, experimental measurement, etc., often contains high dimensionality features that they can be unclear, redundant, or even deceptive. It is challenging to visualize and interpret the relationships between variables, and the trained neural netwokr models with such high dimensionaly data are tend to overfitting (curse of dimensionality). Principle Components Analysis (PCA) is one simple but powerful unsupervised machine learning technique for data dimensionality reduction. It aims to extract a smaller dataset from a large variable set while preserving as much origin information and feature as possible (lossy compression). PCA helps identify the most significant and meaningful features in a dataset, making the data easy for visualization. The applications includes: statistical, noise removal, and prepocessing data for machine learning algorihtms.\nWhat is principle component?\nPrincipal components are new variables that are constructed as linear combinations of the origin variables. These new variables are uncorrelated and contain the most of the information from the original data. 2 Backgroud Mathmetic Knowledge Those knowledge is required for next section.\nOrthogonal vector and matrix: Two vectors are orthogonal if they are perpendicular to each other. i.e. the dot product of the two vectors is zero. Orthogonal matrix square matrix in which its rows and columns are mutually orthogonal unit vectors; Every two rows and two columns have zero dot product, and every row and every column has a magnitude of one. $A$ is orthogonal matrix if $A^{T} = A^{-1}$ or $AA^{T}= A^{T}A = I$. In robotics, a rotation matrix is typical a $3\\times3$ orthogonal matrix, in spatial transformation it rotates vector's direction but keeps the magnitude of origin vector. Matrix, vector multiplication rules: $(AB)^T=B^TA^T$, transpose of the product of two matrices. $\\vec{a}^T\\vec{b}=\\vec{b}^T\\vec{a}$, both sides are multiplication of vector, and results are scalars, scalar's transpose is same. $(A + B)C = AC + BC$, matrice's multiplication is distributive. $AB \\not ={} BA$, matrice's multiplication is not commutative. $A(BC)=(AB)C$, matrice's multiplication is associative. Symmetric matrix: $A=A^T$, $A$ is a symmetric matrix. $X^TX$ is a symmetric matrix, since $(X^TX)^T=X^TX$. Vector derivatives rules ($B$ is constant matrix): $d(x^TB)/dx=B$ $d(x^Tx)/dx=2x$ $d(x^TBx)/dx=2Bx$ Matrix trace rules: $Tr(A)=Tr(A^T)$ $Tr(AB)=Tr(BA)$ $Tr(A)=\\sum_i{\\lambda_i}$, where $\\lambda$ is eigen values of $A$. The trace is invariant under circular shifts: $Tr(ABCD)=Tr(BCDA)=Tr(CDAB)=Tr(DABC)$ Vector and matrix norms: Vector's $L^2$ norm, also known as Euclidean norm: $||x||_2=\\sqrt{\\sum_i|x_i|^2}$. It is also common to measure the size of a vector using the squraed $L^2$ norm, which can be calculated as $x^Tx$. Frobenius norm is used to measure the size of a matrix: $||A||F=\\sqrt{\\sum{i,j}A^2_{i,j}}$ Frobenius norm is square root of the sum of the absolute squares of all matrix's elements. Frobenius norm is the matrix version Euclidean norm. Eigendecomposition and eigenvalue: An eigenvector of a squre matrix $A$ is a non-zero vector $v$ such that multiplication by $A$ alters only the scale of $v$: $Av=\\lambda v$. $\\lambda$ is eigenvalue and $v$ is eigenvector. Suppose matrix $A$ has $n$ linearly independent eigenvectors $v^{(i)}$, we can conctenate all of the eigenvectors to form a matrix $V=[v^{(1)},\\ldots,v^{(n)}]$, and form a vector by concatenating all eigenvalues $\\lambda=[\\lambda_1,\\ldots,\\lambda_n]^T$, then the eigendecomposition of $A$ is $A=Vdiag(\\lambda)V^{-1}$ Every real symmetric matrix can be decompsed into $A=Q\\Lambda Q^T$, where $Q$ is an orthogonal matrix composed of eigenvectors of $A$, and $\\Lambda$ (pronoucation 'lambda') is a diagnoal matrix. Lagrange multipliers: Lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to equation constraints. General form: $\\mathcal{L}(x,\\lambda)=f(x)+\\lambda\\cdot g(x)$, $\\lambda$ is called Lagrange multiplier. 3 Detailed PCA Derivation Requirement description\nWe have input data with $m$ points ${x^{(1)},...,x^{(m)}}$ in $\\mathbb{R}^{n}$ real number sets. So each points $x^{(i)}$ is a colomn vector which has $n$ dimensionalities feature.\nWe need to apply lossy compression to the input data, encoding these points to represent a lower-dimensional version of them. In other words, we want to know the code vector $c^{(i)}\\in \\mathbb{R}^{l}$, $(l\u0026lt;n)$ to represent each input point $x^{(i)}$. Our target is to find the encoding function $f(x)=c$ producing the code vector for the input, and the correspoinding reconstruction (decoding) fuction $x\\approx g(f(x))$, computing the orogin input based on the code vector $c$.\nThe decoded $g(f(x))$ is a new set of points(variables), so it \u0026quot;$\\approx$\u0026quot; to the orgin $x$. Sotring $c^{(i)}$ and decoding function is cheaper in memroy than storing $x^{(i)}$ since $c^{(i)}$ has less dimenstionality.\nDecoding matrix\nWe choose to use matrix $D$ as the decoding matrix, mapping the code vector $c^{(i)}$ back to $\\mathbb{R}^{n}$, so $g(c)=Dc$, where $D\\in \\mathbb{R}^{n\\times l}$. To keep the encoding proble easy, PCA constrains the columns of $D$ to be orthogonal to each other.\nMeasure reconstruction performance\nBefore going on, we need to figure out how to generate the optimal code point $c^{}$, we can measure the distance between the input point $x$ and its reconstruction $g(c^)$, using $L^2$ norm (or Euclidean norm): $c^{}=\\arg\\min_c||x-g(c)||_2$. Since $L^2$ norm is non-negative, and squaring operation is monotonically increasing, so we can switch to use squred $L^2$ norm: $$c^{}=\\argmin_c||x-g(c)||_2^2$$\n$L^2$ norm of a vector is the sum of the squares of its components, it equals to the dot product of he vector with itself e.g. $||x||_2=\\sqrt{\\sum|x_i|^2}=\\sqrt{x^Tx}$, so the squared $L^2$ norm can be wrote in the form: $$||x-g(c)||_2^2 = (x-g(c))^T(x-g(c))$$ By the distributive property: $$=(x^T-g(c)^T)(x-g(c))=x^Tx-x^Tg(c)-g(c)^Tx+g(c)^Tg(c)$$ Since $x^Tg(c)$ and $g(c)^Tx$ are scalar, and scalar is equal to its transpose, $(g(c)^Tx)^T=x^Tg(c)$, so: $$=x^Tx-2x^Tg(c)+g(c)^Tg(c)$$\nTo find the $c$ minimizing the above function, the fiirst term can be omitted since it does not depend on $c$, so: $$c^*=\\argmin_c-2x^Tg(c)+g(c)^Tg(c)$$\nThen substitute in the definition of $g(c)$ with $Dc$: $$=\\argmin_c-2x^TDc+c^TD^TDc$$ By the orthogonality and unit norm constraints on $D$: $$c^*=\\argmin_c-2x^TDc+c^TI_lc$$ $$= \\argmin_c-2x^TDc+c^Tc$$\nObjective function\nNow the objective function is $-2x^TDc+c^Tc$, where we need to find the $c^*$ to minimize the objective function. Using vector calculus and let its derivative equals to 0: $$\\nabla_c(-2x^TDc+c^Tc)=0$$ According to vector derivative rules: $$-2D^Tx+2c=0 \\Rightarrow c=D^Tx$$\nFind the encoding matrix $D$\nSo the encoder function is $f(x)=D^Tx$. Therefore we can define the PCA reconstruction operation as $r(x)=g(f(x))=D(D^Tx)=DD^Tx$.\nSo the encoding matrix $D$ is also used by reconstruction process. We need to find the optimal $D$ to minimize the reconstruction error, the distance between all dimentional feature of inputs and reconstructions. Here use Frobenius norm (matrix norm) define the objective function:\n$$D^*=\\argmin_D\\sqrt{\\sum_{i,j}(x_j^{(i)}-r(x^{i})_j)^2},\\quad D^TD=I_l$$\nStart by considering the case where $l=1$ (this is the first principle component), $D$ is a single vector $d$, and use squared $L^2$ norm form:\n$$d^*=\\argmin_d{\\sum_{i}||(x^{(i)}-r(x^{i}))}||_2^2, ||d||_2=1$$\n$$d^*= \\argmin_d{\\sum_{i}||(x^{(i)}-dd^Tx^{(i)})||_2^2}, ||d||_2=1$$\n$d^Tx^{(i)}$ is a scalar:\n$$= \\argmin_d{\\sum_{i}||(x^{(i)}-d^Tx^{(i)}d)}||_2^2, ||d||_2=1$$\nScalar is its own transpose: $$d^*= \\argmin_d{\\sum_{i}||(x^{(i)}-x^{(i)T}dd)}||_2^2, ||d||_2=1$$\nUse matrix form represent\nLet $X\\in \\mathbb{R}^{m\\times n}$ defined by stacking all of the vectors describing the points ${x^{(1)^T}, x^{(2)^T}, \\ldots, x^{(i)^T}, \\ldots, x^{(m)^T}}$, such that $X_{i,:}=x^{(i)^T}$. $$ X = \\begin{bmatrix} x^{(1)^T}\\ x^{(2)^T}\\ \\ldots\\ x^{(m)^T} \\end{bmatrix} \\Rightarrow Xd = \\begin{bmatrix} x^{(1)^T}d\\ x^{(2)^T}d\\ \\ldots\\ x^{(m)^T}d \\end{bmatrix} $$ $$ \\Rightarrow Xdd^T = \\begin{bmatrix} x^{(1)^T}dd^T\\ x^{(2)^T}dd^T\\ \\ldots\\ x^{(m)^T}dd^T\\ \\end{bmatrix} $$\n$$ \\Rightarrow X-Xdd^T = \\begin{bmatrix} x^{(1)^T}-x^{(1)^T}dd^T\\ x^{(2)^T}-x^{(2)^T}dd^T\\ \\ldots\\ x^{(m)^T}-x^{(m)^T}dd^T\\ \\end{bmatrix} $$\nOne row's transpose in the matrix: $$(x^{(i)^T}-x^{(i)^T}dd^T)^T=x^{(i)}-dd^Tx^{(i)}$$ Since $d^Tx^{(i)}$ is a scalar: $$=x^{(i)}-d^Tx^{(i)}d=x^{(i)}-x^{(i)^T}dd$$\nSo we know the $L^2$ norm of the $i$-th row of $X$ is same as the origin form，therefore we can rerwrite the problem and omit sum sign using the matrix as: $$d^*=\\argmin_{d}||X-Xdd^T||_F^2, \\quad d^Td=1 $$\nSimplify the Frobenius norm portion with matrix trace rules as follows:\n$$\\argmin_{d}||X-Xdd^T||_F^2$$\n$$=\\argmin_{d}Tr((X-Xdd^T)^T(X-Xdd^T))$$ $$=\\argmin_{d}-Tr(X^TXdd^T)-Tr(dd^TX^TX)+Tr(dd^TX^TXdd^T)$$ $$=\\argmin_{d}-2Tr(X^TXdd^T)+Tr(X^TXdd^Tdd^T)$$\nDue to $d^Td=1$: $$=\\argmin_{d}-2Tr(X^TXdd^T)+Tr(X^TXdd^T)$$ $$=\\argmin_{d}-Tr(X^TXdd^T)$$ $$={\\arg\\max}_{d}Tr(X^TXdd^T)$$\nSince the trace is cyclic permutation invariant, rewrite the equation as: $$d^*={\\arg\\max}_{d}Tr(d^TX^TXd), \\quad d^Td=1$$\nDue to the fact $d^TX^TXd$ is a real number, so the trace sign can be omitted: $$d^*={\\arg\\max}_{d}d^TX^TXd,\\quad d^Td=1$$\nFind the optimal $d$\nNow the problem is to find the optimal $d$ to maximize $d^TX^TXd$ with constraint $d^Td=1$.\nRepresent the problem with Lagrange multiplier form w.r.t. $d$: $$\\mathcal{L}(d,\\lambda)=d^TX^TXd+\\lambda(d^Td-1)$$\nDerivate with respect to $d$ (vector derivatives rules): $$\\nabla_d\\mathcal{L}(d,\\lambda)=2X^TXd+2\\lambda d$$\nLet the derivation equal to 0, the $d$ will be the optimal one: $$2X^TXd+2\\lambda d=0$$ $$X^TXd=-\\lambda d$$ $$X^TXd=\\lambda' d,\\quad(\\lambda'=-\\lambda)$$\nThe equation is typical eigen decomposition form, the $d$ is the eigen vector of $X^TX$ and the $\\lambda'$ is the corresponding eigen value.\nWith above, let's recheck the origin equation:\n$$d^*={\\arg\\max}_{d}d^TX^TXd, \\quad d^Td=1$$\n$$={\\arg\\max}_{d}d^T\\lambda' d$$\n$$={\\arg\\max}_{d}\\lambda'd^T d$$\n$$={\\arg\\max}_{d}\\lambda'$$\nNow things are very clear, the largest eigenvalue of $X^TX$ maximizes the result, so the optimal $d$ is given by the eigenvector of $X^TX$ corresponding to the largest eigenvalue.\nThe derivation is specific to the case of $l=1$ and recovers only the first principla component. When $l\u0026gt;1$, $D=[d_1, d_2, \\ldots]$ the first principle component $d_1$ is the eigenvector of matrix $X^TX$ corrsponding to the largest eigenvalue, the second principle component $d_2$ is the eigenvector corresponding to the second largest eigenvalue, etc.\n4 Summary We have a dataset consisting of $m$ points denoted as ${x^{(1)},...,x^{(m)}}$. Let $X\\in \\mathbb{R}^{m\\times n}$ be defined by stacking all of these points as rows: ${x^{(1)^T}, x^{(2)^T}, \\ldots, x^{(i)^T}, \\ldots, x^{(m)^T}}$.\nThe Principal Component Analysis (PCA) encoding function is represented as $f(x)=D^Tx$, and reconstruction function as $x\\approx g(c)=Dc$, where the columns of $D=[d_1, d_2, \\ldots]$ are the eigenvectors of $X^TX$ corrresponding the eigenvalues in descending order.\nReconstruction matrix's column vectors are the eigenvectors of $X^TX$, corresponding to the eigenvalues arranged in descending order.\n","link":"http://localhost:1313/post/math/003-pca/","section":"post","tags":null,"title":"Detailed Deep Derivation of Principle Components Analysis (PCA)"},{"body":"Dockerfile examples\nKeywords FROM: specifies the base image upon which the new image will be built. Every Dockerfile must start with a FROM instruction. For example, FROM ubuntu:20.04 would instruct Docker to use the Ubuntu 20.04 base image.\nRUN: This keyword executes commands within the container during the image build process. For example, RUN apt-get update \u0026amp;\u0026amp; apt-get install -y python3 would update the package list and install Python 3 inside the container.\nCOPY / ADD: These keywords are used to copy files from the host machine into the container. COPY is preferred for simple file copying, while ADD has some additional features like unpacking tar archives. For example, COPY . /app would copy the contents of the current directory into a directory named /app in the container.\nWORKDIR: This sets the working directory for any RUN, CMD, ENTRYPOINT, COPY, and ADD instructions that follow it in the Dockerfile. For example, WORKDIR /app would set the working directory to /app.\nCMD: This specifies the default command to run when the container starts. Unlike RUN, which runs during the build process, CMD is executed when the container is launched. You can only have one CMD instruction in a Dockerfile. For example, CMD [\u0026quot;python3\u0026quot;, \u0026quot;app.py\u0026quot;] would run the Python script app.py when the container starts.\nENTRYPOINT: Similar to CMD, ENTRYPOINT specifies the command to run when the container starts. However, it differs in that it cannot be overridden at runtime, whereas CMD can be overridden by passing arguments to docker run. For example, ENTRYPOINT [\u0026quot;python3\u0026quot;, \u0026quot;app.py\u0026quot;] would run the Python script app.py when the container starts, and additional arguments could be appended to this command when running the container.\nEXPOSE: This keyword informs Docker that the container listens on specific network ports at runtime. It does not actually publish the ports. For example, EXPOSE 80 would indicate that the container listens on port 80.\nDockerfile with Conda Install and activate a Conda virtual envrionment.\n1ARG ARCH_IMG 2ARG CI_PROJECT_DIR 3FROM $ARCH_IMG 4ARG PACKAGE 5ARG LAUNCHFILE 6 7ENV env_launchfile=${LAUNCHFILE} 8ENV env_launchdir=${LAUNCH_DIR} 9ENV env_package=${PACKAGE} 10 11ENV DEBIAN_FRONTEND=noninteractive 12 13RUN apt-get update \u0026amp;\u0026amp; apt-get install -y wget python3-pip libxext6 ffmpeg libsm6 libffi-dev \\ 14 ros-noetic-jsk-recognition-msgs ros-noetic-sensor-msgs python3-rospy python3-roslaunch \\ 15 ros-noetic-cv-bridge wget ros-noetic-jsk-rviz-plugins ros-noetic-roslaunch \u0026amp;\u0026amp;\\ 16 rm -rf /var/lib/apt/lists/* 17 18# Install and activate minicoda 19RUN mkdir -p ~/miniconda3 \u0026amp;\u0026amp;\\ 20 wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh \u0026amp;\u0026amp;\\ 21 bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3 22 rm -rf ~/miniconda3/miniconda.sh 23 24RUN ~/miniconda3/bin/conda init bash 25 26ENV PATH /root/miniconda3/envs/py37/bin:$PATH 27ENV PATH /root/miniconda3/bin:$PATH 28 29RUN conda create -n py37 python=3.7 \u0026amp;\u0026amp;\\ 30 conda init \u0026amp;\u0026amp;\\ 31 echo \u0026#34;conda activate py37\u0026#34; \u0026gt;\u0026gt; ~/.bashrc 32 33# Make RUN commands use the new environment: 34SHELL [\u0026#34;conda\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;-n\u0026#34;, \u0026#34;py37\u0026#34;, \u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;] 35 36RUN pip install --upgrade pip \u0026amp;\u0026amp;\\ 37 pip install torchvision==0.11.1 torch==1.10.0 \u0026amp;\u0026amp;\\ 38 pip install einops prettytable rospkg cv_bridge matplotlib netifaces \u0026amp;\u0026amp;\\ 39 pip install mmcv-full==1.3.16 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.10/index.html 40 41WORKDIR /app 42COPY install install 43 44# Define HOW to run 45ENTRYPOINT [ \u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;] 46# Define WHAT to run 47CMD [ \u0026#34;source /opt/ros/noetic/setup.bash \u0026amp;\u0026amp; \\ 48 source install/setup_all.bash \u0026amp;\u0026amp; \\ 49 source ~/.bashrc \u0026amp;\u0026amp; \\ 50 roslaunch --wait /app/install/ganav_terrain_segmentation/share/ganav_terrain_segmentation/launch/ganav_terrain_segmentation.launch\u0026#34; ] Known issue: the launched docker with a docker-compose file can only activate the base conda environment.\nDockerfile with specified Python version 3.7 1ARG ARCH_IMG 2ARG CI_PROJECT_DIR 3FROM $ARCH_IMG 4ARG PACKAGE 5ARG LAUNCHFILE 6 7ENV env_launchfile=${LAUNCHFILE} 8ENV env_package=${PACKAGE} 9 10ENV DEBIAN_FRONTEND=noninteractive 11 12# Install required system dependencies 13RUN apt-get update \u0026amp;\u0026amp; \\ 14 apt-get install -y wget libxext6 ffmpeg libsm6 libffi-dev 15 16# Install Python 3.7 17RUN apt-get install -y build-essential zlib1g-dev libncurses5-dev libgdbm-dev \\ 18 libnss3-dev libssl-dev libsqlite3-dev libreadline-dev libffi-dev liblzma-dev \u0026amp;\u0026amp; \\ 19 cd /usr/src \u0026amp;\u0026amp;\\ 20 wget https://www.python.org/ftp/python/3.7.17/Python-3.7.17.tgz \u0026amp;\u0026amp;\\ 21 tar xzf Python-3.7.17.tgz \u0026amp;\u0026amp;\\ 22 cd Python-3.7.17 \u0026amp;\u0026amp;\\ 23 ./configure --enable-optimizations \u0026amp;\u0026amp;\\ 24 make altinstall \u0026amp;\u0026amp;\\ 25 ln -s -f /usr/local/bin/python3.7 /usr/local/bin/python 26 27# Link Python3 to Python3.7 28RUN rm /usr/bin/python3 29RUN ln -s /usr/local/bin/python3.7 /usr/bin/python3 30 31# Install Python 3.7 pip 32RUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py \u0026amp;\u0026amp; \\ 33 python3.7 get-pip.py \u0026amp;\u0026amp; \\ 34 ln -s -f /usr/local/bin/pip3.7 /usr/local/bin/pip 35 36# Install requirements 37RUN pip3.7 install --upgrade pip \u0026amp;\u0026amp;\\ 38 pip3.7 install netifaces matplotlib cv_bridge rospkg prettytable einops \\ 39 backports.lzma defusedxml\\ 40 torchvision==0.11.1 torch==1.10.0 41 42# Install MMCV 43RUN cd ~ \u0026amp;\u0026amp; git clone https://github.com/open-mmlab/mmcv.git \u0026amp;\u0026amp;\\ 44 cd mmcv \u0026amp;\u0026amp;\\ 45 git checkout v1.3.16 \u0026amp;\u0026amp;\\ 46 MMCV_WITH_OPS=1 pip install -e . 47 48# Replace lzma.py file, otherwise has error 49RUN rm /usr/local/lib/python3.7/lzma.py 50COPY ./\u0026lt;path\u0026gt;/\u0026lt;to\u0026gt;/lzma.py /usr/local/lib/python3.7/lzma.py 51 52RUN apt-get install -y \\ 53 ros-noetic-jsk-recognition-msgs ros-noetic-sensor-msgs python3-rospy python3-roslaunch \\ 54 ros-noetic-cv-bridge wget ros-noetic-jsk-rviz-plugins ros-noetic-roslaunch 55 56CMD source /opt/ros/noetic/setup.bash 57 58RUN rm -rf /var/lib/apt/lists/* 59 60WORKDIR /app 61COPY install install 62 63# Define HOW to run 64ENTRYPOINT [ \u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34; ] 65# Define WHAT to run 66CMD [ \u0026#34;source /opt/ros/noetic/setup.bash \u0026amp;\u0026amp; \\ 67 source install/setup_deps.bash \u0026amp;\u0026amp; \\ 68 roslaunch --wait ${env_package} ${env_launchfile}\u0026#34; ] The Docker image uses Python3.7 instead of default version which usually Python 3.8.\nSaved Docker script, install python3.7 and use pip3.7 as default.\n1# RUN apt-get update \u0026amp;\u0026amp; \\ 2# # Install system dependencies 3# apt-get install -y wget libxext6 ffmpeg libsm6 libffi-dev \u0026amp;\u0026amp; \\ 4# # Install Python 3.7 5# apt-get install -y build-essential zlib1g-dev libncurses5-dev libgdbm-dev \\ 6# libnss3-dev libssl-dev libsqlite3-dev libreadline-dev libffi-dev liblzma-dev \u0026amp;\u0026amp; \\ 7# cd /usr/src \u0026amp;\u0026amp;\\ 8# wget https://www.python.org/ftp/python/3.7.17/Python-3.7.17.tgz \u0026amp;\u0026amp;\\ 9# tar xzf Python-3.7.17.tgz \u0026amp;\u0026amp;\\ 10# cd Python-3.7.17 \u0026amp;\u0026amp;\\ 11# ./configure --enable-optimizations \u0026amp;\u0026amp;\\ 12# make altinstall \u0026amp;\u0026amp;\\ 13# # Link Python and Python3 to Python3.7 14# ln -s -f /usr/local/bin/python3.7 /usr/local/bin/python \u0026amp;\u0026amp;\\ 15# rm /usr/bin/python3 \u0026amp;\u0026amp;\\ 16# ln -s /usr/local/bin/python3.7 /usr/bin/python3 \u0026amp;\u0026amp;\\ 17# # Install Python 3.7 pip 18# curl https://bootstrap.pypa.io/pip/3.7/get-pip.py -o get-pip.py \u0026amp;\u0026amp; \\ 19# python3.7 get-pip.py \u0026amp;\u0026amp; \\ 20# ln -s -f /usr/local/bin/pip3.7 /usr/local/bin/pip \u0026amp;\u0026amp; \\ 21# # Install pip requirements 22# pip3.7 install --upgrade pip \u0026amp;\u0026amp; \\ 23# pip3.7 install netifaces==0.11.0 matplotlib==3.5.3 cv_bridge rospkg==1.5.1 prettytable==3.7.0 einops==0.6.1 \\ 24# backports.lzma defusedxml ftfy==6.1.1 regex==2024.4.16 \u0026amp;\u0026amp; \\ 25# pip3.7 install torch torchvision torchaudio --force-reinstall --extra-index-url https://download.pytorch.org/whl/cu116 \u0026amp;\u0026amp; \\ 26# # Install mmcv, mmsegmentation 27# pip3.7 install mmcv==2.0.0rc4 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.10/index.html \u0026amp;\u0026amp; \\ 28# pip3.7 install mmsegmentation==1.2.2 \u0026amp;\u0026amp; \\ 29# # Install mmcv from source code 30# # cd ~ \u0026amp;\u0026amp; git clone https://github.com/open-mmlab/mmcv.git \u0026amp;\u0026amp; \\ 31# # cd mmcv \u0026amp;\u0026amp; \\ 32# # git checkout v1.3.16 \u0026amp;\u0026amp; \\ 33# # MMCV_WITH_OPS=1 pip install -e . \u0026amp;\u0026amp; \\ 34# # Remove the file to be replaced 35# rm /usr/local/lib/python3.7/lzma.py ","link":"http://localhost:1313/post/tech/002-run-conda-in-dockerfile/","section":"post","tags":["Docker"],"title":"Dockerfile records"},{"body":"C++ Code for Polygon operation with Boost geometry library\nIntroduction Boost Geometry is part of collection of the Boost C++ libs, defines concepts, primitives and algorithms for solving geometry problems ref.\nPolygon is a planar surface defined by one exterior boundary and zero or more interior boundaries ref.\nPolygon operation is often used in robotics industry's perception systems, particularly for tasks such as defining safety zones and obstacle bounding boxes. Some common applications include:\nCalculation the overlapping proportion between obstacle bounding box and safety zone, to determine the danger level. Verifying if an obstacle enters the danger zone of an ego vehicle. Determining if some LiDAR points reside within a specified zone. Applications Generate Random Polygons: Create two random polygons. Validity Check and Area Calculation: Verify the validity of the polygons and calculate their areas. Intersection Detection: Determine if the two polygons intersect and compute the intersection polygon if they do. Point Inclusion Check: Check if a specified point lies within the intersection polygon. Code example file\nExample output:\n1Polygon is invalid, retrying... 2Polygon is invalid, retrying... 3Polygon 1 points are: (1.05432, 2.2592) (1.5928, 3.05599) (2.93741, 2.09024) (1.47085, 0.798034) (1.05432, 2.2592) Area is: 2.13627 4Polygon is invalid, retrying... 5Polygon is invalid, retrying... 6Polygon is invalid, retrying... 7Polygon 2 points are: (1.97136, 2.36949) (2.81913, 0.603898) (1.81515, 1.29083) (0.327355, 3.53608) (1.97136, 2.36949) Area is: 1.5729 8Polygon intersect points are: (1.11448, 2.34822) (1.40193, 2.77356) (1.97136, 2.36949) (2.35279, 1.57512) (1.93617, 1.20803) (1.81515, 1.29083) (1.11448, 2.34822) Area is: 0.946869 9origin is outside the polygon ","link":"http://localhost:1313/post/code/003-polygon-operation/","section":"post","tags":["C++"],"title":"Boost::geometry Polygon operations (C++)"},{"body":"3000 Maximum Area of Longest Diagonal Rectangle\n1 Problem Leetcode\nexample 1: Input: dimensions = [[9,3],[8,6]] Output: 48 Explanation: For index = 0, length = 9 and width = 3. Diagonal length = sqrt(9 * 9 + 3 * 3) = sqrt(90) ≈ 9.487. For index = 1, length = 8 and width = 6. Diagonal length = sqrt(8 * 8 + 6 * 6) = sqrt(100) = 10. So, the rectangle at index 1 has a greater diagonal length therefore we return area = 8 * 6 = 48.\nexample 2: Input: dimensions = [[3,4],[4,3]] Output: 12 Explanation: Length of diagonal is the same for both which is 5, so maximum area = 12.\n","link":"http://localhost:1313/post/code/001-leetcode-max-area/","section":"post","tags":["Leetcode"],"title":"Leetcode-Max area of longest diagnoal rectangle"},{"body":"Intro of C++ containers and usage method\n1 Overview A container is a holder object that stores a collection of other objects as its elements. Containers are implemented as class templates, they have great flexibility in the elements' types supported. Container manages storage for elements and provides sever member functions to access and manipulate elements.\nTwo main categories:\nSequence container: Maintain elements in a linear sequence. Associative containers: Allow efficient retrieval based on keys rather than positions. Typically implemented using binary search trees or hash tables. Below, I'll review several commonly used sequence container and unordered associative containers along with their typical use cases.\n2 Sequence Container Sequence contaienr refers to a group of container class templates in the standard library that implements storage of data elements. It includes array, vector, list, forword_list, deque.\nArray: A compile-time non-resizable array Vector: Array with fast random access and automatically resize when appending elements. Deque: Double-ended queue with comparatively fast random access. List: Doubly linked list. Forward list: Singly linked list. 2.1 Array std::array is a container that encapsulates fixed size arrays.\n1#include \u0026lt;array\u0026gt; 2// Declare an array of integers with a fixed size of 3 elements 3array\u0026lt;int, 3\u0026gt; arr = {0, 1, 2}; 4// Retrieve the size of the array using the size() member function 5arr.size(); 6// Access the element at index 0 of the array using the subscript operator [] 7arr[0]; 2.2 Vector std::vector is a class template in C++ Standard Library. It is sequency containers representing arrays that can change in size, it uses contiguous storage locations for their elements. Vector's size can be changed dynamically, the storage is handled automatically by the container. It allocates array to store elements dynamically.\nVector consumes more memory to manage storage and grow compare to array.\n1#include \u0026lt;vector\u0026gt; 2// Ways to declare and initialize a vector 3vector\u0026lt;int\u0026gt; vec = {1, 2, 4}; 4vector\u0026lt;int\u0026gt; vec {1, 2, 3}; 5vector\u0026lt;int\u0026gt; vec(4, 3); // 4 is size, 3 is value {3, 3，3， 3} 6// Assign value 5 to the element at index 3 7vec[3] = 5; 8// Declare a vector of strings 9vecotr\u0026lt;string\u0026gt; planets; 10/ Add the string \u0026#34;Mercury\u0026#34; to the vector 11planets.push_back(\u0026#34;Mercury\u0026#34;) 12// Retrieve the number of elements in the vector 13planets.size(); 14// Retrieve the current capacity of the vector 15planets.capacity() 3 Unordered Associative Container Associative containers that store elements formed by the combination of a key value and a mapped value. Fast retrieval of individual elements based on keys. Key value is generally used to uniquely identify the element.\n3.1 Unordered Map Unordered maps are associative containers in C++ that efficiently store key-value pairs without maintaining a specific order. They offer rapid retrieval, insertion, and deletion operations based on keys, making them ideal for scenarios requiring quick access to elements identified by their keys. Compared to ordered containers like maps, unordered maps prioritize speed over element ordering, providing faster access to elements.\n1#include \u0026lt;unordered_map\u0026gt; 2// Declare an unordered_map with integer keys and values 3std::unordered_map\u0026lt;int, int\u0026gt; freq_counter; 4// Access the value associated with key 1 5freq_counter[1]; 6// Insert a key-value pair into the unordered_map 7freq_counter.insert(std::make_pair(2, 1)); 3.2 Unordered Set An unordered set is a container that stores a collection of unique elements in an unordered fashion. Unordered sets do not maintain a particular order among their elements. They offer fast retrieval, insertion, and deletion operations, typically implemented using hash tables. This makes them suitable for scenarios where fast lookup of unique elements is required, without concern for ordering.\n1#include \u0026lt;unordered_set\u0026gt; 2// Declare and initialize an unordered_set with integer elements 3std::unordered_set\u0026lt;int\u0026gt; mySet{2, 7, 1, 8, 2, 8}; 4// Insert the value 5 into the unordered_set 5mySet.insert(5); 6// Erase the value 5 from the unordered_set if it exists 7mySet.erase(5); 3.3 Application Requirment description\nThis is a use case I encountered at work.\nDesign a Vehicle Velocity Management System aimed at storing and regulating the speed of traffic vehicles (obstacle). If detect the vehicle's velocity magnitude is not confident, the system retrieves and apply the last known velocity magnitude along with its current velocity direction. The objective is to maintain a consistent velocity for traffic vehicles, minimizing abrupt changes in speed.\nCode pieces sample\n1// Init associative containers to store obstacle object and obstacle id 2std::unordered_map\u0026lt;int, Eigen::Vector3d\u0026gt; obstacles_; 3std::unordered_set\u0026lt;int\u0026gt; obstacle_ids_; 4// Add obstacle information 5obstacle_ids_.insert(id); 6obstacles_[id] = velocity; 7// Remove obstacle from containers 8obstacles_.erase(id); 9obstacle_ids_.erase(id); 10 11 12// Get last velocity 13auto it = obstacles_.find(id); 14// Return the velocity if the ID is found 15if (it != obstacles_.end()) 16{ 17 return it-\u0026gt;second; 18} 19else 20{ 21 // Return Zeros velocity if not found 22} 23 24 25// Remove unneeded obstacle IDs 26std::unordered_set\u0026lt;int\u0026gt; ids_to_remove; 27for (const auto\u0026amp; obstacle : obstacles_) 28{ 29 int id = obstacle.first; 30 if (obstacle_ids_.find(id) == obstacle_ids_.end()) 31 { 32 ids_to_remove.insert(id); 33 } 34} 35for (int id_to_remove : ids_to_remove) 36{ 37 obstacles_.erase(id_to_remove); 38} 39 40 41// Clear info 42 obstacle_ids_.clear(); 43 44 45// Use magnitude of last_velocity along with current direction 46double magnitude = last_velocity.norm(); 47// Normalize the current_velocity to maintain its direction 48if (current_velocity.norm() \u0026gt; 0.0) // Avoid division by zero 49{ 50 current_velocity.normalize(); 51} 52else 53{ 54 // If current_velocity is zero, just return it 55 return current_velocity; 56} 57// Scale the normalized current_velocity by the magnitude of last_velocity 58current_velocity *= magnitude; 59return current_velocity; 4 Leetcode Problem 3005 Count Elements With Maximum Frequency\nYou are given an array nums consisting of positive integers. Return the total frequencies of elements in nums such that those elements all have the maximum frequency. The frequency of an element is the number of occurrences of that element in the array.\nExample 1:\nInput: nums = [1,2,2,3,1,4] Output: 4 Explanation: The elements 1 and 2 have a frequency of 2 which is the maximum frequency in the array. So the number of elements in the array with maximum frequency is 4.\nExample 2:\nInput: nums = [1,2,3,4,5] Output: 5 Explanation: All elements of the array have a frequency of 1 which is the maximum. So the number of elements in the array with maximum frequency is 5. Constraints: 1 \u0026lt;= nums.length \u0026lt;= 100 1 \u0026lt;= nums[i] \u0026lt;= 100\n4.1 Use Vector Implementation with vector\n1#include \u0026lt;iostream\u0026gt; 2#include \u0026lt;vector\u0026gt; 3 4using namespace std; 5 6class Solution 7{ 8public: 9 int maxFrequencyElements(vector\u0026lt;int\u0026gt;\u0026amp; nums) 10 { 11 vector\u0026lt;int\u0026gt; frequency (nums.size(), 0); 12 for (int i = 0; i \u0026lt; frequency.size(); i ++) 13 { 14 frequency[i] = countDuplicatedNumber(i, nums); 15 } 16 for (int element : frequency) 17 { 18 std::cout \u0026lt;\u0026lt; element \u0026lt;\u0026lt; \u0026#34; \u0026#34;; 19 } 20 cout \u0026lt;\u0026lt; endl; 21 int max_value = checkMaxValue(frequency); 22 return sumMaxValue(max_value, frequency); 23 } 24 int countDuplicatedNumber(const int\u0026amp; index, const vector\u0026lt;int\u0026gt;\u0026amp; vector) 25 { 26 int number = 1; 27 for (int i = 1; i + index \u0026lt; vector.size(); i++) 28 { 29 if (vector[i + index] == vector[index]) 30 { 31 number ++; 32 } 33 } 34 return number; 35 } 36 int checkMaxValue(const vector\u0026lt;int\u0026gt;\u0026amp; vector) 37 { 38 int max = 0; 39 for (int i = 0; i \u0026lt; vector.size(); i++) 40 { 41 if (vector[i] \u0026gt; max) 42 { 43 max = vector[i]; 44 } 45 } 46 cout \u0026lt;\u0026lt; \u0026#34;max value in the vec is: \u0026#34; \u0026lt;\u0026lt; max \u0026lt;\u0026lt; endl; 47 return max; 48 } 49 int sumMaxValue(const int\u0026amp; max, const vector\u0026lt;int\u0026gt;\u0026amp; vector) 50 { 51 int sum = 0; 52 for (int i = 0; i \u0026lt; vector.size(); i++) 53 { 54 if (vector[i] == max) 55 { 56 sum += vector[i]; 57 } 58 } 59 return sum; 60 } 61}; 62 63int main() 64{ 65 vector\u0026lt;int\u0026gt; num1 {1,2,2,3,4,4,1}; 66 Solution solution; 67 float result = solution.maxFrequencyElements(num1); 68 cout \u0026lt;\u0026lt; \u0026#34;result is: \u0026#34; \u0026lt;\u0026lt; result \u0026lt;\u0026lt; endl; 69} 4.1 Use Unnordered map Implementation with unordered map, [reference]\n1#include \u0026lt;iostream\u0026gt; 2#include \u0026lt;vector\u0026gt; 3#include \u0026lt;unordered_map\u0026gt; 4 5using namespace std; 6 7class Solution 8{ 9public: 10 int maxFrequencyElements(vector\u0026lt;int\u0026gt;\u0026amp; nums) 11 { 12 std::unordered_map\u0026lt;int, int\u0026gt; freq_counter; 13 for(int num : nums) 14 { 15 freq_counter[num]++; 16 } 17 18 int max_frequency = 0; 19 for (const auto\u0026amp; entry : freq_counter) 20 { 21 max_frequency = std::max(max_frequency, entry.second); 22 } 23 24 int max_freq_elements = 0; 25 for (const auto\u0026amp; entry : freq_counter) 26 { 27 if (entry.second == max_frequency) 28 { 29 max_freq_elements++; 30 } 31 } 32 33 int total_frequency = max_frequency * max_freq_elements; 34 return total_frequency; 35 } 36}; 37 38int main() 39{ 40 vector\u0026lt;int\u0026gt; num1 {7,7,7,1,2,2,3,4,4,1}; 41 Solution solution; 42 int result = solution.maxFrequencyElements(num1); 43 cout \u0026lt;\u0026lt; \u0026#34;result is: \u0026#34; \u0026lt;\u0026lt; result \u0026lt;\u0026lt; endl; 44} ","link":"http://localhost:1313/post/code/002-c++-containers/","section":"post","tags":["C++"],"title":"C++ Standard Containers and Use Cases"},{"body":"General knowledge recordings.\n1 Class Class has attibutes (variables) and methods (functions).\nDifference among public, private, protect Those are access specifiers, defining how the class members can be accessed.\npublic: accessible from outside the class private: cannot be accessed from outside the class protected: cannot be accessed from outside the class, but can be accessed in inherited classes. Inheritance: inherit attributes and methods from one class (base class, parent) to another (derived class, child). Use : to inherit. Useful for code reusability.\nInheritance and Friendship:\nFriend class has the access to the proteced and private members. Virtual function (methods) is a member function declared in base class, redefined (overridden) by derived class. Mainly used to achieve runtime polymorphism (多态)?\n2 Containers Holds object and stores collection of other objects.\nSequence containers: array, vector, deque, forward_list, list Container adaptoers: stack, queue, priority_queue. Associative containers: set, multiset, map, multimap, Unordered associative containers: unordered_set, unordered_multiset, unordered_map, unordered_multimap. vector, queue, stack, priority_queue, list, set, map, etc. 3 Operator 3.1 Overview Assignment operator =.\nArithmetic operators: addition +; subtraction -; multiplication *; division /; modulo %(gives the remainder of a division of two values).\nCompound assignment: +=, -=, *=, /=, %=, \u0026gt;\u0026gt;=, \u0026amp;=, ^=, |=.\nIncrement and decrement:+=, --.\nRelational and comparison: ==, !=, \u0026gt;, \u0026lt;, \u0026gt;= \u0026lt;=. (Equal to, not equal to, less than, greater than, less than or equal to, greater than or equal to).\nLogical operators: !, \u0026amp;\u0026amp;, ||.\nConditional ternary operator (三元运算子): ?.\nComma (,) to separate two or more expressions.\\ Bitwise operators: \u0026amp;, |, ^, ~, \u0026lt;\u0026lt;, \u0026gt;\u0026gt; (and, or, xor, not, shl, shr) Explicit type casting: (), (int) Size of: sizeof(), parameter can be type or variable, returns the size in bytes.\n3.2 Operator overloading Giving special meaning to an existing operator without changing its original meaning.\nTODO static function compile time and run time? vtable? constructor, default, ~ Initializer list and uniform initialization ","link":"http://localhost:1313/post/code/000-general-question/","section":"post","tags":["C++"],"title":"C++ general knowledge"},{"body":"Introduce vector mutiplication with applications.\n1 Vector Vector has magnitude and direction. Collinear vectors: vectors are parallel.\n1.1 Cross Product $$\\vec{a}\\times\\vec{b}=|A||B|\\sin{\\theta}\\vec{n}$$\n\\theta is the angle between vectors, \\vec{n} is unit vector at right angles to vectors, direction follows right hand rule.\nThe Cross Product of two vector is another vector that is at right angles to both. The magnitude of the cross product equals the areas of parallelogram with both vectors. The magnitude reaches maximum length when both vectos are at right angles.\nThe cross product is the determinant of 3x3 matrix.\n$$ a\\times b=\\det(i,j,k;a_1,a_2,a_3,b_1,b_2,b_3) $$ $$=(a_2b_3 - a_3,b_2)i+(a_3b_1 - a_1,b_3)j+(a_1b_2 - a_2,b_1)k$$\n1.2 Dot Product Cross product gives a vector answer, but Dot Product gives a scalar answer. It multiplies vectors' lengths together but only when they point in the same direction, so using \\cos{\\theta}. So if two vector are at right angle, result is zero.\n$$ a\\cdot b = |a||b|\\cos{\\theta} $$\n2 Application To determine whether two vectors are:\ncollinear: A=k*B, and k is a scalar; Cross product is zero vector (only applied to three-dimensional or spatial problems); Ratio of corresponding coordinates are equal. perpendicular: Dot product is zero. Calcualte the projection point C of point P onto the line segment AB.\nVector AB, AP, dot product is D. D/|AB| = |AC| k = |AC|/|AB| = D/{|AB|^2} C = A + k\\vec{AB} How to verify C is projection point:\nAC = k AB PCxAB=zero vector How to calculate the distance d between a point P to a line AB\nCross product's norm is the area of the parallelogram spanned by two vectors. (AB X AP) The area also equas to distance d * |AB| d = |AB X AP|/|AB| Code implementation of above steps:\nRaw implementation Implementation with Eigen lib ","link":"http://localhost:1313/post/math/001-vector-cross-product/","section":"post","tags":null,"title":"Vector Cross Product and Dot Product"},{"body":"The blog collects some useful Docker commands.\nBasic Concept of Docker container and Docker image:\nDocker container is a running instance of a Docker image, providing an isolated environment that can be started, stopped and detelted. Docker image is a static, read-only file containing everything needed to create containers. It serves as the source file for starting a Docker container. Check Docker Username Check the username and registry of the logged in Docker account.\n1docker system info | grep -E \u0026#39;Username|Registry\u0026#39; Docker Registry If have multiple Docker registries configured, you can check the authentication details stored in your Docker configuration file.\n1cat ~/.docker/config.json # On Unix-based systems To login or logout a docker registry:\n1docker login [REGISTRY] 2docker logout [REGISTRY] 3gcloud auth configure-docker [gcloud-registry] # If it is Gcloud registry Launch containers 1# Start containers in `docker-compose.yml` file (with -d for detach mode) 2docker-compose up 3# Stop containers in `docker-compose.yml` file 4docker-compose down Enter a running docker\n1docker exec -it \u0026lt;container_id\u0026gt; /bin/bash Copy files from docker to external path\n1docker cp \u0026lt;container_id\u0026gt;:/path/to/your/file /path/on/host/system/ Image Operation 1# Pull a docker by its registry 2docker pull \u0026lt;image-name\u0026gt; Remove Image 1# List locally available Docker images (IMAGE ID) 2docker images 3# Delete a Docker image locally by image ID 4docker rmi \u0026lt;image-id\u0026gt; To delete all of them from system, use below command:\n1docker rmi $(docker images -q) Remove multiple tags iamge Remove multiple-tag images from one repository.\nIf you want to remove all images from a specific repository with different tags, use the following script. Firstly use docker images to review the ones want to remove. The output is like below:\n1REPOSITORY TAG IMAGE ID CREATED SIZE 2registry-A 0.0.1 xxxx 25 hours ago 7.5GB 3registry-A 0.0.2 xxxx 25 hours ago 7.5GB 4registry-A 0.1.0 xxxx 25 hours ago 7.5GB 5... ... ... ... ... In the case, all images from repository registry-A need to be removed. Save below script to a file e.g. remove_tags.sh.\n1 #!/bin/bash 2 3 read -p \u0026#34;Enter the name of the repository: \u0026#34; REPOSITORY 4 5 # Get all tags for the repository (exclude headers) 6 tags=$(docker images --format \u0026#34;{{.Tag}}\u0026#34; $REPOSITORY) 7 8 # Loop through each tag and remove the corresponding image 9 for tag in $tags; do 10 docker rmi $REPOSITORY:$tag 11 echo \u0026#34;$REPOSITORY:$tag removed\u0026#34; 12 done To use it, run bash remove_tags.sh in Terminal, then provide the repository name when prompted. In the example case is registry-A. This script will remove all tags from the specified repository one by one.\nSave Image to .tar File Firstly use docker images to review the image you want to save, and then use following command to save:\n1docker save -o /path/to/\u0026lt;filename\u0026gt;.tar image-id The image-id can be replaced by REPOSITORY:TAG format. For a clearer filename, recommend naming it use a REPOSITORY:TAG format.\nLoad a Docker image from a .tar file.\n1docker load -i /path/to/filename.tar Retag Docker Image 1docker tag \u0026lt;source-repository:tag\u0026gt; \u0026lt;new-repository:tag\u0026gt; Container Operation Basic commands 1# List currently running Docker containers (CONTAINER ID) 2docker ps 3# List currently running Docker containers including stopped ones 4docker ps -a 5# View the logs of a specific container 6docker logs \u0026lt;container-id\u0026gt; 7# Stop a running container 8docker stop \u0026lt;container-id\u0026gt; Enter a Docker with interactive mode 1docker run -it your_image_name /bin/bash Stop All Running Containers It is quite common to have a couple of containers running in the background. Sometimes, it is hardly to be aware of them if you don't check. To stop them all at once, use the following command. Consider adding this to an alias for convenience.\n1docker stop $(docker ps -a -q) Remove Containers 1# Remove one container by name 2docker rm container_name 3# Remove all containers 4docker rm $(docker ps -a -q) Docker-Compose File Example Launch ROS1 roscore and play a local rosbag in a ROS2 system envrionment.\n1version: \u0026#39;3.3\u0026#39; 2 3services: 4 roscore: 5 image: ros:noetic-ros-core 6 container_name: roscore_container 7 restart: unless-stopped 8 network_mode: host 9 command: \u0026gt; 10 /bin/bash -c \u0026#34; 11 source /opt/ros/noetic/setup.bash; 12 roscore 13 \u0026#34; 14 15 bag: 16 image: ros:noetic-ros-core 17 container_name: bag_container 18 restart: unless-stopped 19 network_mode: host 20 command: \u0026gt; 21 /bin/bash -c \u0026#34; 22 source /opt/ros/noetic/setup.bash; 23 rosbag play /launch/**.bag 24 \u0026#34; 25 volumes: 26 - /home/data:/launch ","link":"http://localhost:1313/post/tech/003-some-docker-commands/en/","section":"post","tags":["docker"],"title":"Some Useful Docker Commands"},{"body":"Cuda, Conda, MMDetection3D installation and test.\n1. Prepare The installation is tested in environment: Ubuntu 20.04, CUDA-11.6, PyTorch v1.13.1.\n1.1 Miniconda The installation is inside a conda virtual envrionment, so be sure Miniconda is installed.\nInstallation on Linux:\n1mkdir -p ~/miniconda3 2wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh 3bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3 4rm -rf ~/miniconda3/miniconda.sh And initialization:\n1~/miniconda3/bin/conda init bash 2~/miniconda3/bin/conda init zsh 1.2 Remove and Install CUDA 11.6 and Nvidia driver 510.39.01 are recommended in the guide. The Nvidia driver can be installed along with CUDA.\nRemove Cuda Check cuda version: nvcc --version Remove cuda: 1sudo apt-get --purge remove \u0026#34;*cuda*\u0026#34; \u0026#34;*cublas*\u0026#34; \u0026#34;*cufft*\u0026#34; \u0026#34;*cufile*\u0026#34; \u0026#34;*curand*\u0026#34; \u0026#34;*cusolver*\u0026#34; \u0026#34;*cusparse*\u0026#34; \u0026#34;*gds-tools*\u0026#34; \u0026#34;*npp*\u0026#34; \u0026#34;*nvjpeg*\u0026#34; \u0026#34;nsight*\u0026#34; \u0026#34;*nvvm*\u0026#34; or sudo /usr/local/cuda-11.x/bin/cuda-uninstaller (replace x with the version of cuda) Remove Nvidia Driver Check nvidia driver version: nvidia-smi Remove nvidia driver: sudo /usr/bin/nvidia-uninstall or sudo apt-get --purge remove \u0026quot;*nvidia*\u0026quot; \u0026quot;libxnvctrl*\u0026quot; sudo apt-get autoremove (caution) Installation Option 1 Install CUDA 11.6 and driver 510.39.01:\n1wget https://developer.download.nvidia.com/compute/cuda/11.6.0/local_installers/cuda_11.6.0_510.39.01_linux.run 2sudo sh cuda_11.6.0_510.39.01_linux.run Select the Nvidia driver during CUDA installation in Terminal.\nFor other versions, check method from Nvidia toolkit website.\nOption 2 Open Software \u0026amp; Updates, click Additional Drivers, selecet Nvidia driver and click Apply Changes.\n2. Install dependencies 2.1 Create envrionment from file Download the envrionment.yml file. Create the envrionment by:\n1conda env create --file environment.yml Once created successfully, jump to Section 3.3 to test it.\nIf otherwise failed, follow Section 2.2 to install it manually.\n2.2 Manual installation Create a virtual environment:\n1conda create --name openmmlab python=3.8 -y 2conda activate openmmlab Install Pytorch v1.13.1:\n1conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia Pytorch and CUDA compatible page: https://pytorch.org/get-started/previous-versions/ Install SpConv:\n1pip install spconv-cu116 3. Install MMDetection3D See more from: https://mmdetection3d.readthedocs.io/en/latest/get_started.html\n3.1 Install MM* dependencies 1# Install mim 2pip install -U openmim 3mim install mmengine \u0026#39;mmcv\u0026gt;=2.0.0rc4,\u0026lt;=2.2.0\u0026#39; \u0026#39;mmdet\u0026gt;=3.0.0\u0026#39; 3.2 Install MMDetection3D library Case one: use mmdet3d as a dependency or third-party package:\n1# Install mmdet3d (e.g. 1.4.0) 2mim install \u0026#34;mmdet3d\u0026gt;=1.1.0\u0026#34; Case two: develop and run mmdet3d directly, install it from source:\n1git clone https://github.com/open-mmlab/mmdetection3d.git -b dev-1.x 2cd mmdetection3d 3pip install -v -e . After installation, verify the package location by pip show mmdet3d.\n3.3 Test To test the result, try to import libraries from it in a Python IDE, for example:\n1from mmdet3d.apis import init_model, inference_detector or\n1import mmdet3d 2print(mmdet3d.__version__) ","link":"http://localhost:1313/post/tech/001-install-openmm3d-lib/en/","section":"post","tags":["AI"],"title":"MMDetection3D Library Installation Guide"},{"body":"Hugo, blog\nInstall Hugo Install the latest version from Hugo Github releases page. E.g. download the *.deb file and install by dpkg -i *.deb. (I used v0.118.2)\nOr use the official installation Guide (not recommended)\nStep 1: Create the Required Repositories On GitHub, create two repositories, both with the Add a README file option selected:\nBlog source file repository: Store web source files used to generate the blog site (e.g., name it blog-resources). Pages repository: Used to store the generated web files and can be accessed via a URL (name it \u0026lt;username\u0026gt;.github.io, where \u0026lt;username\u0026gt; is your GitHub account name. Official Documentation) Step 2: Create Blog Files Using Hugo Commands Using blog-resources as an example for the source file repository name:\nClone blog-resources to your local machine. Navigate to the blog-resources directory and use the Hugo command to create the overall website file structure: hugo new site \u0026lt;blog-name\u0026gt;, replacing \u0026lt;blog-name\u0026gt; with your desired name, e.g., woods-blog. The repository`s file structure and their main purposes at this stage are as follows: 1├── README.md # Automatically added README when creating the repo 2└── woods-blog # Blog folder name, matching what was created using Hugo in the previous step 3 ├── archetypes 4 ├── config.toml # Custom configuration file for the blog site, to be edited further 5 ├── content # Blog content folder, where blog documents, images, etc., are stored 6 ├── data 7 ├── layouts 8 ├── resources 9 ├── static 10 └── themes # Themes folder, to install Hugo themes further Step 3: Add Theme Files and Configure Browse Hugo themes and choose a theme. Theme introductions typically provide installation instructions; you can follow those. However, I have encountered errors when installing some themes. Here, I`ll introduce the theme I chose, Mainroad, for reference. Navigate to the blog file path blog-resources/woods-blog and add the theme files using git submodule like this: 1git submodule add https://github.com/vimux/mainroad.git themes/mainroad If added successfully, the blog-resources/woods-blog/themes directory will contain the theme named Mainroad: 1└── themes 2 └── Mainroad # The name of the installed theme In blog-resources/woods-blog/themes/Mainroad/exampleSite, copy the content and static folders and the config.toml file to blog-resources/woods-blog, overwriting existing files. Configure the blog-resources/woods-blog/config.toml file. Key items to check include: 1# Ensure the URL is the URL of the Pages repository you created, with a trailing slash 2baseurl = \u0026#34;https://\u0026lt;username\u0026gt;.github.io/\u0026#34; 3#The theme is the name of the installed theme 4theme = \u0026#34;mainroad\u0026#34; Step 4: Test the Blog and Publish as HTML Start the Hugo server: hugo server and preview the blog theme at http://localhost:1313/. To add blog documents, you can create new .md documents in blog-resources/woods-blog/content for testing. The Hugo server updates in real-time for preview. Publish the blog as HTML web pages by running hugo in the blog-resources/woods-blog/ directory. If the publishing is successful, a public folder will be added in blog-resources/woods-blog/ to store the HTML web page files. Step 5: Push HTML Files to the Pages Repository Go to the blog-resources/woods-blog/public directory and set it as a Pages repository:\n1# Initialize a new Git repository 2git init 3# Create the main branch 4git checkout -b `main` 5# Add the SSH of the Pages repository to your local repository 6git remote add origin git@github.com:\u0026lt;username\u0026gt;/\u0026lt;username\u0026gt;.github.io.git Push the local files to the repository:\n1# Synchronize local and remote commits 2git pull --rebase origin main 3# Add all local changes 4git add . 5# Commit with relevant description 6git commit -m \u0026#34;commit information, e.g., add blog template\u0026#34; 7# Push to the remote repository 8git push origin main Enter the URL in your browser to view the blog at https://\u0026lt;username\u0026gt;.github.io/, which may take a few minutes to become accessible.\nNext Steps Push the blog-resources repository to GitHub.\nReference https://cuttontail.blog/blog/create-a-wesite-using-github-pages-and-hugo/#14-%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83 ","link":"http://localhost:1313/post/tech/000-build-hugo-site/en/","section":"post","tags":["hugo","web"],"title":"Build Personal Blog with Hugo"},{"body":"","link":"http://localhost:1313/","section":"","tags":null,"title":""},{"body":"","link":"http://localhost:1313/tags/index/","section":"tags","tags":null,"title":"Index"},{"body":"","link":"http://localhost:1313/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"","link":"http://localhost:1313/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"http://localhost:1313/tags/ai/","section":"tags","tags":null,"title":"AI"},{"body":"","link":"http://localhost:1313/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"http://localhost:1313/categories/dl/","section":"categories","tags":null,"title":"DL"},{"body":"","link":"http://localhost:1313/tags/gpu/","section":"tags","tags":null,"title":"Gpu"},{"body":"","link":"http://localhost:1313/categories/tech/","section":"categories","tags":null,"title":"Tech"},{"body":"","link":"http://localhost:1313/tags/lidar/","section":"tags","tags":null,"title":"Lidar"},{"body":"Driver license quiz records.\nABS Prevent wheels from locking up when braking heavily. Vehicle is not maneuverable with locked braking. Maintain car manoeuvrability (机动性) at all times. ABS light is on in the control panel while driving: ABS has a malfunction. Bicycle Cyclists riding on a lane or bicycle lane are comparable to vehicle, same yielding rules apply to them. Vehicles on a roadway have right of way to cyclists and moped drivers on a bicycle crossing. Unless have give way signs. When bypass a road cyclists, decelerate and move towards the middle of the road to make sure the vehicle has sufficient distance to it. Overtaking Accelerate before changing lanes. Allows overtake two-wheel motorcycles or mopeds even with overtaking prohibition sign. Never allowed to cross a solid white or yellow line in any situation. When the lanes are heading the same direction, the overtaking in the front does not limit your overtaking. Permited to overtake on the right when: in a traffic jam, when the right lane is moving faster; Vehicle ahead turns left; On motorway, overtake to the right is not allowed. Overtaking over a cyclist, there must be enough space in between so that the air flow does not grab the cyclist. Route sign for detour (车辆改道) indicates there is an intersection ahead. Speed limit Pedestrain zone: 20km/h。 Populated area, general speed limit is 50 km/h. Outside populated areas, speed limit is 80 km/h unless traffic signs dictate. Towing a trailer Snow tires are mandatory in trailer if mass (total mass) is 750-3500kg, speed imit is 80 km/h Lightweight trailer (trailer mass \u0026lt;750kg) speed limit is 100 km/h, Use plug to connect trailer for rear lights, etc. Check function correctly. Load trailer so the center of mass is on the axle. Rear-heavy and front-heavy trailer causes manueuvrability issue; Load must not exceed the trailer’s front edge by one meter, nor the back edge by more than two meters. Any extension exceeding one meter must be marked with a red flag. If it’s dark, the extended load must be marked with lights and reflectors of correspoinding colors. Load must not be wider than the vehicle’s body, From the trailer’s technical section of registraion certificate, people know if it is allowed to be connected. All trailers must equip with lights. Towing a vehicle Towing rope is 3-6 meters, warning flag at midpoint. Top speed is 60 km/h. B license allows to drive: Vehicles total mass 3500 kg, max 8 passengers in addition to driver. Combination vehicle, the total mass of the towed vehicle \u0026lt; 750 kg. combination vehicle towed vehicle mass \u0026gt; 750 but the total mass \u0026lt; 3500. Van, moped, quadrimobile, tractor, and snowmobile. Risk Wet clothes can cause foggy windows. Black ice: Thin ice coating, does not look icy but merely wet. If a vehicle has stopped in front of a pedestrian crossing or is blocking the view, you must stop before the crossing. Its not mandatory to stop before pedestrian crossing if there is a raiesd platform or an empty lane between two vehicles. Big red triangle at back of vehicle indicate the vehicle moves slowly Child in backseat risk: I might feel the urge to take glances and reach to the backseat while driving. In a heavy rain, the rear fog light can be turned on to remedy poor visibility from the behind. Parking In urban, park on inside the population center area, cannot be parked outside of street lane. Parking disk set to next half or full hour. Parking sign, black numbers applies to weekdays, black numbers in parentheses are used to a normal Saturday. Red numbers are used for Sundays and other holidays. Parking in the intersection is prohibited. Minimum distance between a parked car and a intersection is 5 meters. In resident area, parking is progibited outside of the marked parking spaces. At railroad crossings, parking should exceed 30m to it. In populated areas, vehicles must be parked on the roadway or in a designated parking area; Outside populated areas, parking on the roadway is not allowed, must park vehicle completely on the shouder. Bus Bus lane is legal to use when grouping in order to turn. Must give way to a bus leaving a stop when the speed limit on the road is no more than 60 km/h At bus stop is permitted to pick up or drop off a passenger. Tram Tram rails if not marked as a tram lane by a sign, just a normal lane that can drive normally. Tram lane with sign must not be used in any case. Must not use a tram lane for aligning or tunrning. cannot be use in any case. Tram has the right of way in intersections only if no limited with any traffic signs. Motorway and motor-traffic way Motorway: for fast-moving motor vehicle traffic. Different directions are physically separated, does not cross at the same level with other transport routes. Motorway has two roadways. Motor traffic way can have a single roadway, with a median line separating the two lanes. Motor and motor-traffic way: no U turns, reversing, overtaking from right, standing, parking. Stopping distance Stopping distance = reaction distance + braking distance Reaction time is 1-2 s. Braking distance is quadrupled is speed doubles. Reaction distance is directly related to your speed. Safety distance On roadways, must maintain a minimum of four seconds’ safety distance. One general rule: current speed is equals the safety distance in meters. Yielding At an intersection, the vehicle making a turn must yield all oncoming traffic. Always yield a tram in an equal intersection. Yield to a bus that is pulling out from bus stop when speed limit is 60 km/h. Alcohol Drunk-driving limit is a blood alcohol content (BAC) of 0.5 per mille (g/l). Dissolve time for 70kg person: 2 hours. Bllod alcohol level of 0.5 permillage or more is guilty of driving under the influence. One small can of beer or cider takes about 2.5 hours to burn off. 3 cans take close to 8 hours. Dash Red battery symbol: malfunction in battery charging system. Economical driving Drive on the highest suitable gear. Engine braking is an effective and economical way of decelerating. Tires Summer tires, minimum depth of tread is 1.5 mm, recommended depth is 4 mm. Summer tires, less than 3 mm increases the risk of aquaplaning significantly. Winter tires, mimum depth of treads is 3 mm. recommended depth is 6 mm. Winter tires, hyroplaning risk increases significantly at depth of 4 mm. When tire is low air pressure, it wears out fast and gets hot easily. If wheelset includes different conditions tires, should put the better condition wheels to the back, to prevent side slip. Other Red jumper cable between plus posts on batteries, Black cable from subsidiary battery’s minu-pole to your car egine’s frame. Have sturdy net between a cargo space and the passenger space when dogs are transported: Dogs won’t come over the passengers in a collision. After heavy rain, risk: Other cars can splash water, cause a sudden visibility block. Do not touch the bulb with bare hands as the grease sticking to its surface may break the bulb when it heats up. If red and yellow lights are lit simultaneously, it means that the light is about to turn green. Vehicle insurance is mandatory for every motorized vehicle, and you must acquire one within seven days of change of ownership. Insurance cover all personal injuries and the material damages for the innocent party. A heater makes cleaning the windows easier and allows to drive without a thick coat under the seat belt. How decrease the amount of harmful particles in the exhaust, by preheating the motor in cold weather. Which areas are traffic laws inapplicable: Areas that are closed to general traffic. When turning onto a side road, steps are indicate, aligh the vehicle, and reduce speed. Police is showing the red light, pull over at the first possible location Signs: No entry: one yellow horizontal line with red round board One-way sign The small downward arrow sign under a no parking sign indicate no parking ends here. Parking prohibited zone Parking prohibited: One red slash with blue background round board Parking and stopping prohibited: Red round cross Parking prohibited on odd/even days： One or two vertical lines with one slash Intersection with a minor road Intersection with equal roads Pedestrain road; allowed to drive but must yield to all pedestrains. Yield and stop signs mandate to traffic from left and right. Regulation ends to the sign Overtaking is prohibited sign. Railroad's level-crossing with multiple tracks Left lane ends Priority road no entry for vehicles Road number is framed by a dash line, means access to the road in question Blue speed round board: Minimum speed Population center: yellow rectangle with black city building outline Noun Adaptive Cruise Control 自动跟车系统 detour 车辆改道 Engine braking: Reducing of speed when you are not giving gas and the gear is on. Transmission: 传输，传动，变速器 Clutch 离合器 ","link":"http://localhost:1313/post/hobby/2024-05-23-driver-license/","section":"post","tags":null,"title":"Driver License Theory"},{"body":"","link":"http://localhost:1313/categories/hobby/","section":"categories","tags":null,"title":"Hobby"},{"body":"","link":"http://localhost:1313/tags/ros2/","section":"tags","tags":null,"title":"ROS2"},{"body":"","link":"http://localhost:1313/categories/math/","section":"categories","tags":null,"title":"Math"},{"body":"Channels of my paintings.\nI have video channels sharing daily drawing processes, digital art, and painting tutorials. Those interested in art are welcome to browse:\nYouTube Bilibili ","link":"http://localhost:1313/post/hobby/000-paintings/","section":"post","tags":null,"title":"My paintings' channel"},{"body":"","link":"http://localhost:1313/tags/docker/","section":"tags","tags":null,"title":"Docker"},{"body":"","link":"http://localhost:1313/tags/c++/","section":"tags","tags":null,"title":"C++"},{"body":"","link":"http://localhost:1313/categories/code/","section":"categories","tags":null,"title":"Code"},{"body":"","link":"http://localhost:1313/tags/leetcode/","section":"tags","tags":null,"title":"Leetcode"},{"body":"Menghao Wu I am a seasoned Senior Software Engineer specializing in the design and development of automated robotics obstacle detection and tracking systems, utilizing advanced LiDAR and camera sensor technologies. With technical leadership background, I oversee project software design, automation workflows, resource planning, and team management, ensuring optimal performance and delivery. My expertise includes researching state-of-the-art deep learning algorithms for recognition systems and implementing advanced models to enhance automation and robotics perception capabilities., I have passion for learning and researching new fields, continuously expanding my knowledge.\nSKILLS Technical Skills:\nC/C++ (Excellent), Python (Excellent), CUDA Programming (Excellent), Linux, Software C4 design, Bash, C#, Octave/Matlab\nMachine Learning Expertise:\nData processing, Statistical analysis, Neural network training and optimization, PyTorch, TensorFlow, scikit-learn, Keras, Pandas\nApplication and Platform:\nAutomation system design, Unit test, CI/CD, Git, Docker, Conan, DevOps, ROS, TCP/IP, TRDP, Embedded systems development, YouTrack, Jupyter, HTML\nAdditional Knowledge:\nScientific\u0026amp;Technical document writing, Agile development, Scrum, Shell scripting, Autonomous Mobile Robots, Reinforcement Learning\nPUBLICATIONS Wu, Menghao, et al. \u0026quot;The multi-dimensional actions control approach for obstacle avoidance based on reinforcement learning.\u0026quot; Symmetry 13.8 (2021): 1335.\nDu, Shitong, Wu, Menghao et al. \u0026quot;LiDAR odometry and mapping based on semantic information for outdoor environment.\u0026quot; Remote Sensing 13.15 (2021): 2864.\nWu, Menghao, et al. \u0026quot;The actor-dueling-critic method for reinforcement learning.\u0026quot; Sensors 19.7 (2019): 1547.\nWu, Menghao, et al. \u0026quot;Graph signal sampling with deep Q-learning.\u0026quot; 2020 International Conference on Computer Information and Big Data Applications (CIBDA). IEEE, 2020\nEXPERIENCE GIM Robotics, Finland, Senior Robotics Engineer\nJUL 2020 - PRESENT\nLead software development for automotive applications, specializing in designing and developing robust robot software structures. Lead the development of Machine Learning-driven perception systems for obstacle detection and tracking, exploring advanced algorithms to enhance system accuracy and stability. Research and apply advances to optimize 3D/2D detectors, strategically improving robotics system performance. Aalto University, Finland, Research Assistant\nOCT 2016 - MAY 2018\nConducted cutting-edge research in Machine Learning and Reinforcement Learning within Aalto University's CS Department Big Data group. Proficient in Python and various data processing, visualization, and neural network libraries, including Numpy, Scipy, Pandas, Matplotlib, TensorFlow, PyTorch, Keras, and Scikit-learn. ZTE Corporation, Beijing, China, Software Engineer Intern\nAUG 2015 - DEC 2015\nContributed to software development and testing in signal processing at ZTE Corporation, Beijing. EDUCATION Harbin Engineering University, China - Ph.D., Master\nSEP 2014 - JUL 2021\nResearched continuous control algorithms in Deep Reinforcement Learning. Alongside expertise in image processing and data mining techniques. Developed real-time object detection software from CCD input using C++, C#, and OpenCV. Designed a human interface for processing GPS/INS signals and displaying real-time information in C++. Northeastern University, China - Bachelor\nSEP 2010 - JUL 2014\nDesigned an obstacle avoidance and road tracking smart car for graduation, utilizing C and hardware components including CCD, ultrasonic sensor, motor, ARM chips, and wireless remote. Achieved 3rd place in the China Electronic Design Contest (C and hardware design) and City 51SCM programming contest. Completed coursework in Computer Theory, C Programming, Control Theory, and Electronics. OTHERS Github: https://github.com/MengWoods\nTechnical blogs:\nhttps://mengwoods.github.io/ https://blog.csdn.net/LiKouSanYou?type=blog (in Chinese) Hobby: Painting, graphic design.\nYoutube channel: https://www.youtube.com/channel/UCn9p3vdbkkQJ_r_XXwrYS8w ","link":"http://localhost:1313/about/","section":"","tags":null,"title":"About Me"},{"body":"The article records operation steps of monthly saving with Nordnet. and records the questions during the whole process.\nBackground questions Why choose index fund? I don't want pay too much time on investments, I don't have more knowledge of investment. And I want to have some level return from deposit. S\u0026amp;P 500 index fund includes most profitable 500 companys in the US, and people trust their ability to make money. So I chose the one as the target for my month saving.\nWhy choose ETF? ETFs are mostly index funds, and ETFs have lower running cost (~0.0x%) compare to mutual fund(~0.x%). For a long-term saving, ETFs are cost saving.\nWhy use Nordnet? In my living region, not many option online brokers. Nordnet is one of famous one.\nWhy monthly investment? Based on some theory, regular investment can average out costs，The risk is lower than a one-time investment.\nNordNet ETF Entrance NordNet App supports Engilish, website does not. But website supports all functions. I chose website to start, and use Chrome translation for English.\nHover mouse at the sencond title (\u0026quot;Stock exchanges and markets\u0026quot;) of top menu, the window menu will show. click the \u0026quot;ETF exchange-traded funds\u0026quot;, ETF data base will show. In the data base page, left-top search bar can be used to search a ETF's name. For browsing s\u0026amp;p 500 funds, searching \u0026quot;500\u0026quot; is OK\nAfter search \u0026quot;500\u0026quot;, I clicked the \u0026quot;Costs\u0026quot; let it show list with running cost ascending order. ETF's naming rule The Name of each fund includes many components, and the name is designed to protect investors. But it is hard to understand if no experience, after investigation, I will explain what they mean in the section. Generally, the name is compose of: \u0026quot;ETF issuer + underlying index + regulation standard + ETF + additional information\u0026quot; ETF issuer is the compnay name who provides the ETF. Underlying index indicates which index it is tracking. Regulation standard is the ruls the ETF complies to protect investors. In EU it is 'UCITS' which means \u0026quot;Undertakings for Collective Investments in Transferable Securities\u0026quot;. ETF is a distinguish means it is an ETF. Additional information might include below aspects: Currency used, such as USD (US Dollar), GBP (British Pound), EUR (Euro), etc. Currency hedging period： Hedged， Daily hedged, ... Use revenue for investment (ACC, C), divendes (D, Dis, Dist) Buy one ETF from NordNet [wip]\nMonthly saving via Nordnet Reference https://www.boerse-frankfurt.de/en/wissen/wertpapiere/etfs-und-etps/etf-names https://zhuanlan.zhihu.com/p/63212211\n","link":"http://localhost:1313/post/hobby/2024-01-30-fund/en/","section":"post","tags":null,"title":"How I start monthly S\u0026P500 investment via Nordnet."},{"body":"","link":"http://localhost:1313/categories/investment/","section":"categories","tags":null,"title":"Investment"},{"body":"","link":"http://localhost:1313/tags/hugo/","section":"tags","tags":null,"title":"Hugo"},{"body":"","link":"http://localhost:1313/tags/web/","section":"tags","tags":null,"title":"Web"},{"body":"","link":"http://localhost:1313/archives/","section":"","tags":null,"title":""},{"body":"","link":"http://localhost:1313/series/","section":"series","tags":null,"title":"Series"}]