
<!DOCTYPE html>
<html
  lang="en"
  data-figures=""
  
    class="page"
  
  
  >
  <head>
<title>Comprehensive Notes on Andrew Ng‚Äôs Machine Learning Course | Menghao blog</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7L1C01SX1E"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-7L1C01SX1E');
</script>





<meta property="og:locale" content="en" />

<meta property="og:type" content="article">
<meta name="description" content="Based on NG&#39;s Machine Learning course" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:creator" content="@janedoe">
<meta name="twitter:title" content="Comprehensive Notes on Andrew Ng‚Äôs Machine Learning Course" />
<meta name="twitter:image" content="https://mengwoods.github.io/images/thumbnail.png"/>
<meta property="og:url" content="https://mengwoods.github.io/post/dl/005-ng-ml/" />
<meta property="og:title" content="Comprehensive Notes on Andrew Ng‚Äôs Machine Learning Course" />
<meta property="og:description" content="Based on NG&#39;s Machine Learning course" />
<meta property="og:image" content="https://mengwoods.github.io/images/thumbnail.png" />

<link rel="apple-touch-icon" sizes="180x180" href="https://mengwoods.github.io/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://mengwoods.github.io/icons/favicon-32x32.png">
<link rel="manifest" href="https://mengwoods.github.io/icons/site.webmanifest">

<link rel="canonical" href="https://mengwoods.github.io/post/dl/005-ng-ml/">



<link rel="preload" href="https://mengwoods.github.io/css/styles.42e2c5f6d8cf9c52872666f8d8b2678ad0c426978b9d78aff3c33b7a1e7f6f97f54bcdaf0518a25fb0fe26367d04f8b07c683b3b38b331cb098daadee06b1f3e.css" integrity = "sha512-QuLF9tjPnFKHJmb42LJnitDEJpeLnXiv88M7eh5/b5f1S82vBRiiX7D&#43;JjZ9BPiwfGg7OzizMcsJjare4GsfPg==" as="style" crossorigin="anonymous">



<link rel="preload" href="https://mengwoods.github.io/en/js/bundle.355cf69c98ebdc2dfd538b625fe47ecdcb4e20c2138f80e51cf5425011e8235debe8bf35913e718ee1c24fd71d1f40c12f73985c98cc237a2996520f768509af.js" as="script" integrity=
"sha512-NVz2nJjr3C39U4tiX&#43;R&#43;zctOIMITj4DlHPVCUBHoI13r6L81kT5xjuHCT9cdH0DBL3OYXJjMI3opllIPdoUJrw==" crossorigin="anonymous">


<link rel="stylesheet" type="text/css" href="https://mengwoods.github.io/css/styles.42e2c5f6d8cf9c52872666f8d8b2678ad0c426978b9d78aff3c33b7a1e7f6f97f54bcdaf0518a25fb0fe26367d04f8b07c683b3b38b331cb098daadee06b1f3e.css" integrity="sha512-QuLF9tjPnFKHJmb42LJnitDEJpeLnXiv88M7eh5/b5f1S82vBRiiX7D&#43;JjZ9BPiwfGg7OzizMcsJjare4GsfPg==" crossorigin="anonymous">


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2798956128980577"
     crossorigin="anonymous"></script>
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false}
          ]
        });"></script>
  </head>
  <body
    data-code="7"
    data-lines="false"
    id="documentTop"
    data-lang="en"
  >

<header class="nav_header" >
  <nav class="nav"><a href='https://mengwoods.github.io/' class="nav_brand nav_item" title="Menghao blog">
  <img src="https://mengwoods.github.io/logos/logo.png" class="logo" alt="Menghao blog">
  <div class="nav_close">
    <div><svg class="icon">
  <title>open-menu</title>
  <use xlink:href="#open-menu"></use>
</svg>
<svg class="icon">
  <title>closeme</title>
  <use xlink:href="#closeme"></use>
</svg>
</div>
  </div>
</a>

    <div class='nav_body nav_body_left'>
      
      
      
        

  <div class="nav_parent">
    <a href="https://mengwoods.github.io/" class="nav_item" title="Home">Home </a>
  </div>
  <div class="nav_parent">
    <a href="https://mengwoods.github.io/categories/math/" class="nav_item" title="Math">Math </a>
  </div>
  <div class="nav_parent">
    <a href="https://mengwoods.github.io/categories/dl/" class="nav_item" title="AI">AI </a>
  </div>
  <div class="nav_parent">
    <a href="https://mengwoods.github.io/categories/tech/" class="nav_item" title="Tech">Tech </a>
  </div>
  <div class="nav_parent">
    <a href="https://mengwoods.github.io/categories/code/" class="nav_item" title="Code">Code </a>
  </div>
  <div class="nav_parent">
    <a href="https://mengwoods.github.io/categories/hobby/" class="nav_item" title="Others">Others </a>
  </div>
  <div class="nav_parent">
    <a href="https://mengwoods.github.io/about/" class="nav_item" title="About Me">About Me </a>
  </div>
      
      <div class="nav_parent">
        <a href="#" class="nav_item">üåê</a>
        <div class="nav_sub">
          <span class="nav_child"></span>
          
          <a href="https://mengwoods.github.io/" class="nav_child nav_item">English</a>
          
          <a href="https://mengwoods.github.io/cn/" class="nav_child nav_item">Chinese</a>
          
        </div>
      </div>
<div class='follow'>
  <a href="https://github.com/MengWoods">
    <svg class="icon">
  <title>github</title>
  <use xlink:href="#github"></use>
</svg>

  </a>
  <a href="https://www.linkedin.com/in/wumenghao/">
    <svg class="icon">
  <title>linkedin</title>
  <use xlink:href="#linkedin"></use>
</svg>

  </a>
<div class="color_mode">
  <input type="checkbox" class="color_choice" id="mode">
</div>

</div>

    </div>
  </nav>
</header>

    <main>
  
<div class="grid-inverse wrap content">
  <article class="post_content">
    <h1 class="post_title">Comprehensive Notes on Andrew Ng‚Äôs Machine Learning Course</h1>
  <div class="post_meta">
    <span><svg class="icon">
  <title>calendar</title>
  <use xlink:href="#calendar"></use>
</svg>
</span>
    <span class="post_date">
      Apr 25, 2024</span>
    <span class="post_time"> ¬∑ 21 min read</span>
    <span class="page_only">&nbsp;¬∑
  <div class="post_share">
    Share on:
    <a href="https://twitter.com/intent/tweet?text=Comprehensive%20Notes%20on%20Andrew%20Ng%e2%80%99s%20Machine%20Learning%20Course&url=https%3a%2f%2fmengwoods.github.io%2fpost%2fdl%2f005-ng-ml%2f&tw_p=tweetbutton" class="twitter" title="Share on Twitter" target="_blank" rel="nofollow">
      <svg class="icon">
  <title>twitter</title>
  <use xlink:href="#twitter"></use>
</svg>

    </a>
    <a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fmengwoods.github.io%2fpost%2fdl%2f005-ng-ml%2f&t=Comprehensive%20Notes%20on%20Andrew%20Ng%e2%80%99s%20Machine%20Learning%20Course" class="facebook" title="Share on Facebook" target="_blank" rel="nofollow">
      <svg class="icon">
  <title>facebook</title>
  <use xlink:href="#facebook"></use>
</svg>

    </a>
    <a href="#linkedinshare" id = "linkedinshare" class="linkedin" title="Share on LinkedIn" rel="nofollow">
      <svg class="icon">
  <title>linkedin</title>
  <use xlink:href="#linkedin"></use>
</svg>

    </a>
    <a href="https://mengwoods.github.io/post/dl/005-ng-ml/" title="Copy Link" class="link link_yank">
      <svg class="icon">
  <title>copy</title>
  <use xlink:href="#copy"></use>
</svg>

    </a>
  </div>
  </span>
  </div>

      <div class="post_toc">
        <h2>Overview</h2>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#c1-supervised-machine-learning---regression-and-classification">C1-Supervised Machine Learning - Regression and Classification</a>
      <ul>
        <li><a href="#p25-feature-scaling">P25 Feature scaling</a></li>
        <li><a href="#p29-feature-engineering-p30-multiple-linear-regression">P29 Feature Engineering, P30 Multiple linear regression</a></li>
        <li><a href="#p31-motivation">P31 Motivation</a></li>
        <li><a href="#p32-logistic-regression">P32 Logistic regression</a></li>
        <li><a href="#p33-decision-boundary">P33 Decision Boundary</a></li>
        <li><a href="#p34-cost-function-for-logistic-regression">P34 Cost Function for Logistic Regression</a></li>
        <li><a href="#p35-simplified-loss-function">P35 Simplified Loss function</a></li>
        <li><a href="#p37-overfitting">P37 Overfitting</a></li>
        <li><a href="#p38-addressing-overfitting">P38 Addressing Overfitting</a></li>
        <li><a href="#p39-regularization">P39 Regularization</a></li>
        <li><a href="#p40-regularization-for-linear-regression">P40 Regularization for Linear Regression</a></li>
      </ul>
    </li>
    <li><a href="#c2-advanced-learning-algorithms">C2-Advanced Learning Algorithms</a>
      <ul>
        <li><a href="#p45-requirement-prediction">P45 Requirement prediction</a></li>
        <li><a href="#p51">P51</a></li>
        <li><a href="#p61">P61</a></li>
        <li><a href="#p62-activation-function">P62 Activation function</a></li>
        <li><a href="#p66-multiclass-softmax">P66 Multiclass, Softmax</a></li>
        <li><a href="#p69-multi-label-classification">P69 Multi-label Classification</a></li>
        <li><a href="#p79-advance-optimization-algorithm-adam">P79 Advance Optimization Algorithm (Adam)</a></li>
        <li><a href="#p71-convolutional-layer">P71 Convolutional Layer</a></li>
        <li><a href="#p75-diagnostic">P75 Diagnostic</a></li>
        <li><a href="#p77-model-selection">P77 Model Selection</a></li>
        <li><a href="#p78-biasvariance">P78 Bias/Variance</a></li>
        <li><a href="#p84-ml-development">P84 ML Development</a></li>
        <li><a href="#p89-data-augmentation">P89 Data Augmentation</a></li>
        <li><a href="#p90-transfer-learning">P90 Transfer learning</a></li>
        <li><a href="#p88-mlops">P88 MLOps</a></li>
        <li><a href="#p90-skewed-datasets">P90 Skewed Datasets</a></li>
        <li><a href="#p91-decision-trees">P91 Decision Trees</a></li>
      </ul>
    </li>
    <li><a href="#c3-unsupervised-learning">C3-Unsupervised Learning</a></li>
  </ul>
</nav>
      </div>
    
    <div class="post_body"><p>Based on NG's Machine Learning course</p>
<h2 id="c1-supervised-machine-learning---regression-and-classification">C1-Supervised Machine Learning - Regression and Classification</h2>
<p><figure>
  <picture>

    
      
        
        
        
        
        
        
    <img
      loading="lazy"
      decoding="async"
      alt="alt text"
      
        class="image_figure image_internal image_unprocessed"
        src="image-1.png"
      
      
    />

    </picture>
</figure>
</p>
<ul>
<li>Regression: Fitting a function that predicts a continuous or probabilistic output from input variables. E.g. linear regression, polynomial regression.</li>
</ul>
<h3 id="p25-feature-scaling">P25 Feature scaling</h3>
<p>Features may have different units and magnitudes, algorithms perform better when features are on a comparable scale.</p>
<ul>
<li>Prevent domination by large-scale features, algorithms like gradient descent, KNN, and clustering can be biased toward features with larger numerical values.</li>
<li>Faster convergence in optimization.</li>
</ul>
<p>Common methods of feature scaling:</p>
<ul>
<li>Min-max normalization, keep data in a range, it is sensitive to outliers.</li>
<li>Standardization or Z-score normalization. Centers data at mean 0, standard deviation to 1. Works better when data has outliers.</li>
<li>Robust scaling (using median and IQR), good for datasets with many outliers.</li>
</ul>
<p>Mean normalization: $x_i:=\dfrac{x_i - \mu_i}{max-min}$</p>
<p><strong>Z-score Normalization</strong> $x_i:=\dfrac{x_i - \mu_i}{\sigma_i}$</p>
<p>All features will have a mean of 0 and a standard deviation of 1. $\sigma$ is the standard deviation$</p>
<p>The scaled features get very accurate results much, much faster.</p>
<h3 id="p29-feature-engineering-p30-multiple-linear-regression">P29 Feature Engineering, P30 Multiple linear regression</h3>
<p><strong>Feature Engineering</strong>: Using intuition to design new features, by transforming or combining original features. So the machine learning model can understand the data better and perform more accurately.</p>
<p>It turns raw sensory signals into meaningful information that makes patterns easier for a model to learn.</p>
<blockquote>
<p>quadratic function: ‰∫åÊ¨°ÊñπÁ®ã, cubic function: ‰∏âÊ¨°ÊñπÁ®ã</p>
</blockquote>
<p>When doing feature engineering, the feature scaling becomes increasingly important.</p>
<p>Gradient descent is picking the 'correct' features for us by emphasizing its associated parameter.</p>
<h3 id="p31-motivation">P31 Motivation</h3>
<p>Binary classification, negative/positive class</p>
<h3 id="p32-logistic-regression">P32 Logistic regression</h3>
<p>Logistic regression is a supervised learning algorithm used for binary classification. Instead of predicting a continuous value like in linear regression, it predicts the probability that a sample belongs to a class.</p>
<ul>
<li>Logistic regression assumes that input features have a linear relationship with some underlying score.</li>
<li>But probabilities must be between 0 and 1, so we pass that score through a sigmoid function.</li>
</ul>
<blockquote>
<p>tumor: ËÇøÁò§  malignant ÊÅ∂ÊÄßÁöÑ</p>
</blockquote>
<p><strong>Logistic function is sigmoid function</strong></p>
<p>$g(z)=\dfrac{1}{1+e^{(-z)}}$</p>
<p>This maps any real number $z$ into the range (0, 1). <strong>Maps values to probabilities</strong>. Logistic means logical reasoning or calculation.</p>
<ul>
<li>This S-shape curve is called the logistic curve, originally used in population growth modeling in the 19th century.</li>
</ul>
<p>Logistic regression model:</p>
<p>$f_{w,b}{x}=g(w\cdot x+b)=\dfrac{1}{1+e^{-(w\cdot x + b)}}$</p>
<p>$f_{w,b}(x)=P(y=1|x;w,b)$ , That means: Probability that $y$ is 1, given input $x$, parameters $w,b$</p>
<p>Even though logistic regression deals with classification, not continuous values, it is still called regression because it models a continuous probability, then we threshold that probability. So logistic regression is regression in form, classification in purpose.</p>
<h3 id="p33-decision-boundary">P33 Decision Boundary</h3>
<p>A decision boundary is a line, plane, or hypersurface that separates different classes predicted by a classifier.</p>
<p>Decision boundary: $z=w\cdot x + b=0$</p>
<p>Non-linear decision boundaries,</p>
<h3 id="p34-cost-function-for-logistic-regression">P34 Cost Function for Logistic Regression</h3>
<p>Loss function: $L(f_{w,b}(x),y)$</p>
<p>If y = 1 $= -\log(f_{w,b}(x))$</p>
<p>If y = 0 $= -\log(1-f_{w,b}(x))$</p>
<h3 id="p35-simplified-loss-function">P35 Simplified Loss function</h3>
<p>Loss function measures the error for a single training example:</p>
<p>$L(f_{w,b}(x,y))=-y\log(f_{w,b}(x))-(1-y)\log(1-f_{w,b}(x))$</p>
<p>Cost function measures the average error across the entire training set:</p>
<p>$J(w,b)=\dfrac{1}{m}\sum[L(f_{w,b}(x),y)]=-\dfrac{1}{m}\sum[y\log(f_{w,b}(x))-(1-y)\log(1-f_{w,b}(x))]$</p>
<p>Why choose this as the cost function, it derives from statistics using maximum likelihood estimation, which is an idea from statistics on how to efficiently find parameters from different models.</p>
<h3 id="p37-overfitting">P37 Overfitting</h3>
<p>Overfitting happens when a model learns the training data too well, including its noise, outliers, and random fluctuations, instead of just the underlying pattern.</p>
<p>Underfit, high bias; Overfit, high variance</p>
<p>Causes of overfitting:</p>
<ul>
<li>Too complex model</li>
<li>Too few training samples</li>
<li>Noisy data</li>
<li>Too many training epochs without regularization</li>
</ul>
<h3 id="p38-addressing-overfitting">P38 Addressing Overfitting</h3>
<ol>
<li>More training data, collect more data.</li>
<li>REgularization (L1, L2 penalties)</li>
<li>Dropout in neural networks</li>
<li>Early stopping, stop training before overfitting.</li>
<li>Cross validation to check generalization</li>
<li>Simplify the model, reduce depth, features, parameters.</li>
<li>Feature selection, drop redundant or irrelevant features, use domain knowledge or use statistical methods. All features + insufficient data = Overfitting, this is the curse of dimensionality.</li>
</ol>
<h3 id="p39-regularization">P39 Regularization</h3>
<p>Regularization discourages a model from becoming too complex.</p>
<ul>
<li>A model with very large weights $w$ tends to fit the training data too perfectly, including noise.</li>
<li>Regularization adds a penalty to the cost function, so the model prefers simpler weights (smaller values).</li>
</ul>
<p>$J(w,b)=\dfrac{1}{2m}\sum(f(x)-y)^2 + \dfrac{\lambda}{2m}\sum w^2, \lambda &gt; 0$</p>
<p>L2 regularization (Ridge):</p>
<ul>
<li>Adds the sum of squared weights to the cost function.</li>
<li>Shrinks weights smoothly, keeps all features but reduces influence.</li>
<li>Decision boundary becomes smoother.</li>
</ul>
<p>$J(w,b)=\dfrac{1}{2m}\sum(f(x)-y)^2 + \dfrac{\lambda}{m}\sum |w|, \lambda &gt; 0$</p>
<p>L1 regularization (Lasso)</p>
<ul>
<li>Adds the sum of absolute weights to the cost function.</li>
<li>Some weights shrink to zero, <strong>automatic feature selection</strong>.</li>
<li>Creates sparse models.</li>
</ul>
<h3 id="p40-regularization-for-linear-regression">P40 Regularization for Linear Regression</h3>
<p>In linear regression, if we have too many features or correlated features, weights can be come very large. Apply regularization prevents overfitting:</p>
<ul>
<li>L2: small weights smooth solution.</li>
<li>L1: sparse weights, feature selection.</li>
<li>Elastic Net: mix of both</li>
</ul>
<h2 id="c2-advanced-learning-algorithms">C2-Advanced Learning Algorithms</h2>
<p><figure>
  <picture>

    
      
        
        
        
        
        
        
    <img
      loading="lazy"
      decoding="async"
      alt="alt text"
      
        class="image_figure image_internal image_unprocessed"
        src="image-2.png"
      
      
    />

    </picture>
</figure>
</p>
<h3 id="p45-requirement-prediction">P45 Requirement prediction</h3>
<p>Inference (prediction); activation; activation values;</p>
<p>$a = f(x) = \dfrac{1}{1+e^{wx+b}}$</p>
<p>$a^{[1]}$ denotes the output (activation value) of layer 1. input layer is layer 0.</p>
<p>$a_j^{[l]}=g(w_j^{[l]}\cdot a^{[l-1]}+b_j^[l])$</p>
<p>$g$ is the activation function</p>
<h3 id="p51">P51</h3>
<p>Matrix 2x3 in Numpy <code>x = np.array([[1,2,3], [4,5,6]])</code>
Matrix is 2D array.</p>
<p>Vector is 1D array. <code>x = np.array([200, 17])</code>, just a list.</p>
<h3 id="p61">P61</h3>
<p>In <strong>binary classification</strong> problem, The <strong>logistic loss function</strong>, also known as <strong>binary cross entropy</strong> is used to measure the performance:</p>
<p>$
L(y, \hat{y}) = -\left[ y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right]
$</p>
<ul>
<li>Penalizes confident but wrong predictions heavily.</li>
<li>Rewards predictions that are close to the true class.</li>
<li>Ensures smooth and differentiable optimization for training neural networks or logistic regression models.</li>
</ul>
<p>Compute derivatives for gradient descent using back propagation.</p>
<h3 id="p62-activation-function">P62 Activation function</h3>
<p>The purpose of activation function is to introduce <strong>nonlinearity</strong>, allowing the network to learn complex relationships between inputs and outputs. Without activation function, the entire neural network would just be a linear function, no matter how many layers it had.</p>
<p>Common activation functions:</p>
<table>
<thead>
<tr>
<th>Function</th>
<th>Formula</th>
<th>Output Range</th>
<th>Key Characteristics</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Sigmoid (Logistic)</strong></td>
<td>$\sigma(z) = \frac{1}{1 + e^{-z}}$</td>
<td>(0, 1)</td>
<td>Smooth ‚ÄúS‚Äù curve, good for probabilities (used in logistic regression)</td>
</tr>
<tr>
<td><strong>Tanh</strong></td>
<td>$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$</td>
<td>(-1, 1)</td>
<td>Centered around 0, stronger gradients than sigmoid</td>
</tr>
<tr>
<td><strong>ReLU (Rectified Linear Unit)</strong></td>
<td>$f(z) = \max(0, z)$</td>
<td>[0, ‚àû)</td>
<td>Very popular ‚Äî fast, sparse, reduces vanishing gradients</td>
</tr>
<tr>
<td><strong>Leaky ReLU</strong></td>
<td>$f(z) = \max(0.01z, z)$</td>
<td>(-‚àû, ‚àû)</td>
<td>Fixes ReLU‚Äôs ‚Äúdead neuron‚Äù problem</td>
</tr>
<tr>
<td><strong>Softmax</strong></td>
<td>$f_i(z) = \frac{e^{z_i}}{\sum_j e^{z_j}}$</td>
<td>(0, 1), sum=1</td>
<td>Converts vector to probability distribution for multi-class classification</td>
</tr>
</tbody>
</table>
<p>Commen used one is ReLU (rectified linear unit) $g(z)=\max (0,z)$.</p>
<ul>
<li>In backbones, typical choice is ReLU or Leaky ReLU, they are simple and computationally cheap, important for real-time inference, leaky ReLU avoids dead neurons when inputs are negative.</li>
<li>In necks, ReLU is used, smooth gradient flow is critical for training stability, especially during feature fusion.</li>
<li>In detection head
<ul>
<li>For multi-label classification, use Sigmoid. Each class is treated independently, can detect overlapping categories.</li>
<li>For single label classification, use Softmax, ensures probabilities sum to 1, only one class per proposal.</li>
</ul>
</li>
</ul>
<h4 id="p63">P63</h4>
<p><strong>In classification problems, the sigmoid function (binary) or softmax (multi-class) is a natural choice for the output layer since they produce probabilities.</strong></p>
<p>In regression, a linear activation is used for unrestricted outputs, or ReLU/Softplus if outputs must be non-negative.</p>
<p>For hidden layers, ReLU (or its variants) is most commonly used today because it‚Äôs fast, simple, and avoids vanishing gradients, while sigmoid is rarely used due to its two flat regions that slow down gradient descent.</p>
<h3 id="p66-multiclass-softmax">P66 Multiclass, Softmax</h3>
<p><strong>Softmax is used for multiclass classification problem</strong></p>
<p>Softmax regression, N possible outputs</p>
<p>$z_j = w_j \cdot x + b_j, j=1,...,N$</p>
<p>Activation function: $a_j=\dfrac{e^{z_j}}{\sum_{k=1}^{N}e^{z_k}}=\mathbf{P}(y=j|x)$</p>
<p>$a_1 + ... + a_N=1$</p>
<p><strong>If use softmax for binary classification, it is same as sigmoid activation.</strong></p>
<h4 id="p67-p68">P67, P68</h4>
<p>For digit recognition problem, use softmax as output layer, and use 10 neutron to form the output layer.</p>
<p>Loss function name in TF is SparseCategoricalCrossentropy.</p>
<p>TF, <code>from_logits=True</code> will make the round off more accurate when with softmax.</p>
<p><code>model.compile(loss=SparseCategoricalCrossEntropy(from_logits=True))</code></p>
<h3 id="p69-multi-label-classification">P69 Multi-label Classification</h3>
<p>Can use sigmoid activations function for the output layer.</p>
<h3 id="p79-advance-optimization-algorithm-adam">P79 Advance Optimization Algorithm (Adam)</h3>
<p>Adam (Adaptive moment estimation) algorithm is one of the most popular optimization algorithm. It automatically adjusts the learning rate alpha. It not just one alpha, but each neutro has one learning rate.</p>
<p>It combines the advantages of two other methods:</p>
<ul>
<li>AdaGrad: adapts learning rates for each parameter individually</li>
<li>RMSProp: smooths learning rate using a moving average of squared gradients
So Adam is essentially adaptive learning rate + momentum. <strong>It keeps track of both average gradient and average squared gradient, making training efficient and robust.</strong></li>
</ul>
<p>In gradient descent, updates are simple: $\theta_t = \theta_{t-1} - \alpha \nabla_{\theta} J(\theta)$, But this can be slow or unstable because:</p>
<ul>
<li>Some parameters may need smaller steps (steep gradients)</li>
<li>Others need larger steps (flat regions)</li>
<li>Gradients may oscillate in certain directions</li>
</ul>
<p>Adam automatically adjusts the learning rate for each parameter based on gradient history, making training faster and more stable.</p>
<p>If <code>w</code> or <code>b</code> keeps moving in same direction, increase alpha. If keeps oscillating, reduce alpha.</p>
<p>In code, model compile select optimizer use Adam. It uses one default init learning rate.</p>
<p>It is better than gradient descent algorithm.</p>
<p>Algorithm Steps:</p>
<ol>
<li>For parameter $\theta$, compute gradient: $g_t = \nabla_\theta J_t \theta$</li>
<li>Update moment estimates:
<ol>
<li>Fist moment (mean of gradients): $m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t$</li>
<li>Second moment (uncentered variance): $v_t = \beta_2 v_{t-1} + (1-\beta_2)g^2_t$</li>
</ol>
</li>
<li>Bias correction (for initialization bias):
<ol>
<li>$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$</li>
<li>$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$</li>
</ol>
</li>
<li>Parameter update: $\theta_t = \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$</li>
</ol>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Meaning</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>( $\alpha$ )</td>
<td>Learning rate</td>
<td>0.001</td>
</tr>
<tr>
<td>( $\beta_1$ )</td>
<td>Decay for first moment</td>
<td>0.9</td>
</tr>
<tr>
<td>( $\beta_2$ )</td>
<td>Decay for second moment</td>
<td>0.999</td>
</tr>
<tr>
<td>( $\epsilon$ )</td>
<td>Small constant for stability</td>
<td>1e-8</td>
</tr>
</tbody>
</table>
<p>Pros:</p>
<ul>
<li>Fast convergence</li>
<li>Works well in practice</li>
<li>Handles sparse gradients</li>
<li>Combines benefits of Momentum and RMSProp</li>
</ul>
<p>Cons:</p>
<ul>
<li>Can overfit if learning rate isn't tuned</li>
<li>Sometimes converges to slightly worse minima than SGD with momentum</li>
<li>Requires more memory</li>
</ul>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Type</th>
<th>What it does</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SGD (Stochastic Gradient Descent)</strong></td>
<td>Optimization algorithm</td>
<td>Updates weights in the opposite direction of gradient using random mini-batches</td>
</tr>
<tr>
<td><strong>Adam (Adaptive Moment Estimation)</strong></td>
<td>Optimization algorithm</td>
<td>Uses momentum + adaptive learning rate for each parameter</td>
</tr>
<tr>
<td><strong>RMSProp, AdaGrad, AdamW, AdamP, etc.</strong></td>
<td>Optimization variants</td>
<td>Improve convergence speed, stability, or generalization</td>
</tr>
</tbody>
</table>
<h3 id="p71-convolutional-layer">P71 Convolutional Layer</h3>
<p>A convolutional layer is a special type of neural network layer designed to automatically learn spatial patterns in data. Instead of connecting every neuron to every input pixel like a fully connected layer, a convolutional layer only looks at a small local region (receptive field) and slides that filter across the entire image.</p>
<ul>
<li>Faster computation</li>
<li>Need less training data, less prone to overfitting.</li>
<li>Learns local spatial features</li>
<li>Weight sharing, same filter slides over the whole image, fewer parameters than fully connected layers.</li>
<li>Translation invariance: detects the same pattern anywhere in the image.</li>
</ul>
<p>Key Hyperparameters</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Meaning</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Filter size (kernel)</strong></td>
<td>size of sliding window</td>
<td>3√ó3, 5√ó5</td>
</tr>
<tr>
<td><strong>Stride</strong></td>
<td>how far the filter moves each step</td>
<td>1, 2</td>
</tr>
<tr>
<td><strong>Padding</strong></td>
<td>add zeros around input borders to preserve size</td>
<td>&quot;same&quot; or &quot;valid&quot;</td>
</tr>
<tr>
<td><strong>Number of filters</strong></td>
<td>number of output feature maps</td>
<td>16, 32, 64, ...</td>
</tr>
</tbody>
</table>
<p>Structure of a Conv Layer: input layer -&gt; Conv2D -&gt; activation -&gt; Pooling -&gt; Next layer</p>
<p><figure>
  <picture>

    
      
        
        
        
        
        
        
    <img
      loading="lazy"
      decoding="async"
      alt="alt text"
      
        class="image_figure image_internal image_unprocessed"
        src="image.png"
      
      
    />

    </picture>
</figure>
</p>
<ul>
<li>Conv2D: performs convolution (learns filters)</li>
<li>Activation (e.g., ReLU): introduces non-linearity</li>
<li>Pooling: reduces spatial size (downsampling)</li>
</ul>
<h3 id="p75-diagnostic">P75 Diagnostic</h3>
<p>Diagnostic: A test that you run to gain insight into what is/isn't working with a learning algorithm, to gain guidance into improving its performance.</p>
<p>Common Diagnostics Examples:</p>
<table>
<thead>
<tr>
<th>Diagnostic</th>
<th>What It Tells You</th>
<th>How It Helps</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Training vs. validation error</strong></td>
<td>Whether your model has high bias or high variance</td>
<td>Guides whether to add data, regularization, or model capacity</td>
</tr>
<tr>
<td><strong>Learning curve</strong></td>
<td>How error changes with more training data</td>
<td>Shows if adding data would help</td>
</tr>
<tr>
<td><strong>Error analysis (confusion matrix, misclassified samples)</strong></td>
<td>Which classes or examples cause mistakes</td>
<td>Helps in data cleaning or model fine-tuning</td>
</tr>
<tr>
<td><strong>Gradient checking</strong></td>
<td>Whether your implementation of backprop is correct</td>
<td>Detects bugs in training code</td>
</tr>
<tr>
<td><strong>Training loss vs. time</strong></td>
<td>Whether learning is progressing or diverging</td>
<td>Helps tune learning rate or optimizer</td>
</tr>
<tr>
<td><strong>Feature importance / ablation test</strong></td>
<td>Which features affect predictions most</td>
<td>Guides feature engineering or pruning</td>
</tr>
</tbody>
</table>
<h3 id="p77-model-selection">P77 Model Selection</h3>
<p>Training set, cross validation data, test set.</p>
<p><strong>Cross-validation</strong> (CV) is a model evaluation technique used to test how well your machine learning model generalizes to unseen data.</p>
<p>Evaluate a model using cross validation data during training, and use testing set to estimate <strong>generalization error</strong>.</p>
<ul>
<li>Cross validation reduces randomness</li>
<li>Gives a more robust estimate of the model's expected performance on new data.</li>
</ul>
<p>$k$-Fold Cross-Validation:</p>
<ol>
<li>Split dataset into $k$ equal parts</li>
<li>For each fold $i$:
<ol>
<li>Train on $k-1$ folds</li>
<li>Test on the remaining fold</li>
</ol>
</li>
<li>Compute the performance (accuracy, F1, loss, etc.) for each fold</li>
<li>Average the results and get overall performance metric.</li>
</ol>
<p>Variants of Cross-Validation</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>k-Fold</strong></td>
<td>Split into <em>k</em> equal parts</td>
<td>Most common (e.g., k = 5 or 10)</td>
</tr>
<tr>
<td><strong>Stratified k-Fold</strong></td>
<td>Keeps class ratios the same in each fold</td>
<td>Classification with imbalanced classes</td>
</tr>
<tr>
<td><strong>Leave-One-Out (LOOCV)</strong></td>
<td>Each data point is a fold (k = N)</td>
<td>Very small datasets</td>
</tr>
<tr>
<td><strong>Time-Series CV</strong></td>
<td>Uses only past data to predict future data</td>
<td>Sequential/time-dependent data</td>
</tr>
</tbody>
</table>
<h3 id="p78-biasvariance">P78 Bias/Variance</h3>
<p>$Expected¬†Error=Bias^2+Variance+Irreducible¬†Noise$</p>
<ul>
<li>Bias is error from wrong assumptions (underfitting)</li>
<li>Variance is error from sensitivity to training data (overfitting)</li>
</ul>
<p><strong>high bias</strong> - underfit
J_train is high; J_cv is high</p>
<p><strong>high variance</strong> (overfit)
J_train is low; J_cv is high</p>
<p><strong>Bias‚ÄìVariance Tradeoff</strong>:</p>
<table>
<thead>
<tr>
<th>Model Behavior</th>
<th>Bias</th>
<th>Variance</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Underfitting</strong></td>
<td>High</td>
<td>Low</td>
<td>Model too simple (misses patterns)</td>
</tr>
<tr>
<td><strong>Good fit</strong></td>
<td>Low</td>
<td>Low</td>
<td>Balanced ‚Äî best generalization</td>
</tr>
<tr>
<td><strong>Overfitting</strong></td>
<td>Low</td>
<td>High</td>
<td>Model too complex (memorizes noise)</td>
</tr>
</tbody>
</table>
<h4 id="p80-establishing-a-baseline">P80 Establishing a Baseline</h4>
<ul>
<li>High bias model (underfitting)
<ul>
<li>Model is too simple, cannot capture the true pattern.</li>
<li>Adding more data usually doesn't help much, because the model lacks capacity to learn the pattern no matter how much data you feet it.</li>
<li>Use more complex model or add better features</li>
</ul>
</li>
<li>High variance model (overfitting)
<ul>
<li>Model is too complex, it fits the training data too well and generalizes poorly.</li>
<li>Adding more data often helps, because it gives the model more examples and reduces overfitting by averaging out noise.</li>
<li>More data, stonger regularization or simpler model.</li>
</ul>
</li>
</ul>
<h4 id="p82-variance-and-bias">P82 Variance and Bias</h4>
<ul>
<li>Get more training examples - fixes high variance</li>
<li>Try smaller sets of features - fixes high variance
<ul>
<li>Reduce flexibility of the model</li>
</ul>
</li>
<li>Try getting additional features - fixes high bias</li>
<li>Try adding polynomial features - fixes high bias</li>
<li>Try decreasing lambda - high bias</li>
<li>Try increasing lambda - high variance</li>
</ul>
<p>Lambda $\lambda$ controls how strongly the model penalizes large weights, in regularization terms:</p>
<ul>
<li>L2: $J(\theta) = \text{Loss} + \lambda \sum_{i} \theta_{i}^{2}$</li>
<li>L1: $J(\theta) = \text{Loss} + \lambda \sum_{i} |\theta_{i}|$</li>
</ul>
<h4 id="p83-trade-off">P83 Trade Off</h4>
<p>Simple model -&gt; High bias
Complex model -&gt; High variance</p>
<p>Tradeoff between high bias and high variance.</p>
<p><strong>Large neural networks are low bias machines</strong>. It just fits very complicated functions very well, so when training neural network, we often facing problems other than high bias problem if neural network is large enough.</p>
<ul>
<li>
<p>Does it do well on training set? if not, use bigger network</p>
</li>
<li>
<p>Does it do well on the cross validation set? If not, use more training data.</p>
</li>
<li>
<p>A large neural network will usually do as well or better than a smaller one so long as <strong>regularization</strong> is chosen appropriately.</p>
</li>
</ul>
<h3 id="p84-ml-development">P84 ML Development</h3>
<p>Interactive loop of ML development</p>
<blockquote>
<p>Design ‚Üí Train ‚Üí Diagnose ‚Üí Improve ‚Üí Repeat</p>
</blockquote>
<ol>
<li>Choose architecture (model, data, etc.)</li>
<li>Train model. Monitor loss curves, convergence, training time, etc.</li>
<li>Diagnostics (bias, variance, error analysis)</li>
<li>Choose architecture and so on.</li>
</ol>
<p>Text Classification.
Features: list the top 10000 words to compute input variables.</p>
<p>Logistic model, or neural network model.</p>
<h4 id="p85-error-analysis">P85 Error analysis</h4>
<p>Group the misclassifies in cross validation set based on common traits/features.</p>
<p>These groups are not mutually exclusive.</p>
<p>If the dataset is large, can get samples randomly from misclassifies to analysis.</p>
<p>How to try to reduce your spam classifier's error?</p>
<ol>
<li>Collect more data</li>
<li>Develop sophisticated features based on email routing, from email header</li>
<li>Define sophisticated features from email body, some words are treated as the same world.</li>
<li>Design algorithms to detect misspellings.</li>
</ol>
<h3 id="p89-data-augmentation">P89 Data Augmentation</h3>
<p><strong>Data augmentation</strong>: Modifying an existing training example to create a new training example.</p>
<p>Examples:</p>
<p>Image recognition: Distortion a image, mirror, rotate, enlarge, shrink, change contrast.</p>
<p>Speech recognition example. Different of noisy background, bad cellphone connection.</p>
<p>Usually does not help to add purely random/meaningless noise to your data. It should be representation of the type of noise/distortions in the test set.</p>
<p><strong>Data synthesis</strong></p>
<p>Photo OCR example, generate synthesis data to train.</p>
<p>Conventional model-centric approach: AI = Code + Data; Works more on Code (algorithm/model)</p>
<p>Data-centric approach: AI = Code + Data; Focus on Data engineering.</p>
<h3 id="p90-transfer-learning">P90 Transfer learning</h3>
<p>Eliminate the last layer of the origin model, and switch it to the layer needed.</p>
<p>Use the parameter from the first layers from the origin model.</p>
<ul>
<li>
<p>Option 1: only train output layers parameters.</p>
</li>
<li>
<p>Option 2: Train all parameters. The first layers are initialized with the origin model.</p>
</li>
</ul>
<p>Two steps: Supervised pretraining, Fine tuning.</p>
<p>Convolution NN: firs layer detects edges, then corners, then curves/basic shapes.</p>
<p>Summary: 1. Download NN parameters pretrained. 2. Further train (fine tune) the network on your own data.</p>
<h3 id="p88-mlops">P88 MLOps</h3>
<p>Define project: Scope the project.</p>
<p>Collect data, Define and collect data.</p>
<p>Train model: Training, error analysis, iterative improvement.</p>
<p>Deploy in production: Deploy, monitor and maintain system.</p>
<p>Implement ML model to a inference server, Mobile app make API call to server, server inferences to mobile app.</p>
<p>Software engineering may be needed for:</p>
<ul>
<li>Ensure reliable and efficient predictions</li>
<li>Scaling</li>
<li>Logging</li>
<li>System monitoring</li>
<li>Model updates</li>
</ul>
<p><strong>MLOps</strong>: ML operations: Practice how to systematically build and deploy, maintain ML model.</p>
<p>Stages:</p>
<ol>
<li>Model Development: Data collection, preprocessing, feature engineering, model training, validation</li>
<li>Model Deployment: Packaging the model e.g. Docker, ONNX, deploying it to production servers or edge devices (API, ROS node, etc.)</li>
<li>Monitoring &amp; Maintenance: Tracking model performance, detecting data drift, retraining with new data, version control and roll back.</li>
</ol>
<h4 id="p89-ethics-pairness-bias-and-other-ethics">P89 Ethics, Pairness, bias, and other ethics.</h4>
<h3 id="p90-skewed-datasets">P90 Skewed Datasets</h3>
<p>Skewed or imbalanced datasets, the distribution of classes is uneven, some classes have many more samples than others.</p>
<ul>
<li>Model bias: Model learns to predict the majority class most of the time because it minimizes overall error.</li>
<li>Misleading accuracy: Accuracy can look high even if the model ignores minority cases.</li>
<li>Poor generalization: Fail in critical minority situation e.g. fraud detection, medical diagnosis.</li>
</ul>
<p>How to handle skewed datasets?</p>
<ol>
<li>Dataa-level approaches:
<ol>
<li>Oversampling minority class</li>
<li>Undersampling majority class</li>
<li>Data augmentation</li>
</ol>
</li>
<li>Algiorithm-level approaches:
<ol>
<li>Class weighting: Give higher penalty to misclassifying the minority class.</li>
<li>Custom loss function: use weighted cross-entropy, focal loss, etc.</li>
</ol>
</li>
<li>Evaluation-level approaches:
<ol>
<li>Use metrics that reflect imbalance better: precision, recall, f1-score, confusion matrix.</li>
</ol>
</li>
</ol>
<p>Cannot know the best model based on its accuracy rate, because the dataset might be skewed.</p>
<p>Use confusion matrix is a better matrix. Precision/recall</p>
<ul>
<li>Precision: $TP/predictedPositive = TP/(TP + FP)$</li>
<li>Recall: $TP/actualPostive = TP / (TP + FN )$</li>
</ul>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Precision</strong></td>
<td>Of all items predicted as <strong>positive</strong>, how many are actually correct</td>
</tr>
<tr>
<td><strong>Recall (Sensitivity)</strong></td>
<td>Of all actual <strong>positive</strong> items, how many did we correctly identify</td>
</tr>
</tbody>
</table>
<h4 id="p91-trading-off-precision-and-recall-">P91 Trading off Precision and Recall ‚≠ê</h4>
<ul>
<li>Rasing the logistic regression threshold will lead to higher precision, lower recall.</li>
<li>Lower the logistic regression will result in lower precision, higher recall.</li>
</ul>
<table>
<thead>
<tr>
<th>Threshold</th>
<th>Precision</th>
<th>Recall</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Low (0.3)</strong></td>
<td>‚Üì Low</td>
<td>‚Üë High</td>
<td>Catch more positives (many false alarms)</td>
</tr>
<tr>
<td><strong>High (0.9)</strong></td>
<td>‚Üë High</td>
<td>‚Üì Low</td>
<td>Only predict positives when very sure</td>
</tr>
</tbody>
</table>
<p>F1 score, $F1 score = 1/[ 1/2 (1/P + 1/R) ] = 2 PR / (P+R)$; <strong>Harmonic mean</strong> of precision and recall.</p>
<h3 id="p91-decision-trees">P91 Decision Trees</h3>
<p>Cat classification example</p>
<p>Input are categorical values (discrete)</p>
<p>Node, Topmost node is root node, Decision nodes in the middle of tree, Bottom node is leaf nodes for prediction.</p>
<h4 id="p92">P92</h4>
<p>Decision 1: how to choose what feature to split on at each node?</p>
<p>Maximize purity (or minimize impurity). (Use the most important feature)</p>
<p>Decision 2: When do you stop splitting?</p>
<ul>
<li>When a node is 100% one class</li>
<li>When splitting a node will result in the tree exceeding a maximum depth.</li>
<li>When improvements in purity score are below a threshold.</li>
<li>When number of examples in a node is below a threshold.</li>
</ul>
<h4 id="p93-measuring-purity">P93 Measuring purity</h4>
<p>Entropy as a measure of impurity</p>
<p>Entropy is from 1 to 0, the lower the entropy, the higher the purity.</p>
<p>$p_0 = 1 - p_1$</p>
<p>$H(p_1)=-p_1\log_2(p_1)-p_0\log_2(p_0)$</p>
<p>Note: $0\log(0) = 0$</p>
<p>The peak of the function is 1 by making the log base is 2.</p>
<h4 id="p94-decision-tree-learning-choose-a-split-information-gain">P94: Decision Tree learning, choose a split: information gain</h4>
<p>Choose the feature with the lowest average weighted entropy.</p>
<p>Information gain.</p>
<h4 id="p96-putting-it-together">P96: Putting it together</h4>
<ol>
<li>Start with all examples at the root node</li>
<li>Calculate information gain for all possible features, and pick the one with the highest information gain</li>
<li>Split dataset according to selected feature, and create left and right branches of the tree</li>
<li>Keep repeating splitting process until stopping criteria is met
<ol>
<li>When a node is 100% one class</li>
<li>When splitting a node will result in the tree exceeding a maximum depth</li>
<li>Information gain from additional splits is less than threshold</li>
<li>When number of examples in a node is below a threshold.</li>
</ol>
</li>
</ol>
<h4 id="p97-using-one-hot-encoding-of-categorical-features">P97: Using one-hot encoding of categorical features</h4>
<p>If a categorical feature can take on k values, create k binary features.</p>
<h4 id="p98-continuous-features">P98: Continuous features</h4>
<h4 id="p99-regression-tree">P99: Regression tree</h4>
<p>Choose the feature that split the data with lower variance.</p>
<p>Calculate the weighted variance of a feature to split on.</p>
<p>Use variance reduction as a measurement. Use the feature with largest variance reduction to split data.</p>
<blockquote>
<p>P100: Tree ensembles</p>
</blockquote>
<p>Trees are highly sensitive to small changes of the data. Using multiple trees and vote for the final result makes prediction more robust.</p>
<blockquote>
<p>P101: Sampling with replacement</p>
</blockquote>
<p>To construct a new traning set with a little bit similar but also pretty different from origin training set.</p>
<blockquote>
<p>P102: Random forest algorithm</p>
</blockquote>
<p>One powerful decision tree (Trees bag) algorithm.</p>
<p>Given training set of size $m$</p>
<p>For $b=1$ to $B$:</p>
<p>Use sampling with replacement to create a new training set of size $m$; Train a decision tree on the new dataset.</p>
<p>Randomizing the feature choice:
At each node, when choosing a feature to use to split, if $n$ features are available, pick a random subset of $k&lt;n$ features and allow the algorithm to only choose from that subset of features. (e.g. $k=sqrt(n)$)</p>
<blockquote>
<p>P103: XGBoost decision tree</p>
</blockquote>
<p>ensamble /Àå…ëÀênÀàs…ëÀêm.b…ôl/</p>
<p>It runs quickly, open source implementations are easily used.</p>
<p>Given training set of size $m$</p>
<p>For $b=1$ to $B$:</p>
<p>Use sampling with replacement to create a new training set of size $m$ But instead of picking from all examples with equal (1/m) probability, make it more likely to pick examples that the previously trained trees misclassify.</p>
<p>Train a decision tree on the new dataset.</p>
<p>XGBoost (extreme gradient boosting)</p>
<ul>
<li>Open source implementation of boosted trees</li>
<li>Fast efficient implementation</li>
<li>Good choice of default splitting criteria and criteria for when to stop splitting</li>
<li>Built in regularization to prevent overfitting</li>
<li>Highly competitive algorithm for machine learning competitions e.g. Kaggle</li>
</ul>
<p>Sample for classfication:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln">1</span><span class="cl"><span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XBGClassifier</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl">
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">()</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl">
</span></span><span class="line"><span class="ln">5</span><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">6</span><span class="cl"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
</span></span></code></pre></div><p>Regression:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Python" data-lang="Python"><span class="line"><span class="ln">1</span><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">XGBegressor</span><span class="p">()</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
</span></span></code></pre></div><blockquote>
<p>P104 Decision Trees vs Neural Networks</p>
</blockquote>
<p>Decision Trees and Tree ensembles</p>
<ul>
<li>Work well on tabular (structured) data</li>
<li>Not recommended for unstructured data, images, audio, text</li>
<li>Fast</li>
<li>Small decision trees may be human interpretable</li>
</ul>
<p>Neural network</p>
<ul>
<li>Works well on all types of data, including tabular and unstructured data.</li>
<li>May be slower than a decision tree</li>
<li>Works with transfer learning</li>
<li>When building a system of multiple models working together, it might easier to string together multiple neural networks.</li>
</ul>
<h2 id="c3-unsupervised-learning">C3-Unsupervised Learning</h2>
<blockquote>
<p>P107 Clustering</p>
</blockquote>
<p>Applications: Grouping similar news, Market segmentation, DNA analysis, Astronomical data analysis.</p>
<blockquote>
<p>P108 K-means clustering</p>
</blockquote>
<p>Cluster centroid.</p>
<p>Repeat until converged:</p>
<p>Step 1: Assign each point to its closest centroid.</p>
<p>Step 2: Recompute the centroids.</p>
<blockquote>
<p>P109 K-means algorithms</p>
</blockquote>
<p>Randomly initialize $K$ cluster centroids $\mu_1, \mu_2, ...,\mu_k$.</p>
<p>Repeat{</p>
<p>// Assign points to cluster centroids</p>
<p>for $i=1$ to $m$ training examples</p>
<p>$c^{(i)}:=$index (from 1 to K) of cluster centroid closest to $x^{(i)}$</p>
<p>// Move cluster centroids</p>
<p>for $k=1$ to K,</p>
<p>$\mu_k:=$ average of points assigned to cluster $k$</p>
<p>}</p>
<blockquote>
<p>P110 Clustering, Optimization objective</p>
</blockquote>
<p>$c^{(i)}=$ index of cluster to which example $x^{(i)} is currently assigned$</p>
<p>$\mu_k=$ cluster centroid $k$</p>
<p>$\mu_c(i)=$ cluster centroid of cluster to which example $x^{(i)}$ has been assigned</p>
<p>Cost function (Distortion function)</p>
<p>$$J(c, \mu) = \frac{1}{m}\sum_{i=1:m}||x-\mu_{c}||$$</p>
<blockquote>
<p>P111 Initilizing K-means</p>
</blockquote>
<p><strong>Randomly initialization</strong></p>
<p>Choose $K&lt;m$, clusters number is smaller than traing exampings.</p>
<p>Randomly pick K training examples, set centroids equal to these K examples.</p>
<p>To avoid local optima, can run K-means multiple times, and pick the clustering result that gave the lowest cost.</p>
<blockquote>
<p>P112 Choossing the number of clusters.</p>
</blockquote>
<p>Elbow method: Try K clusters and plot the cost function value, pick the point looks like elbow position.</p>
<p>More common method: Evaluate K-means based on a metric for how well it performs for that later purpose.</p>
<blockquote>
<p>P113 Anomaly detection</p>
</blockquote>
<p>Density estimation</p>
<p>Example:</p>
<p>Fraud detection, Model features of user;s activities from data. Identify unusual users by checking which have less probability.</p>
<blockquote>
<p>P114 Gaussian Distribution</p>
</blockquote>
<p>$x$ is a distributed Gaussian with mean $\mu$, $\sigma^2$ variance. $\sigma$ is standard deviation.</p>
<p>$p(x)=\frac{1}{\sqrt{2\pi}\sigma} e^{\frac{-(x-\mu)^2}{2\sigma^2}}$</p>
<p>$\mu=\frac{1}{m}\sum x_i$</p>
<p>$\sigma^2=\frac{1}{m}\sum(x_i-\mu)^2$</p>
<blockquote>
<p>P115 Algorithm</p>
</blockquote>
<p>Training set: ${x^{(1)},x^{(2)},...,x^{(m)}}$
Each example $x_i$ has $n$ features.</p>
<p>$p(x)=\Pi_{j=1}^{n} p(x_j;\mu_j,\sigma_j^2)$</p>
<ol>
<li>
<p>Choose $n$ features $x_i$ that you think might be indicative of anomalous examples.</p>
</li>
<li>
<p>Fit parameters $\mu_1,...,\mu_n,\sigma_1^2,...\sigma_n^2$</p>
<p>$\mu_j=\frac{1}{m}\sum x_j^{(i)}$</p>
<p>$\sigma_j^2=\frac{1}{m}(x_j^{(i)}-\mu_j)^2$</p>
</li>
<li>
<p>Given a new example $x$, compute</p>
<p>$p(x)=\Pi_{j=1}^np(x_j;\mu_j,\sigma_j^2)$</p>
<p>Anomaly if $p(x)&lt;\epsilon$</p>
</li>
</ol>
<blockquote>
<p>P116 Developing and evaluation an anomaly detection system</p>
</blockquote>
<p>Have some labeled data.</p>
<p>Training set is unlabeled dataset, assume those are normal (or anomalous).</p>
<p>Cross validation set.</p>
<p>Test set.</p>
<p>Cross validation and test sets include a few anomalous examples.</p>
<p><strong>Aircraft engines monitoring example</strong></p>
<p>10000 good engines, 20 flawed engines</p>
<p>Training set: 6000 good engines.</p>
<p>CV: 2000 good engines, 10 anomalous</p>
<p>Test: 2000 good engines, 10 anomalous</p>
<p>Train the algorithm using training set, verify the anomly detection performance on CV. Tuning $\epsilon$ on CV. Test result on Test set.</p>
<p>Alternative:</p>
<p>Training set: 6000 good engines; CV: 4000 good engines, 20 anomalous; No test set.</p>
<blockquote>
<p>Anomaly Detection VS. Supervised Learning</p>
</blockquote>
<p>Anomaly detection</p>
<ul>
<li>
<p>Very small number of positive examples. (0-20 is common), Large number of negative examples</p>
</li>
<li>
<p>Many different types of anomalies. hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous examples we've seen so far.</p>
</li>
<li>
<p>Fraud detection</p>
</li>
<li>
<p>Manufacturing - finding new previously unseen defects.</p>
</li>
<li>
<p>Monitoring machines in a data center.</p>
</li>
</ul>
<p>Supervised learning</p>
<ul>
<li>
<p>Large number of positive and negative examples.</p>
</li>
<li>
<p>Enough positive examples for algorithm to get a sense of what positive examples are like, future positve example likely to be similar to ones in training set.</p>
</li>
<li>
<p>Email spam classification.</p>
</li>
<li>
<p>Manufacturing - Finding known, previously seen defects.</p>
</li>
<li>
<p>Weather prediction.</p>
</li>
<li>
<p>Deseases classification.</p>
</li>
</ul>
<blockquote>
<p>P118 Choosing what features to use</p>
</blockquote>
<p><strong>Non-gaussian features</strong></p>
<p>Transform feature to more like Gaussian distribution.</p>
<p>Log, Square, Sqrt, etc.</p>
<p><strong>Error analysis for anomaly detection</strong></p>
<p>Choose features that might take on unusually large or small values in the event of an anomaly.</p>
<blockquote>
<p>P120 Recommended System</p>
</blockquote>
    </div>
<div class="post_comments">
  
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "https-mengwoods-github-io-1" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  
  
  
</div>




  </article>
</div>
    </main><svg width="0" height="0" class="hidden">
  <symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="facebook">
    <path d="M437 0H75C33.648 0 0 33.648 0 75v362c0 41.352 33.648 75 75 75h151V331h-60v-90h60v-61c0-49.629 40.371-90 90-90h91v90h-91v61h91l-15 90h-76v181h121c41.352 0 75-33.648 75-75V75c0-41.352-33.648-75-75-75zm0 0"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18.001 18.001" id="twitter">
    <path d="M15.891 4.013c.808-.496 1.343-1.173 1.605-2.034a8.68 8.68 0 0 1-2.351.861c-.703-.756-1.593-1.14-2.66-1.14-1.043 0-1.924.366-2.643 1.078a3.56 3.56 0 0 0-1.076 2.605c0 .309.039.585.117.819-3.076-.105-5.622-1.381-7.628-3.837-.34.601-.51 1.213-.51 1.846 0 1.301.549 2.332 1.645 3.089-.625-.053-1.176-.211-1.645-.47 0 .929.273 1.705.82 2.388a3.623 3.623 0 0 0 2.115 1.291c-.312.08-.641.118-.979.118-.312 0-.533-.026-.664-.083.23.757.664 1.371 1.291 1.841a3.652 3.652 0 0 0 2.152.743C4.148 14.173 2.625 14.69.902 14.69c-.422 0-.721-.006-.902-.038 1.697 1.102 3.586 1.649 5.676 1.649 2.139 0 4.029-.542 5.674-1.626 1.645-1.078 2.859-2.408 3.639-3.974a10.77 10.77 0 0 0 1.172-4.892v-.468a7.788 7.788 0 0 0 1.84-1.921 8.142 8.142 0 0 1-2.11.593z"
      ></path>
  </symbol>
  <symbol aria-hidden="true" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="mail">
    <path  d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="calendar">
    <path d="M452 40h-24V0h-40v40H124V0H84v40H60C26.916 40 0 66.916 0 100v352c0 33.084 26.916 60 60 60h392c33.084 0 60-26.916 60-60V100c0-33.084-26.916-60-60-60zm20 412c0 11.028-8.972 20-20 20H60c-11.028 0-20-8.972-20-20V188h432v264zm0-304H40v-48c0-11.028 8.972-20 20-20h24v40h40V80h264v40h40V80h24c11.028 0 20 8.972 20 20v48z"></path>
    <path d="M76 230h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zM76 310h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zM76 390h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zm80-80h40v40h-40z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="github">
    <path d="M255.968 5.329C114.624 5.329 0 120.401 0 262.353c0 113.536 73.344 209.856 175.104 243.872 12.8 2.368 17.472-5.568 17.472-12.384 0-6.112-.224-22.272-.352-43.712-71.2 15.52-86.24-34.464-86.24-34.464-11.616-29.696-28.416-37.6-28.416-37.6-23.264-15.936 1.728-15.616 1.728-15.616 25.696 1.824 39.2 26.496 39.2 26.496 22.848 39.264 59.936 27.936 74.528 21.344 2.304-16.608 8.928-27.936 16.256-34.368-56.832-6.496-116.608-28.544-116.608-127.008 0-28.064 9.984-51.008 26.368-68.992-2.656-6.496-11.424-32.64 2.496-68 0 0 21.504-6.912 70.4 26.336 20.416-5.696 42.304-8.544 64.096-8.64 21.728.128 43.648 2.944 64.096 8.672 48.864-33.248 70.336-26.336 70.336-26.336 13.952 35.392 5.184 61.504 2.56 68 16.416 17.984 26.304 40.928 26.304 68.992 0 98.72-59.84 120.448-116.864 126.816 9.184 7.936 17.376 23.616 17.376 47.584 0 34.368-.32 62.08-.32 70.496 0 6.88 4.608 14.88 17.6 12.352C438.72 472.145 512 375.857 512 262.353 512 120.401 397.376 5.329 255.968 5.329z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 212 212" id="gitlab">
    <path d="M12.3 74.7h54L43.3 3c-1-3.6-6.4-3.6-7.6 0L12.3 74.8z" />
    <path d="M12.3 74.7L.5 111c-1 3.2 0 6.8 3 8.8l101.6 74-92.5-119z"/>
    <path d="M105 193.7l-38.6-119h-54l92.7 119z"/>
    <path d="M105 193.7l38.7-119H66.4l38.7 119z"/>
    <path d="M105 193.7l38.7-119H198l-93 119z"/>
    <path d="M198 74.7l11.6 36.2c1 3 0 6.6-3 8.6l-101.5 74 93-119z"/>
    <path d="M198 74.7h-54.3L167 3c1.2-3.6 6.4-3.6 7.6 0L198 74.8z"/>
  </symbol>
  <symbol viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" id="rss">
    <circle cx="3.429" cy="20.571" r="3.429"></circle>
    <path d="M11.429 24h4.57C15.999 15.179 8.821 8.001 0 8v4.572c6.302.001 11.429 5.126 11.429 11.428z"></path>
    <path d="M24 24C24 10.766 13.234 0 0 0v4.571c10.714 0 19.43 8.714 19.43 19.429z"></path>
  </symbol>
  <symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="linkedin">
    <path d="M437 0H75C33.648 0 0 33.648 0 75v362c0 41.352 33.648 75 75 75h362c41.352 0 75-33.648 75-75V75c0-41.352-33.648-75-75-75zM181 406h-60V196h60zm0-240h-60v-60h60zm210 240h-60V286c0-16.54-13.46-30-30-30s-30 13.46-30 30v120h-60V196h60v11.309C286.719 202.422 296.93 196 316 196c40.691.043 75 36.547 75 79.688zm0 0"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 612 612" id="to-top">
    <path d="M604.501 440.509L325.398 134.956c-5.331-5.357-12.423-7.627-19.386-7.27-6.989-.357-14.056 1.913-19.387 7.27L7.499 440.509c-9.999 10.024-9.999 26.298 0 36.323s26.223 10.024 36.222 0l262.293-287.164L568.28 476.832c9.999 10.024 26.222 10.024 36.221 0 9.999-10.023 9.999-26.298 0-36.323z"></path>
  </symbol>
  <symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="carly">
    <path d="M504.971 239.029L448 182.059V84c0-46.317-37.682-84-84-84h-44c-13.255 0-24 10.745-24 24s10.745 24 24 24h44c19.851 0 36 16.149 36 36v108c0 6.365 2.529 12.47 7.029 16.971L454.059 256l-47.029 47.029A24.002 24.002 0 0 0 400 320v108c0 19.851-16.149 36-36 36h-44c-13.255 0-24 10.745-24 24s10.745 24 24 24h44c46.318 0 84-37.683 84-84v-98.059l56.971-56.971c9.372-9.372 9.372-24.568 0-33.941zM112 192V84c0-19.851 16.149-36 36-36h44c13.255 0 24-10.745 24-24S205.255 0 192 0h-44c-46.318 0-84 37.683-84 84v98.059l-56.971 56.97c-9.373 9.373-9.373 24.568 0 33.941L64 329.941V428c0 46.317 37.682 84 84 84h44c13.255 0 24-10.745 24-24s-10.745-24-24-24h-44c-19.851 0-36-16.149-36-36V320c0-6.365-2.529-12.47-7.029-16.971L57.941 256l47.029-47.029A24.002 24.002 0 0 0 112 192z"></path>
  </symbol>
  <symbol viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" id="copy">
    <path d="M23 2.75A2.75 2.75 0 0 0 20.25 0H8.75A2.75 2.75 0 0 0 6 2.75v13.5A2.75 2.75 0 0 0 8.75 19h11.5A2.75 2.75 0 0 0 23 16.25zM18.25 14.5h-7.5a.75.75 0 0 1 0-1.5h7.5a.75.75 0 0 1 0 1.5zm0-3h-7.5a.75.75 0 0 1 0-1.5h7.5a.75.75 0 0 1 0 1.5zm0-3h-7.5a.75.75 0 0 1 0-1.5h7.5a.75.75 0 0 1 0 1.5z"></path>
    <path d="M8.75 20.5a4.255 4.255 0 0 1-4.25-4.25V2.75c0-.086.02-.166.025-.25H3.75A2.752 2.752 0 0 0 1 5.25v16A2.752 2.752 0 0 0 3.75 24h12a2.752 2.752 0 0 0 2.75-2.75v-.75z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512.001 512.001" id="closeme">
    <path d="M284.286 256.002L506.143 34.144c7.811-7.811 7.811-20.475 0-28.285-7.811-7.81-20.475-7.811-28.285 0L256 227.717 34.143 5.859c-7.811-7.811-20.475-7.811-28.285 0-7.81 7.811-7.811 20.475 0 28.285l221.857 221.857L5.858 477.859c-7.811 7.811-7.811 20.475 0 28.285a19.938 19.938 0 0 0 14.143 5.857 19.94 19.94 0 0 0 14.143-5.857L256 284.287l221.857 221.857c3.905 3.905 9.024 5.857 14.143 5.857s10.237-1.952 14.143-5.857c7.811-7.811 7.811-20.475 0-28.285L284.286 256.002z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="open-menu">
    <path d="M492 236H20c-11.046 0-20 8.954-20 20s8.954 20 20 20h472c11.046 0 20-8.954 20-20s-8.954-20-20-20zm0-160H20C8.954 76 0 84.954 0 96s8.954 20 20 20h472c11.046 0 20-8.954 20-20s-8.954-20-20-20zm0 320H20c-11.046 0-20 8.954-20 20s8.954 20 20 20h472c11.046 0 20-8.954 20-20s-8.954-20-20-20z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id="instagram">
    <path d="M12 2.163c3.204 0 3.584.012 4.85.07 3.252.148 4.771 1.691 4.919 4.919.058 1.265.069 1.645.069 4.849 0 3.205-.012 3.584-.069 4.849-.149 3.225-1.664 4.771-4.919 4.919-1.266.058-1.644.07-4.85.07-3.204 0-3.584-.012-4.849-.07-3.26-.149-4.771-1.699-4.919-4.92-.058-1.265-.07-1.644-.07-4.849 0-3.204.013-3.583.07-4.849.149-3.227 1.664-4.771 4.919-4.919 1.266-.057 1.645-.069 4.849-.069zm0-2.163c-3.259 0-3.667.014-4.947.072-4.358.2-6.78 2.618-6.98 6.98-.059 1.281-.073 1.689-.073 4.948 0 3.259.014 3.668.072 4.948.2 4.358 2.618 6.78 6.98 6.98 1.281.058 1.689.072 4.948.072 3.259 0 3.668-.014 4.948-.072 4.354-.2 6.782-2.618 6.979-6.98.059-1.28.073-1.689.073-4.948 0-3.259-.014-3.667-.072-4.947-.196-4.354-2.617-6.78-6.979-6.98-1.281-.059-1.69-.073-4.949-.073zm0 5.838c-3.403 0-6.162 2.759-6.162 6.162s2.759 6.163 6.162 6.163 6.162-2.759 6.162-6.163c0-3.403-2.759-6.162-6.162-6.162zm0 10.162c-2.209 0-4-1.79-4-4 0-2.209 1.791-4 4-4s4 1.791 4 4c0 2.21-1.791 4-4 4zm6.406-11.845c-.796 0-1.441.645-1.441 1.44s.645 1.44 1.441 1.44c.795 0 1.439-.645 1.439-1.44s-.644-1.44-1.439-1.44z"/>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id=youtube>
    <path d="M19.615 3.184c-3.604-.246-11.631-.245-15.23 0-3.897.266-4.356 2.62-4.385 8.816.029 6.185.484 8.549 4.385 8.816 3.6.245 11.626.246 15.23 0 3.897-.266 4.356-2.62 4.385-8.816-.029-6.185-.484-8.549-4.385-8.816zm-10.615 12.816v-8l8 3.993-8 4.007z"/>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id="stackoverflow">
    <path d="M21 27v-8h3v11H0V19h3v8h18z"></path><path d="M17.1.2L15 1.8l7.9 10.6 2.1-1.6L17.1.2zm3.7 14.7L10.6 6.4l1.7-2 10.2 8.5-1.7 2zM7.2 12.3l12 5.6 1.1-2.4-12-5.6-1.1 2.4zm-1.8 6.8l13.56 1.96.17-2.38-13.26-2.55-.47 2.97zM19 25H5v-3h14v3z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id="xing">
    <path d="M18.188 0c-.517 0-.741.325-.927.66 0 0-7.455 13.224-7.702 13.657.015.024 4.919 9.023 4.919 9.023.17.308.436.66.967.66h3.454c.211 0 .375-.078.463-.22.089-.151.089-.346-.009-.536l-4.879-8.916c-.004-.006-.004-.016 0-.022L22.139.756c.095-.191.097-.387.006-.535C22.056.078 21.894 0 21.686 0h-3.498zM3.648 4.74c-.211 0-.385.074-.473.216-.09.149-.078.339.02.531l2.34 4.05c.004.01.004.016 0 .021L1.86 16.051c-.099.188-.093.381 0 .529.085.142.239.234.45.234h3.461c.518 0 .766-.348.945-.667l3.734-6.609-2.378-4.155c-.172-.315-.434-.659-.962-.659H3.648v.016z"/>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 71 55" id="discord">
    <path d="M60.1045 4.8978C55.5792 2.8214 50.7265 1.2916 45.6527 0.41542C45.5603 0.39851 45.468 0.440769 45.4204 0.525289C44.7963 1.6353 44.105 3.0834 43.6209 4.2216C38.1637 3.4046 32.7345 3.4046 27.3892 4.2216C26.905 3.0581 26.1886 1.6353 25.5617 0.525289C25.5141 0.443589 25.4218 0.40133 25.3294 0.41542C20.2584 1.2888 15.4057 2.8186 10.8776 4.8978C10.8384 4.9147 10.8048 4.9429 10.7825 4.9795C1.57795 18.7309 -0.943561 32.1443 0.293408 45.3914C0.299005 45.4562 0.335386 45.5182 0.385761 45.5576C6.45866 50.0174 12.3413 52.7249 18.1147 54.5195C18.2071 54.5477 18.305 54.5139 18.3638 54.4378C19.7295 52.5728 20.9469 50.6063 21.9907 48.5383C22.0523 48.4172 21.9935 48.2735 21.8676 48.2256C19.9366 47.4931 18.0979 46.6 16.3292 45.5858C16.1893 45.5041 16.1781 45.304 16.3068 45.2082C16.679 44.9293 17.0513 44.6391 17.4067 44.3461C17.471 44.2926 17.5606 44.2813 17.6362 44.3151C29.2558 49.6202 41.8354 49.6202 53.3179 44.3151C53.3935 44.2785 53.4831 44.2898 53.5502 44.3433C53.9057 44.6363 54.2779 44.9293 54.6529 45.2082C54.7816 45.304 54.7732 45.5041 54.6333 45.5858C52.8646 46.6197 51.0259 47.4931 49.0921 48.2228C48.9662 48.2707 48.9102 48.4172 48.9718 48.5383C50.038 50.6034 51.2554 52.5699 52.5959 54.435C52.6519 54.5139 52.7526 54.5477 52.845 54.5195C58.6464 52.7249 64.529 50.0174 70.6019 45.5576C70.6551 45.5182 70.6887 45.459 70.6943 45.3942C72.1747 30.0791 68.2147 16.7757 60.1968 4.9823C60.1772 4.9429 60.1437 4.9147 60.1045 4.8978ZM23.7259 37.3253C20.2276 37.3253 17.3451 34.1136 17.3451 30.1693C17.3451 26.225 20.1717 23.0133 23.7259 23.0133C27.308 23.0133 30.1626 26.2532 30.1066 30.1693C30.1066 34.1136 27.28 37.3253 23.7259 37.3253ZM47.3178 37.3253C43.8196 37.3253 40.9371 34.1136 40.9371 30.1693C40.9371 26.225 43.7636 23.0133 47.3178 23.0133C50.9 23.0133 53.7545 26.2532 53.6986 30.1693C53.6986 34.1136 50.9 37.3253 47.3178 37.3253Z"/>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 17 18" id="mastodon">
    <path
    fill="#ffffff"
    d="m 15.054695,9.8859583 c -0.22611,1.1632697 -2.02517,2.4363497 -4.09138,2.6830797 -1.0774504,0.12856 -2.1382704,0.24673 -3.2694704,0.19484 -1.84996,-0.0848 -3.30971,-0.44157 -3.30971,-0.44157 0,0.1801 0.0111,0.35157 0.0333,0.51194 0.24051,1.82571 1.81034,1.93508 3.29737,1.98607 1.50088,0.0514 2.8373104,-0.37004 2.8373104,-0.37004 l 0.0617,1.35686 c 0,0 -1.0498104,0.56374 -2.9199404,0.66742 -1.03124,0.0567 -2.3117,-0.0259 -3.80308,-0.42069 -3.23454998,-0.85613 -3.79081998,-4.304 -3.87592998,-7.8024197 -0.026,-1.03871 -0.01,-2.01815 -0.01,-2.83732 0,-3.57732 2.34385998,-4.62587996 2.34385998,-4.62587996 1.18184,-0.54277 3.20976,-0.77101 5.318,-0.7882499985409 h 0.0518 C 9.8267646,0.01719834 11.856025,0.24547834 13.037775,0.78824834 c 0,0 2.34377,1.04855996 2.34377,4.62587996 0,0 0.0294,2.63937 -0.32687,4.47183"/>
 <path
    fill="#000000"
    d="m 12.616925,5.6916583 v 4.3315297 h -1.71607 V 5.8189683 c 0,-0.88624 -0.37289,-1.33607 -1.1187604,-1.33607 -0.82467,0 -1.23799,0.53361 -1.23799,1.58875 v 2.30122 h -1.70594 v -2.30122 c 0,-1.05514 -0.4134,-1.58875 -1.23808,-1.58875 -0.74587,0 -1.11876,0.44983 -1.11876,1.33607 v 4.2042197 h -1.71607 V 5.6916583 c 0,-0.88527 0.22541,-1.58876 0.67817,-2.10922 0.46689,-0.52047 1.07833,-0.78727 1.83735,-0.78727 0.87816,0 1.54317,0.33752 1.98288,1.01267 l 0.42744,0.71655 0.42753,-0.71655 c 0.43961,-0.67515 1.10463,-1.01267 1.9828704,-1.01267 0.75893,0 1.37037,0.2668 1.83735,0.78727 0.45268,0.52046 0.67808,1.22395 0.67808,2.10922"/>
  </symbol>
</svg>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<footer class="footer">
  <div class="footer_inner wrap pale">
    <img src='https://mengwoods.github.io/icons/apple-touch-icon.png' class="icon icon_2 transparent" alt="Copyright ¬© 2024, Meng blog; all rights reserved.">
    <p>Copyright&nbsp;<span class="year"></span>&nbsp;COPYRIGHT ¬© 2024, MENG BLOG; ALL RIGHTS RESERVED.. All Rights Reserved</p><a class="to_top" href="#documentTop">
  <svg class="icon">
  <title>to-top</title>
  <use xlink:href="#to-top"></use>
</svg>

</a>

  </div>
</footer>

<script type="text/javascript" src="https://mengwoods.github.io/en/js/bundle.355cf69c98ebdc2dfd538b625fe47ecdcb4e20c2138f80e51cf5425011e8235debe8bf35913e718ee1c24fd71d1f40c12f73985c98cc237a2996520f768509af.js" integrity="sha512-NVz2nJjr3C39U4tiX&#43;R&#43;zctOIMITj4DlHPVCUBHoI13r6L81kT5xjuHCT9cdH0DBL3OYXJjMI3opllIPdoUJrw==" crossorigin="anonymous"></script>

  <script src="https://mengwoods.github.io/js/search.min.786102b9f5b14ee0b60197dc064e532809aec49bcc43fea72e5a513113f04b44a7241d6683d3993d5b7a48582f4986e0b9a28c72ebc48a2072c52aad8f40a2b3.js"></script>

  </body>
</html>
