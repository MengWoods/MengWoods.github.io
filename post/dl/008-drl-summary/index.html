
<!DOCTYPE html>
<html
  lang="en"
  data-figures=""
  
    class="page"
  
  
  >
  <head>
<title>An Introduction to the Fundamentals of Reinforcement Learning | Menghao blog</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7L1C01SX1E"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-7L1C01SX1E');
</script>





<meta property="og:locale" content="en" />

<meta property="og:locale:alternate" content="cn" />

<meta property="og:type" content="article">
<meta name="description" content="Fundamentals of Reinforcement Learning" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:creator" content="@janedoe">
<meta name="twitter:title" content="An Introduction to the Fundamentals of Reinforcement Learning" />
<meta name="twitter:image" content="https://mengwoods.github.io/images/thumbnail.png"/>
<meta property="og:url" content="https://mengwoods.github.io/post/dl/008-drl-summary/" />
<meta property="og:title" content="An Introduction to the Fundamentals of Reinforcement Learning" />
<meta property="og:description" content="Fundamentals of Reinforcement Learning" />
<meta property="og:image" content="https://mengwoods.github.io/images/thumbnail.png" />

<link rel="apple-touch-icon" sizes="180x180" href="https://mengwoods.github.io/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://mengwoods.github.io/icons/favicon-32x32.png">
<link rel="manifest" href="https://mengwoods.github.io/icons/site.webmanifest">

<link rel="canonical" href="https://mengwoods.github.io/post/dl/008-drl-summary/">



<link rel="preload" href="https://mengwoods.github.io/css/styles.42e2c5f6d8cf9c52872666f8d8b2678ad0c426978b9d78aff3c33b7a1e7f6f97f54bcdaf0518a25fb0fe26367d04f8b07c683b3b38b331cb098daadee06b1f3e.css" integrity = "sha512-QuLF9tjPnFKHJmb42LJnitDEJpeLnXiv88M7eh5/b5f1S82vBRiiX7D&#43;JjZ9BPiwfGg7OzizMcsJjare4GsfPg==" as="style" crossorigin="anonymous">



<link rel="preload" href="https://mengwoods.github.io/en/js/bundle.355cf69c98ebdc2dfd538b625fe47ecdcb4e20c2138f80e51cf5425011e8235debe8bf35913e718ee1c24fd71d1f40c12f73985c98cc237a2996520f768509af.js" as="script" integrity=
"sha512-NVz2nJjr3C39U4tiX&#43;R&#43;zctOIMITj4DlHPVCUBHoI13r6L81kT5xjuHCT9cdH0DBL3OYXJjMI3opllIPdoUJrw==" crossorigin="anonymous">


<link rel="stylesheet" type="text/css" href="https://mengwoods.github.io/css/styles.42e2c5f6d8cf9c52872666f8d8b2678ad0c426978b9d78aff3c33b7a1e7f6f97f54bcdaf0518a25fb0fe26367d04f8b07c683b3b38b331cb098daadee06b1f3e.css" integrity="sha512-QuLF9tjPnFKHJmb42LJnitDEJpeLnXiv88M7eh5/b5f1S82vBRiiX7D&#43;JjZ9BPiwfGg7OzizMcsJjare4GsfPg==" crossorigin="anonymous">


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2798956128980577"
     crossorigin="anonymous"></script>
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false}
          ]
        });"></script>
  </head>
  <body
    data-code="7"
    data-lines="false"
    id="documentTop"
    data-lang="en"
  >

<header class="nav_header" >
  <nav class="nav"><a href='https://mengwoods.github.io/' class="nav_brand nav_item" title="Menghao blog">
  <img src="https://mengwoods.github.io/logos/logo.png" class="logo" alt="Menghao blog">
  <div class="nav_close">
    <div><svg class="icon">
  <title>open-menu</title>
  <use xlink:href="#open-menu"></use>
</svg>
<svg class="icon">
  <title>closeme</title>
  <use xlink:href="#closeme"></use>
</svg>
</div>
  </div>
</a>

    <div class='nav_body nav_body_left'>
      
      
      
        

  <div class="nav_parent">
    <a href="https://mengwoods.github.io/" class="nav_item" title="Home">Home </a>
  </div>
  <div class="nav_parent">
    <a href="https://mengwoods.github.io/categories/math/" class="nav_item" title="Math">Math </a>
  </div>
  <div class="nav_parent">
    <a href="https://mengwoods.github.io/categories/dl/" class="nav_item" title="AI">AI </a>
  </div>
  <div class="nav_parent">
    <a href="https://mengwoods.github.io/categories/tech/" class="nav_item" title="Tech">Tech </a>
  </div>
  <div class="nav_parent">
    <a href="https://mengwoods.github.io/categories/code/" class="nav_item" title="Code">Code </a>
  </div>
  <div class="nav_parent">
    <a href="https://mengwoods.github.io/categories/hobby/" class="nav_item" title="Others">Others </a>
  </div>
  <div class="nav_parent">
    <a href="https://mengwoods.github.io/about/" class="nav_item" title="About Me">About Me </a>
  </div>
      
      <div class="nav_parent">
        <a href="#" class="nav_item">üåê</a>
        <div class="nav_sub">
          <span class="nav_child"></span>
          
          <a href="https://mengwoods.github.io/" class="nav_child nav_item">English</a>
          
          <a href="https://mengwoods.github.io/cn/" class="nav_child nav_item">Chinese</a>
          
        </div>
      </div>
<div class='follow'>
  <a href="https://github.com/MengWoods">
    <svg class="icon">
  <title>github</title>
  <use xlink:href="#github"></use>
</svg>

  </a>
  <a href="https://www.linkedin.com/in/wumenghao/">
    <svg class="icon">
  <title>linkedin</title>
  <use xlink:href="#linkedin"></use>
</svg>

  </a>
<div class="color_mode">
  <input type="checkbox" class="color_choice" id="mode">
</div>

</div>

    </div>
  </nav>
</header>

    <main>
  
<div class="grid-inverse wrap content">
  <article class="post_content">
    <h1 class="post_title">An Introduction to the Fundamentals of Reinforcement Learning</h1>
  <div class="post_meta">
    <span><svg class="icon">
  <title>calendar</title>
  <use xlink:href="#calendar"></use>
</svg>
</span>
    <span class="post_date">
      Jul 7, 2024</span>
    <span class="post_time"> ¬∑ 12 min read</span>
    <span class="page_only">&nbsp;¬∑
  <div class="post_share">
    Share on:
    <a href="https://twitter.com/intent/tweet?text=An%20Introduction%20to%20the%20Fundamentals%20of%20Reinforcement%20Learning&url=https%3a%2f%2fmengwoods.github.io%2fpost%2fdl%2f008-drl-summary%2f&tw_p=tweetbutton" class="twitter" title="Share on Twitter" target="_blank" rel="nofollow">
      <svg class="icon">
  <title>twitter</title>
  <use xlink:href="#twitter"></use>
</svg>

    </a>
    <a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fmengwoods.github.io%2fpost%2fdl%2f008-drl-summary%2f&t=An%20Introduction%20to%20the%20Fundamentals%20of%20Reinforcement%20Learning" class="facebook" title="Share on Facebook" target="_blank" rel="nofollow">
      <svg class="icon">
  <title>facebook</title>
  <use xlink:href="#facebook"></use>
</svg>

    </a>
    <a href="#linkedinshare" id = "linkedinshare" class="linkedin" title="Share on LinkedIn" rel="nofollow">
      <svg class="icon">
  <title>linkedin</title>
  <use xlink:href="#linkedin"></use>
</svg>

    </a>
    <a href="https://mengwoods.github.io/post/dl/008-drl-summary/" title="Copy Link" class="link link_yank">
      <svg class="icon">
  <title>copy</title>
  <use xlink:href="#copy"></use>
</svg>

    </a>
  </div>
  </span>
  </div>

      <div class="post_toc">
        <h2>Overview</h2>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#background">Background</a></li>
    <li><a href="#mathematical-foundations">Mathematical Foundations</a>
      <ul>
        <li><a href="#value-function">Value Function</a></li>
      </ul>
    </li>
    <li><a href="#key-reinforcement-learning-algorithms">Key Reinforcement Learning Algorithms</a>
      <ul>
        <li><a href="#deep-q-network">Deep Q-Network</a></li>
        <li><a href="#dueling-network-architecture">Dueling Network Architecture</a></li>
        <li><a href="#policy-gradient">Policy Gradient</a></li>
        <li><a href="#actor-critic-algorithm">Actor-Critic Algorithm</a></li>
      </ul>
    </li>
    <li><a href="#summary">Summary</a></li>
  </ul>
</nav>
      </div>
    
    <div class="post_body"><p>Fundamentals of Reinforcement Learning</p>
<h2 id="background">Background</h2>
<p>Reinforcement Learning (RL) is a powerful and rapidly advancing branch of machine learning, inspired by behavioral psychology. It focuses on how agents should take actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, where the learning agent is given input-output pairs, reinforcement learning emphasizes learning from interaction.</p>
<p>Reinforcement Learning (RL) is pivotal across diverse applications. In robotics, it powers autonomous navigation and manipulation tasks, while in gaming, it excels in strategic decision-making as seen with AlphaGo. RL optimizes trading in finance, adapts healthcare treatments, and enhances capabilities in natural language processing and computer vision. Its expanding reach includes smart grids, recommendation systems, and virtual assistants, highlighting RL's transformative impact across various domains.</p>
<h2 id="mathematical-foundations">Mathematical Foundations</h2>
<p>The RL problem is meant to be a straightforward framing of the problem of learning from interaction with environments $\mathcal{E}$ over several discrete time steps to achieve a goal. At each time step $t$, the agent receives a state ${s}<em>{t}$ in the environment's state space $\mathcal{S}$ and selects an action, $a_t \in \mathcal {A}(s_t)$ according to a policy $\pi({a}</em>{t}|{s}<em>{t})$, where $\mathcal{A}(s_t)$ is the set of actions available in state $s_t$. The policy amounts to a conditional probability $\pi(a|s)$ of the agent taking action if the current state is $s$. It is a mapping from state and action to the probability of taking an action. After that, the agent will receive a scalar reward ${r}</em>{t}$ and store the transition in the agent's memory as experiences. The process continues until the agent reaches a terminal state. The agent seeks to learn a policy ${ \pi }^{ \ast }$ that maximizes the expected discounted return ${ R }_{ t }=\sum <em>{ k=0 }^{ \infty  }{ { \gamma  }^{ k }{ r }</em>{ t+k } }$, accumulated reward with the discount factor $\gamma \in (0,1]$ trades-off the importance of immediate and future rewards.</p>
<p>RL tasks that satisfy the Markov property can be described as Markov decision processes (MDPs), which are defined by a tuple $(\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma)$, where $\mathcal{R}$ is a reward function $\mathcal{R}(s,a)$ and $\mathcal{P}$ is a state transition probability $\mathcal{P}({s}<em>{t+1}|{s}</em>{t},{a}<em>{t})$. The Markov property indicates the future states are conditionally independent of the past given the present. So, in an RL task, the decisions and values are assumed to be a function only of the current state. Markov property can be defined as $p({ s }</em>{ t+1 }|{ s }<em>{ 1 },{ a }</em>{ 1 },...,{ s }<em>{ t },{ a }</em>{ t }) = p({ s }<em>{ t+1 }|{ s }</em>{ t },{ a }<em>{ t })$ , which means the future states are conditionally independent of the past given the present. RL task which satisfies Markov property can be described as MDPs, defined by the 5-tuple $(\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma)$, where $\mathcal{R}$ is reward function $\mathcal{R}(s,a)$ and $\mathcal{P}$ is state transition probability $\mathcal{P}({s}</em>{t+1}|{s}<em>{t},{a}</em>{t})$. In an episodic task, the state will reset after each episode of length, and the sequence of states, actions, and rewards in an episode constitute a trajectory or rollout of the policy.</p>
<h3 id="value-function">Value Function</h3>
<p>Value functions are a core component of RL systems, which constructs a function approximator that estimates the long-term reward from any state. It estimates how good (expected return) it is for an agent to be in a given state (or given action in a given state). By this way, the function approximator exploits the structure in the state space to efficiently learn the value of observed states and generalize to the value of similar, unseen states. A typical form of value function can be defined as:</p>
<p>$${ V }^{ \pi  }(s)=\mathbb{ E }[R|s,\pi ]= \mathbb{E}[\sum <em>{ k=0 }^{ \infty  }{ { \gamma  }^{ k }{ r }</em>{ t+k } }|s,\pi] $$</p>
<p>Normally we refer to ${ V }^{ \pi  }(s)$ as the state-value function, which measures the expected discounted return when starting in a state $s$ and following a policy $\pi$. When actions follow by the optimal policy ${\pi}^{\ast}$, the state-value function can be optimal:</p>
<p>$${ V }^{ \ast }(s)=\max _{ \pi  }{ { V }^{ \pi  }(s) } \quad \forall s\in \mathcal{ S }$$</p>
<p>In addition to measuring the value of states, there is also an indicator for measuring the quality of actions' selection, which is denoted as state-action-value or quality function ${Q}^{\pi}(s,a)$. It defines the value of choosing an action $a$ from a given state $s$ and thereafter following a policy $\pi$.</p>
<p>$${ Q }^{ \pi  }(s,a)=\mathbb{ E }[R|s,a,\pi ]= \mathbb{E}[\sum <em>{ k=0 }^{ \infty  }{ { \gamma  }^{ k }{ r }</em>{ t+k } }|s,a,\pi] $$</p>
<p>State-action-value is similar to the state value $V^{\pi}$ except the initial action $a$ is provided, and the policy $\pi$ is only followed from the succeeding state onwards. The optimal state-action-value function is denoted as:</p>
<p>$${ Q }^{ \ast  }(s,a)=\max _{ \pi  }{ { Q }^{ \pi  }(s,a) }  \quad \forall s\in \mathcal{ S } , \forall a\in \mathcal{ A } $$</p>
<p>${ Q }^{ \ast  }(s,a)$ gives the maximum state-action value for state $s$ and action $a$ achievable by any policy.
This action value function satisfies a recursive property, which is a fundamental property of value functions in the RL setting, and it expresses a relationship between the value of a state and its successor states:</p>
<p>$${Q}^{\pi}(s,a)=\mathbb{E}<em>{{s}^{\prime}}[r+\gamma\mathbb{E}</em>{{a}^{\prime}\sim{\pi}({s}^{\prime})}[{Q}^{\ast}({s}^{\prime},{a}^{\prime})]|s,a,\pi] $$</p>
<p>Unlike producing absolute state-action values as with $Q^{\pi}$, an advantage function represents relative state-action values, which measures whether or not the action is better or worse than the policy's default behavior. Often, it is easier to learn that action yields higher reward than another, than it is to learn the actual return from taking one particular action. Advantage function expresses a relative advantage of actions through this simple relationship:</p>
<p>$${ A }^{ \pi  }(s,a)={ Q }^{ \pi  }(s,a)-{ V }^{ \pi  }(s) $$</p>
<p>Many successful value-based RL algorithms rely on the idea of advantage updates.</p>
<h2 id="key-reinforcement-learning-algorithms">Key Reinforcement Learning Algorithms</h2>
<h3 id="deep-q-network">Deep Q-Network</h3>
<p>Deep reinforcement learning (DRL) applies deep neural nets for representing the value functions within reinforcement learning methods. DRL algorithms have attained superhuman performance in several challenging task domains to attribute to the powerful function approximation and representation learning properties of the DL. The DQN algorithm achieves human-level performance on Atari series games from pixels input. It parameterizes the quality function $Q$ with a neural network $Q(s,a;\theta)$ that approximates the $Q$ values. Two main techniques of the DQN algorithm can learn value functions in a stable and robust way are using the target network and experience replay. At each iteration, the network's parameters are updated by minimizing the following loss function:</p>
<p>$${L}<em>{i}({\theta}</em>{i})=\mathbb{E}<em>{s,a,r,{s}^{\prime}}[({y}</em>{i}^{DQN}-Q(s,a;{\theta}_{i}))^{2}]$$</p>
<p>with</p>
<p>$${y}_{i}^{DQN}=r+\gamma \underset {{a}^{\prime}}{max}Q({s}^{\prime},{a}^{\prime};{\theta}^{-}) $$</p>
<p>in which ${\theta}^{-}$ is the parameter for the target network. The first stabilizing method is fixing the target network's parameters rather than calculating the TD error based on its own rapidly fluctuating estimates of the $Q$-values. The second one, experience replay, uses a buffer for storing a certain size of transitions $({s}<em>{t},{a}</em>{t},{s}<em>{t+1},{r}</em>{t+1},)$ makes it possible for training off-policy and enhancing the efficiency of sampling data.</p>
<p>There is a series of improvements in the value-based RL setting after the DQN algorithm ignited this field. To reduce the overestimated $Q$-values in DQN, van Hasselt et al. proposed the double DQN algorithm. Wang et al. presented a dueling Q-network architecture to estimate state-value function $V(s)$ and associated advantage function $A(s,a)$ respectively. Tamar et al. proposed a value iteration network that can effectively learn to plan, and it leads to better generalization in many RL tasks. Schaul et al. developed the PER approach built on top of double DQN, it makes the experience replay process more efficient and effective than all transitions are replayed uniformly.</p>
<h3 id="dueling-network-architecture">Dueling Network Architecture</h3>
<p>Unlike the standard single sequence $Q$-networks design, the dueling network structure consists of two sequences (streams) of networks (A-network and V-network) which separately learn action-advantage function and state-value function. This construction decouples the value and advantage functions and combines the two streams to produce the estimate of the state-action value function with a special aggregating module. The two streams share a common feature extraction layer (or lower layers). The deep $Q$-network focuses on estimating every state-action pairs' value. However, the idea of dueling network is to estimate action-independent state function and action-dependent advantage function separately, because in RL environments, not all states are related to a specific action, there are many states independent of action, and under these states the agent does not need to change actions to adapt to the new states. Therefore, it is meaningless and inefficient to estimate such state-action pairs' value. Dueling network firstly presented by Wang et al. and through this change, the training efficiency has been greatly improved than the single-stream $Q$ networks. The dueling network results in a new state of the art for tasks in the discrete action space according to Wang's work. Shortly, the $Q$-values generated by dueling network are more advantageous to the performance improvement than deep $Q$-network in an RL task.</p>
<h3 id="policy-gradient">Policy Gradient</h3>
<p>The methods mentioned above indirectly learn the policy $\pi(s)$ based on the estimate of the value functions. These value-based approaches are effective in handling problem in a discrete actions field. However, when dealing with a problem with a continuous action space such as physical control tasks, the value-based approaches cannot be straightforwardly applied, and it is difficult to ensure the results' convergence since it relies on each actions' $Q$ value. An obvious approach to implement value-based algorithms such as DQN to continuous domains is to discretize the action space to several fixed actions. However, it has many drawbacks and limitations such as throwing information (maybe essential) about the structure of the action domain.</p>
<p>There is no such worry in policy-based approaches since the policy network output agent's actions without the estimation of the action-value function. They directly parameterize the control policy $\pi(a|s;\theta)$ and update the parameters $\theta$ to optimize the cumulative reward, therefore, policy-based methods are more applicable to continuous control problem such as tasks of robotic controls than the value-based methods.</p>
<p>Policy gradient (PG) is an appealing policy-based algorithm which optimizes the parametric policy ${\pi}<em>{\theta}(a|s)=\mathbb{P}[a|s;\theta]$ following the gradient ${\nabla}</em>{\theta}J({\pi}<em>{\theta})$ of its expectation of cumulative reward with respect to the policy parameters. Policy-gradient methods are effective in high-dimensional or continuous action spaces, and can learn stochastic policies. In an RL task, the agent's goal is to find parameter $\theta$ maximizes the objective function $J(\pi)$. A typical performance objective to be considered is the average reward function: $J(\pi)=\mathbb{E}[R|{\pi}</em>{\theta}]$. The policy-gradient theorem provides the gradient of $J$ with respect to the parameters $\theta$ of policy $\pi$:</p>
<p>$${\nabla}<em>{\theta}J({\pi}</em>{\theta})=\int <em>{\mathcal{S}}^{  }{{\rho}^{\pi} }\int</em>{\mathcal{A}}^{  }{{\nabla}<em>{\theta}}{\pi}</em>{\theta}(a|s){Q}^{ \pi}(s,a)dads$$</p>
<p>$$\quad\quad\quad\quad=\mathbb{E}<em>{s\sim{\rho}^{\pi},a\sim {\pi}^{\theta}}[{\nabla}</em>{\theta} log{\pi}^{\theta}(a|s){Q}^{\pi}(s,a)] $$</p>
<p>Where the ${\rho}^{\pi}(s)$ is the state distribution. The unknown part, ${Q}^{\pi}(s,a)$ is normally estimated by using the actual returns ${ R }_{ t }=\sum <em>{ k=0 }^{ \infty  }{ { \gamma  }^{ k }{ r }</em>{ t+k } }$ as an approximation for each ${Q}^{\pi}(s_t,a_t)$. Based on this theorem, Silver et al. proposed a deterministic policy-gradient (DPG) algorithm for estimating gradient and it is more efficient than the usual stochastic policy-gradient method. O'Donoghue et al. referred to a new technique by combining PGQL and discussed the practical implementation of this technique in RL setting.</p>
<h3 id="actor-critic-algorithm">Actor-Critic Algorithm</h3>
<p>Regular policy-gradient methods often exhibit slow convergence due to the large variances of the gradient estimates. The actor-critic methods attempt to reduce the variance by adopting a critic network to estimate the value of the current policy, which is then used to update the actor's policy parameters in a direction of performance improvement.</p>
<p>The action-selection policy is known as the actor ${\pi}<em>{\theta}:\mathcal{S}\rightarrow \mathcal{A}$, which make decisions without the need for optimization procedures on a value function, mapping representation of the states to action-selection probabilities . The~value function is known as the critic ${Q}</em>{\phi}^{\pi}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$, which estimates the expected return to reduce variance and accelerate learning, mapping states to expected cumulative future reward.</p>
<p>The actor and critic are two separated networks share a common observation. At each step, the action selected by actor network is also an input factor to the critic network. In the process of policy improvement, the critic network estimates the state-action value of the current policy by DQN, then actor network updates its policy in a direction improves the $Q$-value. Compared with the previous pure policy-gradient methods, which do not have a value function, using a critic network to evaluate the current policy is more conducive to convergence and stability. The better the state-action value evaluation is, the lower the learning performance's variance is. It is important and helpful to have a better policy evaluation in the critic network.</p>
<p>Policy-gradient-based actor-critic algorithms are useful in many real-life applications because they can search for optimal policies using low-variance gradient estimates. Lillicrap et al. presented the DDPG algorithm, which combines the actor-critic approach with insights from DQN, to solve simulated physics tasks and it has been widely used in many robotic control tasks. It uses two neural networks; the actor network learns a deterministic policy and the critic network approximates the Q-function of the current policy.</p>
<h2 id="summary">Summary</h2>
<p>Reinforcement Learning (RL) represents a powerful paradigm in machine learning, drawing inspiration from behavioral psychology to enable agents to make decisions that maximize cumulative rewards in complex environments. Formulated as Markov Decision Processes (MDPs), RL tasks involve states, actions, rewards, and transition probabilities. Algorithms such as Deep Q-Networks (DQN) leverage deep neural networks to approximate Q-values efficiently, facilitating decision-making in discrete action spaces.</p>
<p>Value-based RL methods, exemplified by DQN, estimate state-action values to optimize policies. Actor-critic approaches improve upon traditional policy-gradient methods by incorporating a critic network to estimate value functions, thereby reducing variance and enhancing learning stability. These advancements extend to continuous action spaces with algorithms like Deep Deterministic Policy Gradient (DDPG), which combines deterministic policies and Q-function approximations.</p>
<p>Policy-gradient methods directly optimize policies based on gradient estimates of expected rewards, proving effective in scenarios with continuous action spaces. The dueling network architecture further refines training efficiency by separating state-value and advantage functions, emphasizing action-dependent advantages.</p>
<p>Overall, RL continues to evolve through innovations in value estimation, policy optimization, and application across diverse domains such as robotics and game playing. Advances in neural network architectures and learning algorithms continue to drive progress in RL research and application. Recent trends focus on adapting RL to continuous action spaces, integrating with other domains like NLP and computer vision, and improving sample efficiency and training stability. Future directions include enhancing RL's applicability to real-world challenges through interdisciplinary collaborations and addressing ethical considerations in deployment.</p>
    </div>
<div class="post_comments">
  
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "https-mengwoods-github-io-1" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  
  
  
</div>


<div class="post_i18n">
  <h3 class="mb-0">Translations:</h3>
  <nav class="tags_nav">
    
      <a href="https://mengwoods.github.io/cn/post/dl/008-drl-summary/" class="post_tag button button_translucent">Chinese</a> 
      
    
    </nav>
</div>



  </article>
</div>
    </main><svg width="0" height="0" class="hidden">
  <symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="facebook">
    <path d="M437 0H75C33.648 0 0 33.648 0 75v362c0 41.352 33.648 75 75 75h151V331h-60v-90h60v-61c0-49.629 40.371-90 90-90h91v90h-91v61h91l-15 90h-76v181h121c41.352 0 75-33.648 75-75V75c0-41.352-33.648-75-75-75zm0 0"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18.001 18.001" id="twitter">
    <path d="M15.891 4.013c.808-.496 1.343-1.173 1.605-2.034a8.68 8.68 0 0 1-2.351.861c-.703-.756-1.593-1.14-2.66-1.14-1.043 0-1.924.366-2.643 1.078a3.56 3.56 0 0 0-1.076 2.605c0 .309.039.585.117.819-3.076-.105-5.622-1.381-7.628-3.837-.34.601-.51 1.213-.51 1.846 0 1.301.549 2.332 1.645 3.089-.625-.053-1.176-.211-1.645-.47 0 .929.273 1.705.82 2.388a3.623 3.623 0 0 0 2.115 1.291c-.312.08-.641.118-.979.118-.312 0-.533-.026-.664-.083.23.757.664 1.371 1.291 1.841a3.652 3.652 0 0 0 2.152.743C4.148 14.173 2.625 14.69.902 14.69c-.422 0-.721-.006-.902-.038 1.697 1.102 3.586 1.649 5.676 1.649 2.139 0 4.029-.542 5.674-1.626 1.645-1.078 2.859-2.408 3.639-3.974a10.77 10.77 0 0 0 1.172-4.892v-.468a7.788 7.788 0 0 0 1.84-1.921 8.142 8.142 0 0 1-2.11.593z"
      ></path>
  </symbol>
  <symbol aria-hidden="true" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="mail">
    <path  d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="calendar">
    <path d="M452 40h-24V0h-40v40H124V0H84v40H60C26.916 40 0 66.916 0 100v352c0 33.084 26.916 60 60 60h392c33.084 0 60-26.916 60-60V100c0-33.084-26.916-60-60-60zm20 412c0 11.028-8.972 20-20 20H60c-11.028 0-20-8.972-20-20V188h432v264zm0-304H40v-48c0-11.028 8.972-20 20-20h24v40h40V80h264v40h40V80h24c11.028 0 20 8.972 20 20v48z"></path>
    <path d="M76 230h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zM76 310h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zM76 390h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zm80-80h40v40h-40z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="github">
    <path d="M255.968 5.329C114.624 5.329 0 120.401 0 262.353c0 113.536 73.344 209.856 175.104 243.872 12.8 2.368 17.472-5.568 17.472-12.384 0-6.112-.224-22.272-.352-43.712-71.2 15.52-86.24-34.464-86.24-34.464-11.616-29.696-28.416-37.6-28.416-37.6-23.264-15.936 1.728-15.616 1.728-15.616 25.696 1.824 39.2 26.496 39.2 26.496 22.848 39.264 59.936 27.936 74.528 21.344 2.304-16.608 8.928-27.936 16.256-34.368-56.832-6.496-116.608-28.544-116.608-127.008 0-28.064 9.984-51.008 26.368-68.992-2.656-6.496-11.424-32.64 2.496-68 0 0 21.504-6.912 70.4 26.336 20.416-5.696 42.304-8.544 64.096-8.64 21.728.128 43.648 2.944 64.096 8.672 48.864-33.248 70.336-26.336 70.336-26.336 13.952 35.392 5.184 61.504 2.56 68 16.416 17.984 26.304 40.928 26.304 68.992 0 98.72-59.84 120.448-116.864 126.816 9.184 7.936 17.376 23.616 17.376 47.584 0 34.368-.32 62.08-.32 70.496 0 6.88 4.608 14.88 17.6 12.352C438.72 472.145 512 375.857 512 262.353 512 120.401 397.376 5.329 255.968 5.329z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 212 212" id="gitlab">
    <path d="M12.3 74.7h54L43.3 3c-1-3.6-6.4-3.6-7.6 0L12.3 74.8z" />
    <path d="M12.3 74.7L.5 111c-1 3.2 0 6.8 3 8.8l101.6 74-92.5-119z"/>
    <path d="M105 193.7l-38.6-119h-54l92.7 119z"/>
    <path d="M105 193.7l38.7-119H66.4l38.7 119z"/>
    <path d="M105 193.7l38.7-119H198l-93 119z"/>
    <path d="M198 74.7l11.6 36.2c1 3 0 6.6-3 8.6l-101.5 74 93-119z"/>
    <path d="M198 74.7h-54.3L167 3c1.2-3.6 6.4-3.6 7.6 0L198 74.8z"/>
  </symbol>
  <symbol viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" id="rss">
    <circle cx="3.429" cy="20.571" r="3.429"></circle>
    <path d="M11.429 24h4.57C15.999 15.179 8.821 8.001 0 8v4.572c6.302.001 11.429 5.126 11.429 11.428z"></path>
    <path d="M24 24C24 10.766 13.234 0 0 0v4.571c10.714 0 19.43 8.714 19.43 19.429z"></path>
  </symbol>
  <symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="linkedin">
    <path d="M437 0H75C33.648 0 0 33.648 0 75v362c0 41.352 33.648 75 75 75h362c41.352 0 75-33.648 75-75V75c0-41.352-33.648-75-75-75zM181 406h-60V196h60zm0-240h-60v-60h60zm210 240h-60V286c0-16.54-13.46-30-30-30s-30 13.46-30 30v120h-60V196h60v11.309C286.719 202.422 296.93 196 316 196c40.691.043 75 36.547 75 79.688zm0 0"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 612 612" id="to-top">
    <path d="M604.501 440.509L325.398 134.956c-5.331-5.357-12.423-7.627-19.386-7.27-6.989-.357-14.056 1.913-19.387 7.27L7.499 440.509c-9.999 10.024-9.999 26.298 0 36.323s26.223 10.024 36.222 0l262.293-287.164L568.28 476.832c9.999 10.024 26.222 10.024 36.221 0 9.999-10.023 9.999-26.298 0-36.323z"></path>
  </symbol>
  <symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="carly">
    <path d="M504.971 239.029L448 182.059V84c0-46.317-37.682-84-84-84h-44c-13.255 0-24 10.745-24 24s10.745 24 24 24h44c19.851 0 36 16.149 36 36v108c0 6.365 2.529 12.47 7.029 16.971L454.059 256l-47.029 47.029A24.002 24.002 0 0 0 400 320v108c0 19.851-16.149 36-36 36h-44c-13.255 0-24 10.745-24 24s10.745 24 24 24h44c46.318 0 84-37.683 84-84v-98.059l56.971-56.971c9.372-9.372 9.372-24.568 0-33.941zM112 192V84c0-19.851 16.149-36 36-36h44c13.255 0 24-10.745 24-24S205.255 0 192 0h-44c-46.318 0-84 37.683-84 84v98.059l-56.971 56.97c-9.373 9.373-9.373 24.568 0 33.941L64 329.941V428c0 46.317 37.682 84 84 84h44c13.255 0 24-10.745 24-24s-10.745-24-24-24h-44c-19.851 0-36-16.149-36-36V320c0-6.365-2.529-12.47-7.029-16.971L57.941 256l47.029-47.029A24.002 24.002 0 0 0 112 192z"></path>
  </symbol>
  <symbol viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" id="copy">
    <path d="M23 2.75A2.75 2.75 0 0 0 20.25 0H8.75A2.75 2.75 0 0 0 6 2.75v13.5A2.75 2.75 0 0 0 8.75 19h11.5A2.75 2.75 0 0 0 23 16.25zM18.25 14.5h-7.5a.75.75 0 0 1 0-1.5h7.5a.75.75 0 0 1 0 1.5zm0-3h-7.5a.75.75 0 0 1 0-1.5h7.5a.75.75 0 0 1 0 1.5zm0-3h-7.5a.75.75 0 0 1 0-1.5h7.5a.75.75 0 0 1 0 1.5z"></path>
    <path d="M8.75 20.5a4.255 4.255 0 0 1-4.25-4.25V2.75c0-.086.02-.166.025-.25H3.75A2.752 2.752 0 0 0 1 5.25v16A2.752 2.752 0 0 0 3.75 24h12a2.752 2.752 0 0 0 2.75-2.75v-.75z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512.001 512.001" id="closeme">
    <path d="M284.286 256.002L506.143 34.144c7.811-7.811 7.811-20.475 0-28.285-7.811-7.81-20.475-7.811-28.285 0L256 227.717 34.143 5.859c-7.811-7.811-20.475-7.811-28.285 0-7.81 7.811-7.811 20.475 0 28.285l221.857 221.857L5.858 477.859c-7.811 7.811-7.811 20.475 0 28.285a19.938 19.938 0 0 0 14.143 5.857 19.94 19.94 0 0 0 14.143-5.857L256 284.287l221.857 221.857c3.905 3.905 9.024 5.857 14.143 5.857s10.237-1.952 14.143-5.857c7.811-7.811 7.811-20.475 0-28.285L284.286 256.002z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="open-menu">
    <path d="M492 236H20c-11.046 0-20 8.954-20 20s8.954 20 20 20h472c11.046 0 20-8.954 20-20s-8.954-20-20-20zm0-160H20C8.954 76 0 84.954 0 96s8.954 20 20 20h472c11.046 0 20-8.954 20-20s-8.954-20-20-20zm0 320H20c-11.046 0-20 8.954-20 20s8.954 20 20 20h472c11.046 0 20-8.954 20-20s-8.954-20-20-20z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id="instagram">
    <path d="M12 2.163c3.204 0 3.584.012 4.85.07 3.252.148 4.771 1.691 4.919 4.919.058 1.265.069 1.645.069 4.849 0 3.205-.012 3.584-.069 4.849-.149 3.225-1.664 4.771-4.919 4.919-1.266.058-1.644.07-4.85.07-3.204 0-3.584-.012-4.849-.07-3.26-.149-4.771-1.699-4.919-4.92-.058-1.265-.07-1.644-.07-4.849 0-3.204.013-3.583.07-4.849.149-3.227 1.664-4.771 4.919-4.919 1.266-.057 1.645-.069 4.849-.069zm0-2.163c-3.259 0-3.667.014-4.947.072-4.358.2-6.78 2.618-6.98 6.98-.059 1.281-.073 1.689-.073 4.948 0 3.259.014 3.668.072 4.948.2 4.358 2.618 6.78 6.98 6.98 1.281.058 1.689.072 4.948.072 3.259 0 3.668-.014 4.948-.072 4.354-.2 6.782-2.618 6.979-6.98.059-1.28.073-1.689.073-4.948 0-3.259-.014-3.667-.072-4.947-.196-4.354-2.617-6.78-6.979-6.98-1.281-.059-1.69-.073-4.949-.073zm0 5.838c-3.403 0-6.162 2.759-6.162 6.162s2.759 6.163 6.162 6.163 6.162-2.759 6.162-6.163c0-3.403-2.759-6.162-6.162-6.162zm0 10.162c-2.209 0-4-1.79-4-4 0-2.209 1.791-4 4-4s4 1.791 4 4c0 2.21-1.791 4-4 4zm6.406-11.845c-.796 0-1.441.645-1.441 1.44s.645 1.44 1.441 1.44c.795 0 1.439-.645 1.439-1.44s-.644-1.44-1.439-1.44z"/>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id=youtube>
    <path d="M19.615 3.184c-3.604-.246-11.631-.245-15.23 0-3.897.266-4.356 2.62-4.385 8.816.029 6.185.484 8.549 4.385 8.816 3.6.245 11.626.246 15.23 0 3.897-.266 4.356-2.62 4.385-8.816-.029-6.185-.484-8.549-4.385-8.816zm-10.615 12.816v-8l8 3.993-8 4.007z"/>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id="stackoverflow">
    <path d="M21 27v-8h3v11H0V19h3v8h18z"></path><path d="M17.1.2L15 1.8l7.9 10.6 2.1-1.6L17.1.2zm3.7 14.7L10.6 6.4l1.7-2 10.2 8.5-1.7 2zM7.2 12.3l12 5.6 1.1-2.4-12-5.6-1.1 2.4zm-1.8 6.8l13.56 1.96.17-2.38-13.26-2.55-.47 2.97zM19 25H5v-3h14v3z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id="xing">
    <path d="M18.188 0c-.517 0-.741.325-.927.66 0 0-7.455 13.224-7.702 13.657.015.024 4.919 9.023 4.919 9.023.17.308.436.66.967.66h3.454c.211 0 .375-.078.463-.22.089-.151.089-.346-.009-.536l-4.879-8.916c-.004-.006-.004-.016 0-.022L22.139.756c.095-.191.097-.387.006-.535C22.056.078 21.894 0 21.686 0h-3.498zM3.648 4.74c-.211 0-.385.074-.473.216-.09.149-.078.339.02.531l2.34 4.05c.004.01.004.016 0 .021L1.86 16.051c-.099.188-.093.381 0 .529.085.142.239.234.45.234h3.461c.518 0 .766-.348.945-.667l3.734-6.609-2.378-4.155c-.172-.315-.434-.659-.962-.659H3.648v.016z"/>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 71 55" id="discord">
    <path d="M60.1045 4.8978C55.5792 2.8214 50.7265 1.2916 45.6527 0.41542C45.5603 0.39851 45.468 0.440769 45.4204 0.525289C44.7963 1.6353 44.105 3.0834 43.6209 4.2216C38.1637 3.4046 32.7345 3.4046 27.3892 4.2216C26.905 3.0581 26.1886 1.6353 25.5617 0.525289C25.5141 0.443589 25.4218 0.40133 25.3294 0.41542C20.2584 1.2888 15.4057 2.8186 10.8776 4.8978C10.8384 4.9147 10.8048 4.9429 10.7825 4.9795C1.57795 18.7309 -0.943561 32.1443 0.293408 45.3914C0.299005 45.4562 0.335386 45.5182 0.385761 45.5576C6.45866 50.0174 12.3413 52.7249 18.1147 54.5195C18.2071 54.5477 18.305 54.5139 18.3638 54.4378C19.7295 52.5728 20.9469 50.6063 21.9907 48.5383C22.0523 48.4172 21.9935 48.2735 21.8676 48.2256C19.9366 47.4931 18.0979 46.6 16.3292 45.5858C16.1893 45.5041 16.1781 45.304 16.3068 45.2082C16.679 44.9293 17.0513 44.6391 17.4067 44.3461C17.471 44.2926 17.5606 44.2813 17.6362 44.3151C29.2558 49.6202 41.8354 49.6202 53.3179 44.3151C53.3935 44.2785 53.4831 44.2898 53.5502 44.3433C53.9057 44.6363 54.2779 44.9293 54.6529 45.2082C54.7816 45.304 54.7732 45.5041 54.6333 45.5858C52.8646 46.6197 51.0259 47.4931 49.0921 48.2228C48.9662 48.2707 48.9102 48.4172 48.9718 48.5383C50.038 50.6034 51.2554 52.5699 52.5959 54.435C52.6519 54.5139 52.7526 54.5477 52.845 54.5195C58.6464 52.7249 64.529 50.0174 70.6019 45.5576C70.6551 45.5182 70.6887 45.459 70.6943 45.3942C72.1747 30.0791 68.2147 16.7757 60.1968 4.9823C60.1772 4.9429 60.1437 4.9147 60.1045 4.8978ZM23.7259 37.3253C20.2276 37.3253 17.3451 34.1136 17.3451 30.1693C17.3451 26.225 20.1717 23.0133 23.7259 23.0133C27.308 23.0133 30.1626 26.2532 30.1066 30.1693C30.1066 34.1136 27.28 37.3253 23.7259 37.3253ZM47.3178 37.3253C43.8196 37.3253 40.9371 34.1136 40.9371 30.1693C40.9371 26.225 43.7636 23.0133 47.3178 23.0133C50.9 23.0133 53.7545 26.2532 53.6986 30.1693C53.6986 34.1136 50.9 37.3253 47.3178 37.3253Z"/>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 17 18" id="mastodon">
    <path
    fill="#ffffff"
    d="m 15.054695,9.8859583 c -0.22611,1.1632697 -2.02517,2.4363497 -4.09138,2.6830797 -1.0774504,0.12856 -2.1382704,0.24673 -3.2694704,0.19484 -1.84996,-0.0848 -3.30971,-0.44157 -3.30971,-0.44157 0,0.1801 0.0111,0.35157 0.0333,0.51194 0.24051,1.82571 1.81034,1.93508 3.29737,1.98607 1.50088,0.0514 2.8373104,-0.37004 2.8373104,-0.37004 l 0.0617,1.35686 c 0,0 -1.0498104,0.56374 -2.9199404,0.66742 -1.03124,0.0567 -2.3117,-0.0259 -3.80308,-0.42069 -3.23454998,-0.85613 -3.79081998,-4.304 -3.87592998,-7.8024197 -0.026,-1.03871 -0.01,-2.01815 -0.01,-2.83732 0,-3.57732 2.34385998,-4.62587996 2.34385998,-4.62587996 1.18184,-0.54277 3.20976,-0.77101 5.318,-0.7882499985409 h 0.0518 C 9.8267646,0.01719834 11.856025,0.24547834 13.037775,0.78824834 c 0,0 2.34377,1.04855996 2.34377,4.62587996 0,0 0.0294,2.63937 -0.32687,4.47183"/>
 <path
    fill="#000000"
    d="m 12.616925,5.6916583 v 4.3315297 h -1.71607 V 5.8189683 c 0,-0.88624 -0.37289,-1.33607 -1.1187604,-1.33607 -0.82467,0 -1.23799,0.53361 -1.23799,1.58875 v 2.30122 h -1.70594 v -2.30122 c 0,-1.05514 -0.4134,-1.58875 -1.23808,-1.58875 -0.74587,0 -1.11876,0.44983 -1.11876,1.33607 v 4.2042197 h -1.71607 V 5.6916583 c 0,-0.88527 0.22541,-1.58876 0.67817,-2.10922 0.46689,-0.52047 1.07833,-0.78727 1.83735,-0.78727 0.87816,0 1.54317,0.33752 1.98288,1.01267 l 0.42744,0.71655 0.42753,-0.71655 c 0.43961,-0.67515 1.10463,-1.01267 1.9828704,-1.01267 0.75893,0 1.37037,0.2668 1.83735,0.78727 0.45268,0.52046 0.67808,1.22395 0.67808,2.10922"/>
  </symbol>
</svg>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<footer class="footer">
  <div class="footer_inner wrap pale">
    <img src='https://mengwoods.github.io/icons/apple-touch-icon.png' class="icon icon_2 transparent" alt="Copyright ¬© 2024, Meng blog; all rights reserved.">
    <p>Copyright&nbsp;<span class="year"></span>&nbsp;COPYRIGHT ¬© 2024, MENG BLOG; ALL RIGHTS RESERVED.. All Rights Reserved</p><a class="to_top" href="#documentTop">
  <svg class="icon">
  <title>to-top</title>
  <use xlink:href="#to-top"></use>
</svg>

</a>

  </div>
</footer>

<script type="text/javascript" src="https://mengwoods.github.io/en/js/bundle.355cf69c98ebdc2dfd538b625fe47ecdcb4e20c2138f80e51cf5425011e8235debe8bf35913e718ee1c24fd71d1f40c12f73985c98cc237a2996520f768509af.js" integrity="sha512-NVz2nJjr3C39U4tiX&#43;R&#43;zctOIMITj4DlHPVCUBHoI13r6L81kT5xjuHCT9cdH0DBL3OYXJjMI3opllIPdoUJrw==" crossorigin="anonymous"></script>

  <script src="https://mengwoods.github.io/js/search.min.786102b9f5b14ee0b60197dc064e532809aec49bcc43fea72e5a513113f04b44a7241d6683d3993d5b7a48582f4986e0b9a28c72ebc48a2072c52aad8f40a2b3.js"></script>

  </body>
</html>
